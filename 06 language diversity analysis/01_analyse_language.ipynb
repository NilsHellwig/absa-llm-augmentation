{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Analyse Language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_CATEGORIES = [\"GENERAL-IMPRESSION\",\n",
    "                     \"FOOD\", \"SERVICE\", \"AMBIENCE\", \"PRICE\"]\n",
    "LLMS = [\"GPT-3\", \"Llama70B\"]\n",
    "FS_CONDITIONS = [\"fixed\", \"random\"]\n",
    "PROMPTING_ENCODING = {\"fixed\": \"25 fixed examples\",\n",
    "                      \"random\": \"25 random examples\"}\n",
    "N_FOLDS = 3\n",
    "CRITERIA_RS = \"tag_with_polarity\"\n",
    "POLARITIES = [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]\n",
    "MENTIONING_TYPE = [\"implicit\", \"explicit\"]\n",
    "COMBINATIONS = [f\"{aspect}-{polarity}\" for aspect in [\"SERVICE\", \"FOOD\",\n",
    "                                                      \"GENERAL-IMPRESSION\", \"AMBIENCE\", \"PRICE\"] for polarity in POLARITIES]\n",
    "RANDOM_STATE = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nils_hellwig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "nltk.download('punkt')\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_sentences(sentences):\n",
    "    unique_sentences = set(sentences)\n",
    "    return len(unique_sentences)\n",
    "\n",
    "\n",
    "def count_unique_tokens(tokens):\n",
    "    unique_tokens = set(token.text for token in tokens)\n",
    "    return len(unique_tokens)\n",
    "\n",
    "\n",
    "def count_unique_lemmas(tokens):\n",
    "    unique_lemmas = set(token.lemma_ for token in tokens)\n",
    "    return len(unique_lemmas)\n",
    "\n",
    "\n",
    "def remove_stopwords_and_punctuation(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if token.text.lower(\n",
    "    ) not in STOP_WORDS and token.text not in string.punctuation and token.text.isalpha()]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def count_top_n_lemmas(texts, n):\n",
    "    lemma_counts = {}\n",
    "    for text in texts:\n",
    "        cleaned_text = remove_stopwords_and_punctuation(text)\n",
    "        doc = nlp(cleaned_text)\n",
    "        for token in doc:\n",
    "            lemma = token.lemma_\n",
    "            if lemma in lemma_counts:\n",
    "                lemma_counts[lemma] += 1\n",
    "            else:\n",
    "                lemma_counts[lemma] = 1\n",
    "\n",
    "    sorted_lemmas = sorted(\n",
    "        lemma_counts, key=lambda lemma: lemma_counts[lemma], reverse=True)\n",
    "    top_n_lemmas = sorted_lemmas[:n]\n",
    "\n",
    "    return ', '.join(top_n_lemmas)\n",
    "\n",
    "\n",
    "def get_avg_unique_words_in_k_words(tokens, n_selection=100, n_repetitions=100000):\n",
    "    iterations_n_unique_words = []\n",
    "    for i in range(n_repetitions):\n",
    "        random_indices = random.sample(range(len(tokens)), n_selection)\n",
    "        random_words = [tokens[index] for index in random_indices]\n",
    "        n_unique_words = len(set(random_words))\n",
    "        iterations_n_unique_words.append(n_unique_words)\n",
    "    return np.mean(iterations_n_unique_words)\n",
    "\n",
    "\n",
    "def average_word_level_levenshtein_distance(docs, norm=False):\n",
    "    tokenized_texts = [\n",
    "        [token.text for token in doc[\"tokenized_text\"]] for doc in docs]\n",
    "\n",
    "    total_distance = 0\n",
    "    pair_count = 0\n",
    "\n",
    "    for i in range(len(tokenized_texts)):\n",
    "        for j in range(i + 1, len(tokenized_texts)):\n",
    "            tokens1 = tokenized_texts[i]\n",
    "            tokens2 = tokenized_texts[j]\n",
    "\n",
    "            if len(tokens1) >= len(tokens2):\n",
    "                max_tokens = len(tokens1)\n",
    "            else:\n",
    "                max_tokens = len(tokens2)\n",
    "\n",
    "            distance = Levenshtein.distance(tokens1, tokens2)\n",
    "            if norm:\n",
    "                distance = distance / max_tokens\n",
    "            total_distance += distance\n",
    "            pair_count += 1\n",
    "\n",
    "    average_distance = total_distance / pair_count if pair_count > 0 else 0\n",
    "    return average_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = {\"synth\": {}, \"real\": []}\n",
    "\n",
    "# Load Synth\n",
    "for llm in LLMS:\n",
    "    dataset_raw[\"synth\"][llm] = {}\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        dataset_raw[\"synth\"][llm][prompting] = []\n",
    "        for split in range(5):\n",
    "            with open(f\"../07 train models/synth/{llm}/{prompting}/split_{split}.json\", 'r', encoding='utf-8') as json_file:\n",
    "                split_data = json.load(json_file)\n",
    "            for example in split_data:\n",
    "                example[\"tokenized_text\"] = nlp(example[\"text\"])\n",
    "            dataset_raw[\"synth\"][llm][prompting].append(split_data)\n",
    "\n",
    "# Load Real\n",
    "for split in range(6):\n",
    "    with open(f\"../07 train models/real/split_{split}.json\", 'r', encoding='utf-8') as json_file:\n",
    "        split_data = json.load(json_file)\n",
    "    for example in split_data:\n",
    "        example[\"tokenized_text\"] = nlp(example[\"text\"])\n",
    "    dataset_raw[\"real\"].append(split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(subset):\n",
    "    labels_one_hot = []\n",
    "    for i in range(len(subset)):\n",
    "        tags_in_example = list(set([tag[CRITERIA_RS]\n",
    "                               for tag in subset[i][\"tags\"]]))\n",
    "        one_hot_encoded_combination = np.array(\n",
    "            [1 if tag in tags_in_example else 0 for tag in COMBINATIONS])\n",
    "        labels_one_hot.append(one_hot_encoded_combination)\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 fixed 0\n",
      "GPT-3 fixed 1\n",
      "GPT-3 fixed 2\n",
      "GPT-3 fixed 3\n",
      "GPT-3 fixed 4\n",
      "GPT-3 random 0\n",
      "GPT-3 random 1\n",
      "GPT-3 random 2\n",
      "GPT-3 random 3\n",
      "GPT-3 random 4\n",
      "Llama70B fixed 0\n",
      "Llama70B fixed 1\n",
      "Llama70B fixed 2\n",
      "Llama70B fixed 3\n",
      "Llama70B fixed 4\n",
      "Llama70B random 0\n",
      "Llama70B random 1\n",
      "Llama70B random 2\n",
      "Llama70B random 3\n",
      "Llama70B random 4\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for few_shot_condition in FS_CONDITIONS:\n",
    "        for iteration in range(5):\n",
    "            print(llm, few_shot_condition, iteration)\n",
    "            if few_shot_condition == \"random\":\n",
    "                subset = dataset[\"synth\"][llm][few_shot_condition][iteration]\n",
    "            else:\n",
    "                subset = dataset[\"synth\"][llm][few_shot_condition][iteration][475:]\n",
    "\n",
    "            found_5_split = False\n",
    "            restart_idx = 0\n",
    "            while found_5_split == False:\n",
    "                mskf = MultilabelStratifiedKFold(\n",
    "                    n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE+restart_idx)\n",
    "                section = []\n",
    "                for train_index, test_index in mskf.split(subset, get_one_hot(subset)):\n",
    "                    split_500 = [subset[i] for i in test_index]\n",
    "                    section.append(split_500)\n",
    "\n",
    "                if len(section[0]) == 500 and len(section[1]) == 500 and len(section[2]) == 500:\n",
    "                    found_5_split = True\n",
    "\n",
    "                restart_idx += 1\n",
    "\n",
    "            dataset[\"synth\"][llm][few_shot_condition][iteration] = section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_examples = []\n",
    "for i in [0, 1, 2, 3, 4, 5]:\n",
    "    real_examples.append([])\n",
    "    for k in [0, 1, 2]:\n",
    "        if (i+k) < 6:\n",
    "            t = i+k\n",
    "        else:\n",
    "            t = i+k - 6\n",
    "        real_examples[i].append(dataset[\"real\"][t])\n",
    "dataset[\"real\"] = real_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 & fixed & 500 & 307.6 & 289.6 & 208.2 & 47.86 & 9.79 & 0.79\n",
      "GPT-3 & fixed & 1000 & 549.0 & 369.6 & 264.4 & 47.85 & 9.79 & 0.79\n",
      "GPT-3 & fixed & 1500 & 769.8 & 428.4 & 307.0 & 48.01 & 9.8 & 0.79\n",
      "\\hline\n",
      "GPT-3 & random & 500 & 317.0 & 295.4 & 217.0 & 48.09 & 8.81 & 0.77\n",
      "GPT-3 & random & 1000 & 561.2 & 389.4 & 281.2 & 48.32 & 8.84 & 0.77\n",
      "GPT-3 & random & 1500 & 782.2 & 456.6 & 328.4 & 48.26 & 8.85 & 0.77\n",
      "\\hline\n",
      "Llama70B & fixed & 500 & 480.0 & 694.4 & 536.6 & 59.18 & 11.08 & 0.85\n",
      "Llama70B & fixed & 1000 & 934.0 & 1023.8 & 787.0 & 59.69 & 11.09 & 0.85\n",
      "Llama70B & fixed & 1500 & 1383.2 & 1269.2 & 973.0 & 59.63 & 11.06 & 0.85\n",
      "\\hline\n",
      "Llama70B & random & 500 & 485.4 & 751.8 & 580.2 & 61.61 & 11.06 & 0.86\n",
      "Llama70B & random & 1000 & 949.4 & 1103.0 & 846.8 & 61.36 & 11.04 & 0.86\n",
      "Llama70B & random & 1500 & 1400.0 & 1380.4 & 1054.0 & 61.22 & 10.98 & 0.86\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for few_shot_condition in FS_CONDITIONS:\n",
    "        for n_sample in [500, 1000, 1500]:\n",
    "            iterations_n_unique_tokens = []\n",
    "            iterations_n_lemmas = []\n",
    "            iterations_avg_unique_sentences = []\n",
    "            iterations_avg_unique_words_in_k_words = []\n",
    "            iterations_avg_levenshtein_distance = []\n",
    "            iterations_avg_levenshtein_distance_norm = []\n",
    "            for it in range(5):\n",
    "                samples = [item for k in range(\n",
    "                    int(n_sample / 500)) for item in dataset[\"synth\"][llm][few_shot_condition][it][k]]\n",
    "                n_unique_tokens = count_unique_tokens(\n",
    "                    [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "                n_unique_lemmas = count_unique_lemmas(\n",
    "                    [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "                n_unique_sentences = count_unique_sentences(\n",
    "                    [example[\"text\"] for example in samples])\n",
    "                n_unique_words_in_k_words = get_avg_unique_words_in_k_words(\n",
    "                    [token.text for example in samples for token in example[\"tokenized_text\"]])\n",
    "                avg_levenshtein_distance = average_word_level_levenshtein_distance(\n",
    "                    samples)\n",
    "                avg_levenshtein_distance_norm = average_word_level_levenshtein_distance(\n",
    "                    samples, norm=True)\n",
    "\n",
    "                iterations_n_unique_tokens.append(n_unique_tokens)\n",
    "                iterations_n_lemmas.append(n_unique_lemmas)\n",
    "                iterations_avg_unique_sentences.append(n_unique_sentences)\n",
    "                iterations_avg_unique_words_in_k_words.append(\n",
    "                    n_unique_words_in_k_words)\n",
    "                iterations_avg_levenshtein_distance.append(\n",
    "                    avg_levenshtein_distance)\n",
    "                iterations_avg_levenshtein_distance_norm.append(\n",
    "                    avg_levenshtein_distance_norm)\n",
    "\n",
    "            print(llm, \"&\", few_shot_condition, \"&\", n_sample, \"&\",\n",
    "                  round(np.mean(iterations_avg_unique_sentences), 2), \"&\",\n",
    "                  round(np.mean(iterations_n_unique_tokens), 2), \"&\",\n",
    "                  round(np.mean(iterations_n_lemmas), 2), \"&\",\n",
    "                  round(np.mean(iterations_avg_unique_words_in_k_words), 2), \"&\",\n",
    "                  round(np.mean(iterations_avg_levenshtein_distance), 2), \"&\",\n",
    "                  round(np.mean(iterations_avg_levenshtein_distance_norm), 2))\n",
    "        print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- & - & 500 & 496.4 & 1914.8 & 1492.8 & 78.18 & 16.37 & 0.93\n",
      "- & - & 1000 & 988.8 & 3064.2 & 2352.4 & 78.28 & 16.42 & 0.93\n",
      "- & - & 1500 & 1480.8 & 3998.6 & 3041.0 & 78.31 & 16.39 & 0.93\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for n_sample in [500, 1000, 1500]:\n",
    "    iterations_n_unique_tokens = []\n",
    "    iterations_n_lemmas = []\n",
    "    iterations_avg_unique_sentences = []\n",
    "    iterations_avg_unique_words_in_k_words = []\n",
    "    iterations_avg_levenshtein_distance = []\n",
    "    iterations_avg_levenshtein_distance_norm = []\n",
    "    for it in range(5):\n",
    "        samples = [item for k in range(\n",
    "            int(n_sample / 500)) for item in dataset[\"real\"][it][k]]\n",
    "        n_unique_tokens = count_unique_tokens(\n",
    "            [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "        n_unique_lemmas = count_unique_lemmas(\n",
    "            [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "        n_unique_sentences = count_unique_sentences(\n",
    "            [example[\"text\"] for example in samples])\n",
    "        n_unique_words_in_k_words = get_avg_unique_words_in_k_words(\n",
    "            [token.text for example in samples for token in example[\"tokenized_text\"]])\n",
    "        avg_levenshtein_distance = average_word_level_levenshtein_distance(\n",
    "            samples)\n",
    "        avg_levenshtein_distance_norm = average_word_level_levenshtein_distance(\n",
    "            samples, norm=True)\n",
    "\n",
    "        iterations_n_unique_tokens.append(n_unique_tokens)\n",
    "        iterations_n_lemmas.append(n_unique_lemmas)\n",
    "        iterations_avg_unique_sentences.append(n_unique_sentences)\n",
    "        iterations_avg_unique_words_in_k_words.append(\n",
    "            n_unique_words_in_k_words)\n",
    "        iterations_avg_levenshtein_distance.append(\n",
    "            avg_levenshtein_distance)\n",
    "        iterations_avg_levenshtein_distance_norm.append(\n",
    "            avg_levenshtein_distance_norm)\n",
    "\n",
    "    print(\"-\", \"&\", \"-\", \"&\", n_sample, \"&\",\n",
    "          round(np.mean(iterations_avg_unique_sentences), 2), \"&\",\n",
    "          round(np.mean(iterations_n_unique_tokens), 2), \"&\",\n",
    "          round(np.mean(iterations_n_lemmas), 2), \"&\",\n",
    "          round(np.mean(iterations_avg_unique_words_in_k_words), 2), \"&\",\n",
    "          round(np.mean(iterations_avg_levenshtein_distance), 2), \"&\",\n",
    "          round(np.mean(iterations_avg_levenshtein_distance_norm), 2))\n",
    "print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Token in Sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prozentsatz der Artikel (GPT-3, random): 91.55 %\n",
      "Prozentsatz der Artikel (GPT-3, random): 91.67 %\n",
      "Prozentsatz der Artikel (Llama70B, random): 57.87 %\n",
      "Prozentsatz der Artikel (Llama70B, random): 57.33 %\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for few_shot_condition in FS_CONDITIONS:\n",
    "        first_tokens = [example[\"tokenized_text\"][0].pos_ for idx in range(\n",
    "            5) for split_data in dataset_raw[\"synth\"][llm][few_shot_condition][idx] for example in split_data]\n",
    "        pos_counts = Counter(first_tokens)\n",
    "        article_percentage = (pos_counts[\"DET\"] / len(first_tokens)) * 100\n",
    "        print(\n",
    "            f\"Prozentsatz der Artikel ({llm}, {prompting}): {round(article_percentage, 2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prozentsatz der Artikel Real: 27.6 %\n"
     ]
    }
   ],
   "source": [
    "first_tokens = [example[\"tokenized_text\"][0].pos_ for split_idx in range(\n",
    "    5) for example in dataset_raw[\"real\"][split_idx]]\n",
    "pos_counts = Counter(first_tokens)\n",
    "article_percentage = (pos_counts[\"DET\"] / len(first_tokens)) * 100\n",
    "print(\n",
    "    f\"Prozentsatz der Artikel Real: {round(article_percentage, 2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVG Number of Tokens in Sentence for each Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 & fixed & 9.67 [9.22, 9.73, 9.08, 10.33, 9.98] 0.466\n",
      "GPT-3 & random & 9.04 [8.97, 9.11, 8.84, 8.87, 9.4] 0.204\n",
      "Llama70B & fixed & 10.36 [9.52, 10.93, 10.0, 10.48, 10.85] 0.532\n",
      "Llama70B & random & 10.2 [10.06, 10.26, 9.89, 10.24, 10.57] 0.227\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for few_shot_condition in FS_CONDITIONS:\n",
    "        word_counts = [len(example[\"tokenized_text\"]) for idx in range(\n",
    "            5) for split_data in dataset_raw[\"synth\"][llm][few_shot_condition][idx] for example in split_data]\n",
    "\n",
    "        word_counts_splits = []\n",
    "        for idx in range(5):\n",
    "            word_counts_splits.append(round(np.mean(\n",
    "                [len(example[\"tokenized_text\"]) for split_data in dataset_raw[\"synth\"][llm][few_shot_condition][idx] for example in split_data]), 2))\n",
    "\n",
    "        print(llm, \"&\", few_shot_condition, \"&\", round(\n",
    "            np.mean(word_counts), 2), word_counts_splits, round(np.std(word_counts_splits), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real 13.12\n"
     ]
    }
   ],
   "source": [
    "word_counts = [len(example[\"tokenized_text\"]) for idx in range(6) for example in dataset_raw[\"real\"][idx]]\n",
    "\n",
    "print(\"Real\", round(np.mean(word_counts),2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect Term Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 & fixed & 500 & 787.6 & 271.4 & 516.2 & 38.8 & 17.92\n",
      "GPT-3 & fixed & 1000 & 1576.0 & 543.6 & 1032.4 & 50.6 & 17.56\n",
      "GPT-3 & fixed & 1500 & 2365.4 & 815.0 & 1550.4 & 61.8 & 17.45\n",
      "GPT-3 & random & 500 & 694.4 & 304.2 & 390.2 & 33.0 & 18.11\n",
      "GPT-3 & random & 1000 & 1387.8 & 606.8 & 781.0 & 43.8 & 17.57\n",
      "GPT-3 & random & 1500 & 2082.8 & 902.6 & 1180.2 & 54.2 & 17.61\n",
      "Llama70B & fixed & 500 & 787.2 & 208.4 & 578.8 & 104.8 & 34.14\n",
      "Llama70B & fixed & 1000 & 1574.2 & 424.8 & 1149.4 & 170.0 & 33.91\n",
      "Llama70B & fixed & 1500 & 2360.0 & 639.6 & 1720.4 & 220.0 & 33.43\n",
      "Llama70B & random & 500 & 695.2 & 178.0 & 517.2 & 116.8 & 37.99\n",
      "Llama70B & random & 1000 & 1390.8 & 353.4 & 1037.4 & 194.4 & 38.97\n",
      "Llama70B & random & 1500 & 2088.2 & 538.2 & 1550.0 & 256.6 & 38.88\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for few_shot_condition in FS_CONDITIONS:\n",
    "        for n_sample in [500, 1000, 1500]:\n",
    "            tags_from_splits_count = []\n",
    "            tags_from_splits_count_implicit = []\n",
    "            tags_from_splits_count_explicit = []\n",
    "            count_unique_aspect_terms_in_split = []\n",
    "            count_unique_aspect_terms_in_k_aspect_terms = []\n",
    "            for it in range(5):\n",
    "                tags = [tag for k in range(\n",
    "                    int(n_sample / 500)) for example in dataset[\"synth\"][llm][few_shot_condition][it][k] for tag in example[\"tags\"]]\n",
    "                tags_explicit = [tag[\"text\"]\n",
    "                                 for tag in tags if tag[\"type\"] == \"label-explicit\"]\n",
    "                tags_from_splits_count.append(len(tags))\n",
    "                tags_from_splits_count_explicit.append(\n",
    "                    len([tag for tag in tags if tag[\"type\"] == \"label-explicit\"]))\n",
    "                tags_from_splits_count_implicit.append(\n",
    "                    len([tag for tag in tags if tag[\"type\"] == \"label-implicit\"]))\n",
    "\n",
    "                unique_tags = len(set(tags_explicit))\n",
    "\n",
    "                # Calculate number of unique tokens in 100 aspect terms\n",
    "                count_unique_aspect_terms_in_k_aspect_terms.append(\n",
    "                    get_avg_unique_words_in_k_words(tags_explicit))\n",
    "\n",
    "                count_unique_aspect_terms_in_split.append(unique_tags)\n",
    "            print(llm, \"&\", few_shot_condition,\n",
    "                  \"&\", n_sample,\n",
    "                  \"&\", round(np.mean(tags_from_splits_count), 2),\n",
    "                  \"&\", round(np.mean(tags_from_splits_count_implicit), 2),\n",
    "                  \"&\", round(np.mean(tags_from_splits_count_explicit), 2),\n",
    "                  \"&\", round(np.mean(count_unique_aspect_terms_in_split), 2),\n",
    "                  \"&\", round(np.mean(count_unique_aspect_terms_in_k_aspect_terms), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- & - & 500 & 703.2 & 186.0 & 517.2 & 256.4 & 68.27\n",
      "- & - & 1000 & 1403.8 & 374.0 & 1029.8 & 438.4 & 68.25\n",
      "- & - & 1500 & 2107.2 & 564.8 & 1542.4 & 595.6 & 68.11\n"
     ]
    }
   ],
   "source": [
    "for n_sample in [500, 1000, 1500]:\n",
    "    tags_from_splits_count = []\n",
    "    tags_from_splits_count_implicit = []\n",
    "    tags_from_splits_count_explicit = []\n",
    "    count_unique_aspect_terms_in_split = []\n",
    "    count_unique_aspect_terms_in_k_aspect_terms = []\n",
    "    for it in range(5):\n",
    "        tags = [tag for k in range(int(n_sample / 500))\n",
    "                for example in dataset[\"real\"][it][k] for tag in example[\"tags\"]]\n",
    "        tags_explicit = [tag[\"text\"]\n",
    "                         for tag in tags if tag[\"type\"] == \"label-explicit\"]\n",
    "        tags_from_splits_count.append(len(tags))\n",
    "        tags_from_splits_count_explicit.append(\n",
    "            len([tag for tag in tags if tag[\"type\"] == \"label-explicit\"]))\n",
    "        tags_from_splits_count_implicit.append(\n",
    "            len([tag for tag in tags if tag[\"type\"] == \"label-implicit\"]))\n",
    "\n",
    "        unique_tags = len(set(tags_explicit))\n",
    "\n",
    "        # Calculate number of unique tokens in 100 aspect terms\n",
    "        count_unique_aspect_terms_in_k_aspect_terms.append(\n",
    "            get_avg_unique_words_in_k_words(tags_explicit))\n",
    "\n",
    "        count_unique_aspect_terms_in_split.append(unique_tags)\n",
    "    print(\"-\", \"&\", \"-\",\n",
    "          \"&\", n_sample,\n",
    "          \"&\", round(np.mean(tags_from_splits_count), 2),\n",
    "          \"&\", round(np.mean(tags_from_splits_count_implicit), 2),\n",
    "          \"&\", round(np.mean(tags_from_splits_count_explicit), 2),\n",
    "          \"&\", round(np.mean(count_unique_aspect_terms_in_split), 2),\n",
    "          \"&\", round(np.mean(count_unique_aspect_terms_in_k_aspect_terms), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
