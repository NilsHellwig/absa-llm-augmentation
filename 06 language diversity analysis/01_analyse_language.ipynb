{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Analyse Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_CATEGORIES = [\"GENERAL-IMPRESSION\",\n",
    "                     \"FOOD\", \"SERVICE\", \"AMBIENCE\", \"PRICE\"]\n",
    "LLMS = [\"GPT-3\", \"Llama70B\"]\n",
    "FS_CONDITIONS = [\"fixed\", \"random\"]\n",
    "PROMPTING_ENCODING = {\"fixed\": \"25 fixed examples\",\n",
    "                      \"random\": \"25 random examples\"}\n",
    "N_FOLDS = 3\n",
    "CRITERIA_RS = \"tag_with_polarity\"\n",
    "POLARITIES = [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]\n",
    "MENTIONING_TYPE = [\"implicit\", \"explicit\"]\n",
    "COMBINATIONS = [f\"{aspect}-{polarity}\" for aspect in [\"SERVICE\", \"FOOD\", \"GENERAL-IMPRESSION\", \"AMBIENCE\", \"PRICE\"] for polarity in POLARITIES]\n",
    "RANDOM_STATE = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nils_hellwig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "nltk.download('punkt')\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_sentences(sentences):\n",
    "    unique_sentences = set(sentences)\n",
    "    return len(unique_sentences)\n",
    "\n",
    "def count_unique_tokens(tokens):\n",
    "    unique_tokens = set(token.text for token in tokens)\n",
    "    return len(unique_tokens)\n",
    "\n",
    "\n",
    "def count_unique_lemmas(tokens):\n",
    "    unique_lemmas = set(token.lemma_ for token in tokens)\n",
    "    return len(unique_lemmas)\n",
    "\n",
    "\n",
    "def remove_stopwords_and_punctuation(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if token.text.lower(\n",
    "    ) not in STOP_WORDS and token.text not in string.punctuation and token.text.isalpha()]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def count_top_n_lemmas(texts, n):\n",
    "    lemma_counts = {}\n",
    "    for text in texts:\n",
    "        cleaned_text = remove_stopwords_and_punctuation(text)\n",
    "        doc = nlp(cleaned_text)\n",
    "        for token in doc:\n",
    "            lemma = token.lemma_\n",
    "            if lemma in lemma_counts:\n",
    "                lemma_counts[lemma] += 1\n",
    "            else:\n",
    "                lemma_counts[lemma] = 1\n",
    "\n",
    "    sorted_lemmas = sorted(\n",
    "        lemma_counts, key=lambda lemma: lemma_counts[lemma], reverse=True)\n",
    "    top_n_lemmas = sorted_lemmas[:n]\n",
    "\n",
    "    return ', '.join(top_n_lemmas)\n",
    "\n",
    "\n",
    "def get_avg_unique_words_in_k_words(tokens, n_selection=100, n_repetitions=100000):\n",
    "    lemmas = [token.text for token in tokens]\n",
    "    iterations_n_unique_lemmas = []\n",
    "    for i in range(n_repetitions):\n",
    "        random_indices = random.sample(range(len(lemmas)), n_selection)\n",
    "        random_lemmas = [lemmas[index] for index in random_indices]\n",
    "        n_unique_lemmas = len(set(random_lemmas))\n",
    "        iterations_n_unique_lemmas.append(n_unique_lemmas)\n",
    "    return np.mean(iterations_n_unique_lemmas)\n",
    "\n",
    "\n",
    "def average_word_level_levenshtein_distance(docs, norm=False):\n",
    "    tokenized_texts = [\n",
    "        [token.text for token in doc[\"tokenized_text\"]] for doc in docs]\n",
    "\n",
    "    total_distance = 0\n",
    "    pair_count = 0\n",
    "\n",
    "    for i in range(len(tokenized_texts)):\n",
    "        for j in range(i + 1, len(tokenized_texts)):\n",
    "            tokens1 = tokenized_texts[i]\n",
    "            tokens2 = tokenized_texts[j]\n",
    "\n",
    "            if len(tokens1) >= len(tokens2):\n",
    "                max_tokens = len(tokens1)\n",
    "            else:\n",
    "                max_tokens = len(tokens2)\n",
    "\n",
    "            distance = Levenshtein.distance(tokens1, tokens2)\n",
    "            if norm:\n",
    "                distance = distance / max_tokens\n",
    "            total_distance += distance\n",
    "            pair_count += 1\n",
    "\n",
    "    average_distance = total_distance / pair_count if pair_count > 0 else 0\n",
    "    return average_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = {\"synth\": {}, \"real\": []}\n",
    "\n",
    "# Load Synth\n",
    "for llm in LLMS:\n",
    "    dataset_raw[\"synth\"][llm] = {}\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        dataset_raw[\"synth\"][llm][prompting] = []\n",
    "        for split in range(5):\n",
    "            with open(f\"../07 train models/synth/{llm}/{prompting}/split_{split}.json\", 'r', encoding='utf-8') as json_file:\n",
    "                split_data = json.load(json_file)\n",
    "            for example in split_data:\n",
    "                example[\"tokenized_text\"] = nlp(example[\"text\"])\n",
    "            dataset_raw[\"synth\"][llm][prompting].append(split_data)\n",
    "\n",
    "# Load Real\n",
    "for split in range(5):\n",
    "    with open(f\"../07 train models/real/split_{split}.json\", 'r', encoding='utf-8') as json_file:\n",
    "        split_data = json.load(json_file)\n",
    "    for example in split_data:\n",
    "        example[\"tokenized_text\"] = nlp(example[\"text\"])\n",
    "    dataset_raw[\"real\"].append(split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(subset):\n",
    "    labels_one_hot = []\n",
    "    for i in range(len(subset)):\n",
    "        tags_in_example = list(set([tag[CRITERIA_RS] for tag in subset[i][\"tags\"]]))\n",
    "        one_hot_encoded_combination = np.array([1 if tag in tags_in_example else 0 for tag in COMBINATIONS])\n",
    "        labels_one_hot.append(one_hot_encoded_combination)\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 fixed 0\n",
      "GPT-3 fixed 1\n",
      "GPT-3 fixed 2\n",
      "GPT-3 fixed 3\n",
      "GPT-3 fixed 4\n",
      "GPT-3 random 0\n",
      "GPT-3 random 1\n",
      "GPT-3 random 2\n",
      "GPT-3 random 3\n",
      "GPT-3 random 4\n",
      "Llama70B fixed 0\n",
      "Llama70B fixed 1\n",
      "Llama70B fixed 2\n",
      "Llama70B fixed 3\n",
      "Llama70B fixed 4\n",
      "Llama70B random 0\n",
      "Llama70B random 1\n",
      "Llama70B random 2\n",
      "Llama70B random 3\n",
      "Llama70B random 4\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for few_shot_condition in FS_CONDITIONS:\n",
    "        for iteration in range(5):\n",
    "            print(llm, few_shot_condition, iteration)\n",
    "            if few_shot_condition == \"random\":\n",
    "                subset = dataset[\"synth\"][llm][few_shot_condition][iteration]\n",
    "            else:\n",
    "                subset = dataset[\"synth\"][llm][few_shot_condition][iteration][475:]\n",
    "\n",
    "            found_5_split = False\n",
    "            restart_idx = 0\n",
    "            while found_5_split == False:\n",
    "                mskf = MultilabelStratifiedKFold(\n",
    "                    n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE+restart_idx)\n",
    "                section = []\n",
    "                for train_index, test_index in mskf.split(subset, get_one_hot(subset)):\n",
    "                    split_500 = [subset[i] for i in test_index]\n",
    "                    section.append(split_500)\n",
    "\n",
    "                if len(section[0]) == 500 and len(section[1]) == 500 and len(section[2]) == 500:\n",
    "                    found_5_split = True\n",
    "\n",
    "                restart_idx += 1\n",
    "\n",
    "            dataset[\"synth\"][llm][few_shot_condition][iteration] = section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_examples = []\n",
    "for i in [0, 1, 2, 3, 4]:\n",
    "    real_examples.append([])\n",
    "    for k in [0, 1, 2]:\n",
    "        if (i+k) < 5:\n",
    "            t = i+k\n",
    "        else:\n",
    "            t = i+k - 5\n",
    "        real_examples[i].append(dataset[\"real\"][t])\n",
    "dataset[\"real\"] = real_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 & fixed & 500 & 307.6 & 289.6 & 208.2 & 47.86 & 9.79 & 0.79\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m n_unique_sentences \u001b[38;5;241m=\u001b[39m count_unique_sentences([example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m samples])\n\u001b[1;32m     18\u001b[0m n_unique_words_in_k_words \u001b[38;5;241m=\u001b[39m get_avg_unique_words_in_k_words(\n\u001b[1;32m     19\u001b[0m     [token \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m samples \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m---> 20\u001b[0m avg_levenshtein_distance \u001b[38;5;241m=\u001b[39m \u001b[43maverage_word_level_levenshtein_distance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m avg_levenshtein_distance_norm \u001b[38;5;241m=\u001b[39m average_word_level_levenshtein_distance(\n\u001b[1;32m     23\u001b[0m     samples, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m iterations_n_unique_tokens\u001b[38;5;241m.\u001b[39mappend(n_unique_tokens)\n",
      "Cell \u001b[0;32mIn[4], line 77\u001b[0m, in \u001b[0;36maverage_word_level_levenshtein_distance\u001b[0;34m(docs, norm)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens2)\n\u001b[0;32m---> 77\u001b[0m distance \u001b[38;5;241m=\u001b[39m \u001b[43mLevenshtein\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m     79\u001b[0m     distance \u001b[38;5;241m=\u001b[39m distance \u001b[38;5;241m/\u001b[39m max_tokens\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.11/site-packages/Levenshtein/__init__.py:116\u001b[0m, in \u001b[0;36mdistance\u001b[0;34m(s1, s2, weights, processor, score_cutoff)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistance\u001b[39m(s1, s2, \u001b[38;5;241m*\u001b[39m, weights\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), processor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, score_cutoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    Calculates the minimum number of insertions, deletions, and substitutions\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    required to change one sequence into the other according to Levenshtein with custom\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_Levenshtein\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_cutoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_cutoff\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for few_shot_condition in FS_CONDITIONS:\n",
    "        for n_sample in [500, 1000, 1500]:\n",
    "            iterations_n_unique_tokens = []\n",
    "            iterations_n_lemmas = []\n",
    "            iterations_avg_unique_sentences = []\n",
    "            iterations_avg_unique_words_in_k_words = []\n",
    "            iterations_avg_levenshtein_distance = []\n",
    "            iterations_avg_levenshtein_distance_norm = []\n",
    "            for it in range(5):\n",
    "                samples = [item for k in range(\n",
    "                    int(n_sample / 500)) for item in dataset[\"synth\"][llm][few_shot_condition][it][k]]\n",
    "                n_unique_tokens = count_unique_tokens(\n",
    "                    [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "                n_unique_lemmas = count_unique_lemmas(\n",
    "                    [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "                n_unique_sentences = count_unique_sentences([example[\"text\"] for example in samples])\n",
    "                n_unique_words_in_k_words = get_avg_unique_words_in_k_words(\n",
    "                    [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "                avg_levenshtein_distance = average_word_level_levenshtein_distance(\n",
    "                    samples)\n",
    "                avg_levenshtein_distance_norm = average_word_level_levenshtein_distance(\n",
    "                    samples, norm=True)\n",
    "\n",
    "                iterations_n_unique_tokens.append(n_unique_tokens)\n",
    "                iterations_n_lemmas.append(n_unique_lemmas)\n",
    "                iterations_avg_unique_sentences.append(n_unique_sentences)\n",
    "                iterations_avg_unique_words_in_k_words.append(\n",
    "                    n_unique_words_in_k_words)\n",
    "                iterations_avg_levenshtein_distance.append(\n",
    "                    avg_levenshtein_distance)\n",
    "                iterations_avg_levenshtein_distance_norm.append(\n",
    "                    avg_levenshtein_distance_norm)\n",
    "\n",
    "            print(llm, \"&\", few_shot_condition, \"&\", n_sample, \"&\",\n",
    "                  round(np.mean(iterations_avg_unique_sentences), 2), \"&\",\n",
    "                  round(np.mean(iterations_n_unique_tokens), 2), \"&\",\n",
    "                  round(np.mean(iterations_n_lemmas), 2), \"&\",\n",
    "                  round(np.mean(iterations_avg_unique_words_in_k_words), 2), \"&\",\n",
    "                  round(np.mean(iterations_avg_levenshtein_distance), 2), \"&\",\n",
    "                  round(np.mean(iterations_avg_levenshtein_distance_norm), 2))\n",
    "        print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 9, 56, 12, 4, 6, 9, 4, 2, 35, 7, 10, 8, 11, 4, 10, 7, 3, 11, 32] 12.85\n",
      "[10, 12, 6, 3, 4, 15, 30, 19, 6, 3, 10, 6, 6, 6, 13, 27, 9, 12, 9, 8] 10.7\n",
      "[9, 15, 11, 6, 13, 33, 9, 20, 22, 30, 35, 13, 3, 12, 4, 23, 9, 14, 33, 8] 16.1\n",
      "[11, 13, 6, 11, 17, 22, 13, 15, 16, 10, 11, 10, 18, 14, 22, 10, 14, 5, 17, 10] 13.25\n",
      "[27, 10, 6, 11, 21, 24, 14, 20, 10, 8, 4, 12, 5, 6, 15, 64, 7, 15, 19, 17] 15.75\n",
      "- & - & 500 & 496.4 & 1914.8 & 1492.8 & 78.19 & 16.37 & 0.93\n",
      "[17, 9, 56, 12, 4, 6, 9, 4, 2, 35, 7, 10, 8, 11, 4, 10, 7, 3, 11, 32] 12.85\n",
      "[10, 12, 6, 3, 4, 15, 30, 19, 6, 3, 10, 6, 6, 6, 13, 27, 9, 12, 9, 8] 10.7\n",
      "[9, 15, 11, 6, 13, 33, 9, 20, 22, 30, 35, 13, 3, 12, 4, 23, 9, 14, 33, 8] 16.1\n",
      "[11, 13, 6, 11, 17, 22, 13, 15, 16, 10, 11, 10, 18, 14, 22, 10, 14, 5, 17, 10] 13.25\n",
      "[27, 10, 6, 11, 21, 24, 14, 20, 10, 8, 4, 12, 5, 6, 15, 64, 7, 15, 19, 17] 15.75\n",
      "- & - & 1000 & 989.4 & 3054.6 & 2345.0 & 78.2 & 16.38 & 0.93\n",
      "[17, 9, 56, 12, 4, 6, 9, 4, 2, 35, 7, 10, 8, 11, 4, 10, 7, 3, 11, 32] 12.85\n",
      "[10, 12, 6, 3, 4, 15, 30, 19, 6, 3, 10, 6, 6, 6, 13, 27, 9, 12, 9, 8] 10.7\n",
      "[9, 15, 11, 6, 13, 33, 9, 20, 22, 30, 35, 13, 3, 12, 4, 23, 9, 14, 33, 8] 16.1\n",
      "[11, 13, 6, 11, 17, 22, 13, 15, 16, 10, 11, 10, 18, 14, 22, 10, 14, 5, 17, 10] 13.25\n",
      "[27, 10, 6, 11, 21, 24, 14, 20, 10, 8, 4, 12, 5, 6, 15, 64, 7, 15, 19, 17] 15.75\n",
      "- & - & 1500 & 1480.0 & 3986.4 & 3032.0 & 78.19 & 16.38 & 0.93\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for n_sample in [500, 1000, 1500]:\n",
    "    iterations_n_unique_tokens = []\n",
    "    iterations_n_lemmas = []\n",
    "    iterations_avg_unique_sentences = []\n",
    "    iterations_avg_unique_words_in_k_words = []\n",
    "    iterations_avg_levenshtein_distance = []\n",
    "    iterations_avg_levenshtein_distance_norm = []\n",
    "    for it in range(5):\n",
    "        samples = [item for k in range(\n",
    "            int(n_sample / 500)) for item in dataset[\"real\"][it][k]]\n",
    "        n_unique_tokens = count_unique_tokens(\n",
    "            [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "        n_unique_lemmas = count_unique_lemmas(\n",
    "            [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "        n_unique_sentences = count_unique_sentences(\n",
    "            [example[\"text\"] for example in samples])\n",
    "        n_unique_words_in_k_words = get_avg_unique_words_in_k_words(\n",
    "            [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "        avg_levenshtein_distance = average_word_level_levenshtein_distance(\n",
    "            samples)\n",
    "        avg_levenshtein_distance_norm = average_word_level_levenshtein_distance(\n",
    "            samples, norm=True)\n",
    "\n",
    "        iterations_n_unique_tokens.append(n_unique_tokens)\n",
    "        iterations_n_lemmas.append(n_unique_lemmas)\n",
    "        iterations_avg_unique_sentences.append(n_unique_sentences)\n",
    "        iterations_avg_unique_words_in_k_words.append(\n",
    "            n_unique_words_in_k_words)\n",
    "        iterations_avg_levenshtein_distance.append(\n",
    "            avg_levenshtein_distance)\n",
    "        iterations_avg_levenshtein_distance_norm.append(\n",
    "            avg_levenshtein_distance_norm)\n",
    "        \n",
    "    print(\"-\", \"&\", \"-\", \"&\", n_sample, \"&\",\n",
    "          round(np.mean(iterations_avg_unique_sentences), 2), \"&\",\n",
    "          round(np.mean(iterations_n_unique_tokens), 2), \"&\",\n",
    "          round(np.mean(iterations_n_lemmas), 2), \"&\",\n",
    "          round(np.mean(iterations_avg_unique_words_in_k_words), 2), \"&\",\n",
    "          round(np.mean(iterations_avg_levenshtein_distance), 2), \"&\",\n",
    "          round(np.mean(iterations_avg_levenshtein_distance_norm), 2))\n",
    "print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Token in Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prozentsatz der Artikel (GPT-3, fixed): 91.55 %\n",
      "Prozentsatz der Artikel (GPT-3, fixed): 91.67 %\n",
      "Prozentsatz der Artikel (Llama70B, fixed): 57.87 %\n",
      "Prozentsatz der Artikel (Llama70B, fixed): 57.33 %\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for few_shot_condition in FS_CONDITIONS:\n",
    "        first_tokens = [example[\"tokenized_text\"][0].pos_ for idx in range(5) for split_data in dataset_raw[\"synth\"][llm][few_shot_condition][idx] for example in split_data]\n",
    "        pos_counts = Counter(first_tokens)\n",
    "        article_percentage = (pos_counts[\"DET\"] / len(first_tokens)) * 100\n",
    "        print(f\"Prozentsatz der Artikel ({llm}, {prompting}): {round(article_percentage, 2)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prozentsatz der Artikel (Llama70B, fixed): 27.6 %\n"
     ]
    }
   ],
   "source": [
    "first_tokens = [example[\"tokenized_text\"][0].pos_ for split_idx in range(\n",
    "    5) for example in dataset_raw[\"real\"][split_idx]]\n",
    "pos_counts = Counter(first_tokens)\n",
    "article_percentage = (pos_counts[\"DET\"] / len(first_tokens)) * 100\n",
    "print(\n",
    "    f\"Prozentsatz der Artikel ({llm}, {prompting}): {round(article_percentage, 2)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
