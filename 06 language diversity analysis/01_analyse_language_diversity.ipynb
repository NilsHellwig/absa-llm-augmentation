{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d22bba-6bd8-4cf5-a82d-8902e533973c",
   "metadata": {},
   "source": [
    "# Notebook: Analyse Language Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dcfb1-6eab-4730-bda2-ca3a90d80987",
   "metadata": {},
   "source": [
    "This notebook is used to analyse the language diversity of all conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa234d8-c34b-4aa5-a156-a7faf6555811",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e001435c-ddc2-4522-9c2f-5cb7ec941aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b526110f-23cd-481d-a46e-c237e2b19721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c272eaaa-e283-4a2d-89d0-65cfa446bd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nils_hellwig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc5537f-e0b7-4d17-9d98-7c8a3c9f8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c26256-a86f-4545-94a0-da98d5cf9d01",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d62364c6-b0f8-42c7-b6ff-99da24cecb8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_REAL = 0\n",
    "N_SYNTH = 500\n",
    "LABELS_FIXED = True\n",
    "MODEL_NAME = \"Llama13B\" # \"Llama70B\", \"GPT-3\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b678461-d604-4c91-86a0-b4c843a52f49",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829019d1-c2fc-4b6b-bcfc-89ff1a568b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"Llama70B\", \"GPT-3\"]\n",
    "SAMPLE_COUNT = [500, 1000, 2000]\n",
    "SAMPLING_STRATEGY = [True, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba7e45-2617-4526-8bd4-f479848f628f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eeb129b-44e0-455f-ba71-83ac329f2abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_tokens(texts):\n",
    "    token_counts = [] \n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        token_counts.append(len(tokens))\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2254fe-973c-4513-a4ea-7040202381a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_lemmas(texts):\n",
    "    unique_lemmas = set()\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            unique_lemmas.add(token.lemma_)\n",
    "    return len(unique_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "935d6b48-6643-47b7-a634-bd454375fe53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords_and_punctuation(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if token.text.lower() not in STOP_WORDS and token.text not in string.punctuation]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def count_top_n_lemmas(texts, n):\n",
    "    lemma_counts = {}\n",
    "    for text in texts:\n",
    "        cleaned_text = remove_stopwords_and_punctuation(text)\n",
    "        doc = nlp(cleaned_text)\n",
    "        for token in doc:\n",
    "            lemma = token.lemma_\n",
    "            if lemma in lemma_counts:\n",
    "                lemma_counts[lemma] += 1\n",
    "            else:\n",
    "                lemma_counts[lemma] = 1\n",
    "    \n",
    "    sorted_lemmas = sorted(lemma_counts, key=lambda lemma: lemma_counts[lemma], reverse=True)\n",
    "    top_n_lemmas = sorted_lemmas[:n]\n",
    "    \n",
    "    return ', '.join(top_n_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a47b0424-7629-4b12-a941-d3008fb1132d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_language_statistic(n_synth, n_real, labels_fixed, model_name):\n",
    "    total_texts = []\n",
    "    unique_lemma_counts = []\n",
    "    texts_token_counts = []\n",
    "    n_aspects_total = []\n",
    "    n_implicit_aspects_total = []\n",
    "    n_explicit_aspects_total = []\n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        print(i)\n",
    "        examples_in_split = []\n",
    "    \n",
    "        # Load Real Split\n",
    "        real_path = f\"../03 dataset split/real/real_{i}.json\"\n",
    "        with open(real_path, 'r') as file:\n",
    "            real_data = json.load(file)[:n_real]\n",
    "            examples_in_split = real_data\n",
    "    \n",
    "        # Load Synth Split\n",
    "        if n_synth > 0:\n",
    "            fake_path = f\"../04 llm synthesis/synth/{model_name}/{'fixed' if labels_fixed else 'random'}/split_{i}.json\"\n",
    "            with open(fake_path, 'r') as file:\n",
    "                fake_data = json.load(file)[:n_synth]\n",
    "                examples_in_split = fake_data\n",
    "        \n",
    "        texts = [example[\"text\"] for example in examples_in_split]\n",
    "    \n",
    "    \n",
    "        # Calculate n tokens in texts\n",
    "        texts_token_count = count_tokens(texts)\n",
    "        for count in texts_token_count:\n",
    "            texts_token_counts.append(count)\n",
    "    \n",
    "        # Calcuate unique lemmas in text\n",
    "        unique_lemma_count = count_unique_lemmas(texts)\n",
    "        unique_lemma_counts.append(unique_lemma_count)\n",
    "    \n",
    "        # Calculate number of aspects (implicit+explicit)\n",
    "        n_aspects = len([tag[\"text\"] for example in examples_in_split for tag in example[\"tags\"]])\n",
    "        n_aspects_total.append(n_aspects)\n",
    "    \n",
    "    \n",
    "        # Calculate number of implicit aspects\n",
    "        n_implicit_aspects = len([tag[\"text\"] for example in examples_in_split for tag in example[\"tags\"] if tag[\"text\"] is None])\n",
    "        n_implicit_aspects_total.append(n_implicit_aspects)\n",
    "    \n",
    "        # Calculate number of unique aspect terms\n",
    "        explicit_aspects = [tag[\"text\"] for example in examples_in_split for tag in example[\"tags\"] if tag[\"text\"] is not None]\n",
    "        n_unique_aspect_terms = len(set(explicit_aspects))\n",
    "        n_explicit_aspects_total.append(n_unique_aspect_terms)\n",
    "    \n",
    "    \n",
    "        # Add to total text collection\n",
    "        total_texts.extend(texts)\n",
    "        \n",
    "    top_n_lemmas = count_top_n_lemmas(total_texts, 5)\n",
    "    unique_lemmas_avg = np.mean(unique_lemma_counts)\n",
    "    texts_token_counts_avg = np.mean(texts_token_counts)\n",
    "    n_aspects_avg = np.mean(n_aspects_total)\n",
    "    n_implicit_aspects_avg = np.mean(n_implicit_aspects_total) / (np.mean(n_implicit_aspects_total) + np.mean(n_explicit_aspects_total))\n",
    "    n_explicit_aspects_avg = np.mean(n_explicit_aspects_total) / (np.mean(n_implicit_aspects_total) + np.mean(n_explicit_aspects_total))\n",
    "    \n",
    "    statistic = {\n",
    "      \"condition\": f\"{n_synth} fake\" if n_synth > 0 else (f\"{n_real} fake\" if n_real > 0 else \"unknown condition\"),\n",
    "      \"llm\": model_name,\n",
    "      \"few-shot examples\": \"fixed\" if labels_fixed else \"random\",\n",
    "      \"top_n_lemmas\": top_n_lemmas,\n",
    "      \"unique_lemmas_avg\": unique_lemmas_avg,\n",
    "      \"texts_token_counts_avg\": texts_token_counts_avg,\n",
    "      \"n_aspects_avg\": n_aspects_avg,\n",
    "      \"n_implicit_aspects_avg\": n_implicit_aspects_avg,\n",
    "      \"n_explicit_aspects_avg\": n_explicit_aspects_avg,\n",
    "    }\n",
    "\n",
    "    return statistic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41d6e56e-3bfd-4218-b38b-048d224d6f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4444e027-4b67-4373-b9b5-38bea8574ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for synth_count in SAMPLE_COUNT:\n",
    "    for sampling_strategy in SAMPLING_STRATEGY:\n",
    "        for model in MODELS:\n",
    "            pass\n",
    "            #statistics.append(get_language_statistic(synth_count, 0, sampling_strategy, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02791222-a855-4263-93a3-b2a0b8da0cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1000\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "2000\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for real_count in SAMPLE_COUNT:\n",
    "    # model and sampling strategy are irrelevant. only real ones are considered anyway\n",
    "    print(real_count)\n",
    "    statistics.append(get_language_statistic(0, real_count, False, MODELS[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bdac3de-6fc0-4f9a-ac7a-bf046ef75bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'condition': '500 fake',\n",
       "  'llm': 'Llama70B',\n",
       "  'few-shot examples': 'random',\n",
       "  'top_n_lemmas': 'Kellner, 1, zuständig, fühlen, Getränk',\n",
       "  'unique_lemmas_avg': 120.0,\n",
       "  'texts_token_counts_avg': 13.4792,\n",
       "  'n_aspects_avg': 700.2,\n",
       "  'n_implicit_aspects_avg': 0.0,\n",
       "  'n_explicit_aspects_avg': 1.0},\n",
       " {'condition': '1000 fake',\n",
       "  'llm': 'Llama70B',\n",
       "  'few-shot examples': 'random',\n",
       "  'top_n_lemmas': 'Kellner, 1, zuständig, fühlen, Getränk',\n",
       "  'unique_lemmas_avg': 120.0,\n",
       "  'texts_token_counts_avg': 13.4792,\n",
       "  'n_aspects_avg': 700.2,\n",
       "  'n_implicit_aspects_avg': 0.0,\n",
       "  'n_explicit_aspects_avg': 1.0},\n",
       " {'condition': '2000 fake',\n",
       "  'llm': 'Llama70B',\n",
       "  'few-shot examples': 'random',\n",
       "  'top_n_lemmas': 'Kellner, 1, zuständig, fühlen, Getränk',\n",
       "  'unique_lemmas_avg': 120.0,\n",
       "  'texts_token_counts_avg': 13.4792,\n",
       "  'n_aspects_avg': 700.2,\n",
       "  'n_implicit_aspects_avg': 0.0,\n",
       "  'n_explicit_aspects_avg': 1.0}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4ba0132-6de0-432d-adaf-ae7f08bd3b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"language_statistics.json\", 'w') as json_file:\n",
    "    json.dump(statistics, json_file, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a0c42-fb87-4113-a035-059a7304ab70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
