{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d22bba-6bd8-4cf5-a82d-8902e533973c",
   "metadata": {},
   "source": [
    "# Notebook: Analyse Language Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dcfb1-6eab-4730-bda2-ca3a90d80987",
   "metadata": {},
   "source": [
    "This notebook is used to analyse the language diversity of all conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa234d8-c34b-4aa5-a156-a7faf6555811",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dbaa62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../07 train classifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e001435c-ddc2-4522-9c2f-5cb7ec941aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from load_dataset_folds import load_dataset_folds\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c272eaaa-e283-4a2d-89d0-65cfa446bd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nils_hellwig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20356b0",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9691d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_CATEGORIES = [\"FOOD\", \"SERVICE\", \"PRICE\", \"GENERAL-IMPRESSION\", \"AMBIENCE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba7e45-2617-4526-8bd4-f479848f628f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7eeb129b-44e0-455f-ba71-83ac329f2abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_tokens(texts):\n",
    "    token_counts = [] \n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        token_counts.append(len(tokens))\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5e2254fe-973c-4513-a4ea-7040202381a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_lemmas(texts):\n",
    "    unique_lemmas = set()\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            unique_lemmas.add(token.lemma_)\n",
    "    return len(unique_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "935d6b48-6643-47b7-a634-bd454375fe53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords_and_punctuation(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if token.text.lower() not in STOP_WORDS and token.text not in string.punctuation]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def count_top_n_lemmas(texts, n):\n",
    "    lemma_counts = {}\n",
    "    for text in texts:\n",
    "        cleaned_text = remove_stopwords_and_punctuation(text)\n",
    "        doc = nlp(cleaned_text)\n",
    "        for token in doc:\n",
    "            lemma = token.lemma_\n",
    "            if lemma in lemma_counts:\n",
    "                lemma_counts[lemma] += 1\n",
    "            else:\n",
    "                lemma_counts[lemma] = 1\n",
    "    \n",
    "    sorted_lemmas = sorted(lemma_counts, key=lambda lemma: lemma_counts[lemma], reverse=True)\n",
    "    top_n_lemmas = sorted_lemmas[:n]\n",
    "    \n",
    "    return ', '.join(top_n_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a47b0424-7629-4b12-a941-d3008fb1132d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_language_statistic(n_synth, n_real, labels_fixed, model_name, splits):\n",
    "    total_texts = []\n",
    "    unique_lemma_counts = []\n",
    "    texts_token_counts = []\n",
    "    n_unique_explicit_aspects_total = []\n",
    "    total_llm_invalid_xml_schema = 0\n",
    "    total_llm_invalid_xml_tags = 0\n",
    "    total_llm_aspect_polarity_in_text_but_not_in_label = 0\n",
    "    total_llm_more_than_one_sentences = 0\n",
    "    total_llm_no_german_language = 0\n",
    "\n",
    "    n_unique_explicit_aspects_total_aspect = {}\n",
    "\n",
    "    for i in range(0, len(splits)):\n",
    "        examples_in_split = splits[i]\n",
    "\n",
    "        texts = [example[\"text\"] for example in examples_in_split]\n",
    "\n",
    "        # Calculate n tokens in texts\n",
    "        texts_token_count = count_tokens(texts)\n",
    "        for count in texts_token_count:\n",
    "            texts_token_counts.append(count)\n",
    "\n",
    "        # Calcuate unique lemmas in text\n",
    "        unique_lemma_count = count_unique_lemmas(texts)\n",
    "        unique_lemma_counts.append(unique_lemma_count)\n",
    "\n",
    "        # Calculate number of unique aspect terms\n",
    "        explicit_aspects = [\n",
    "            tag[\"text\"] for example in examples_in_split for tag in example[\"tags\"] if tag[\"type\"] == \"explicit\"]\n",
    "        n_unique_aspect_terms = len(set(explicit_aspects))\n",
    "        n_unique_explicit_aspects_total.append(n_unique_aspect_terms)\n",
    "\n",
    "        for ac in ASPECT_CATEGORIES:\n",
    "            explicit_aspects = [tag[\"text\"] for example in examples_in_split for tag in example[\"tags\"] if tag[\"type\"] == \"explicit\" and tag[\"label\"] == ac]\n",
    "            n_unique_aspect_terms = len(set(explicit_aspects))\n",
    "            if f\"n_unique_aspect_terms_{ac}\" in n_unique_explicit_aspects_total_aspect:\n",
    "                n_unique_explicit_aspects_total_aspect[f\"n_unique_aspect_terms_{ac}\"].append(n_unique_aspect_terms)\n",
    "            else:\n",
    "                n_unique_explicit_aspects_total_aspect[f\"n_unique_aspect_terms_{ac}\"] = [n_unique_aspect_terms]\n",
    "\n",
    "\n",
    "\n",
    "        if model_name != None:\n",
    "            # Calculate number of retries for example\n",
    "            for example in examples_in_split:\n",
    "                total_llm_invalid_xml_schema += example[\"llm_invalid_xml_schema\"]\n",
    "                total_llm_invalid_xml_tags += example[\"llm_invalid_xml_tags\"]\n",
    "                total_llm_aspect_polarity_in_text_but_not_in_label += example[\n",
    "                \"llm_aspect_polarity_in_text_but_not_in_label\"]\n",
    "                total_llm_more_than_one_sentences += example[\"llm_more_than_one_sentences\"]\n",
    "                total_llm_no_german_language += example[\"llm_no_german_language\"]\n",
    "\n",
    "        # Add to total text collection\n",
    "        total_texts.extend(texts)\n",
    "\n",
    "    top_n_lemmas = count_top_n_lemmas(total_texts, 5)\n",
    "    unique_lemmas_avg = np.mean(unique_lemma_counts)\n",
    "    texts_token_counts_avg = np.mean(texts_token_counts)\n",
    "    texts_token_counts_sd = np.std(texts_token_counts)\n",
    "\n",
    "    statistic = {\n",
    "        \"n_real\": n_real,\n",
    "        \"n_synth\": n_synth,\n",
    "        \"llm\": model_name,\n",
    "        \"few-shot examples\": \"fixed\" if labels_fixed else \"random\",\n",
    "        \"top_n_lemmas\": top_n_lemmas,\n",
    "        \"unique_lemmas_avg\": unique_lemmas_avg,\n",
    "        \"avg_number_of_tokens_in_example_text\": texts_token_counts_avg,\n",
    "        \"sd_number_of_tokens_in_example_text\": texts_token_counts_sd,\n",
    "        \"n_unique_explicit_aspects_total\": np.mean(n_unique_explicit_aspects_total),\n",
    "        # Summe aller invaliden retries Ã¼ber alle 5 folds hinweg\n",
    "        \"total_llm_invalid_xml_schema\": total_llm_invalid_xml_schema,\n",
    "        \"total_llm_invalid_xml_tags\": total_llm_invalid_xml_tags,\n",
    "        \"total_llm_aspect_polarity_in_text_but_not_in_label\": total_llm_aspect_polarity_in_text_but_not_in_label,\n",
    "        \"total_llm_more_than_one_sentences\": total_llm_more_than_one_sentences,\n",
    "        \"total_llm_no_german_language\": total_llm_no_german_language,\n",
    "        \"total_llm_retries\": total_llm_invalid_xml_schema + total_llm_invalid_xml_tags + total_llm_aspect_polarity_in_text_but_not_in_label + total_llm_more_than_one_sentences + total_llm_no_german_language\n",
    "    }\n",
    "\n",
    "    for key in n_unique_explicit_aspects_total_aspect.keys():\n",
    "        statistic[key] = np.mean(n_unique_explicit_aspects_total_aspect[key])\n",
    "\n",
    "    return statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "41d6e56e-3bfd-4218-b38b-048d224d6f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "859883f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 5\n",
      "500 5\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_dataset_folds(\"Llama13B\", 2000, 0, \"random\")\n",
    "real_split_counts = [500, 1000, 1500]\n",
    "\n",
    "for i in range(len(real_split_counts)):\n",
    "    splits = train_dataset[:1]\n",
    "    splits[0] = splits[0][0: real_split_counts[i]]\n",
    "\n",
    "    statistic = get_language_statistic(\n",
    "        0, real_split_counts[i], False, None, splits)\n",
    "    statistics.append(statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0802ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-Do: FÃ¼r die Synthetischen immer die Train splits betrachten\n",
    "# To-Do: Je nach Modell unterschiedlichen Tokenizer nehmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "265032a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 5\n",
      "500 5\n",
      "llm: Llama13B |synth_size: 500 |few-shot-fixed: True |n_synth: 5\n",
      "llm: Llama13B |synth_size: 1000 |few-shot-fixed: True |n_synth: 5\n",
      "llm: Llama13B |synth_size: 1500 |few-shot-fixed: True |n_synth: 5\n",
      "5 5\n",
      "500 5\n",
      "llm: Llama13B |synth_size: 500 |few-shot-fixed: False |n_synth: 5\n",
      "llm: Llama13B |synth_size: 1000 |few-shot-fixed: False |n_synth: 5\n",
      "llm: Llama13B |synth_size: 1500 |few-shot-fixed: False |n_synth: 5\n"
     ]
    }
   ],
   "source": [
    "llms = [\"Llama13B\"]\n",
    "synth_splits = [500, 1000, 1500]\n",
    "few_shot_examples_conditions = [True, False]\n",
    "\n",
    "\n",
    "for model_name in llms:\n",
    "    for label_fixed in few_shot_examples_conditions:\n",
    "        # 1990 is the number of synthetic examples that were synthesized for each of the five real splits\n",
    "        train_dataset, test_dataset = load_dataset_folds(\n",
    "            model_name, 0, 1990 if label_fixed else 2000, \"fixed\" if label_fixed else \"random\")\n",
    "        # exclude real samples from training\n",
    "        train_dataset = [[example for example in split if \"model\" in example]\n",
    "                         for split in train_dataset]\n",
    "\n",
    "        for split_size in synth_splits:\n",
    "            splits = [subset[0:split_size] for subset in train_dataset]\n",
    "            statistic = get_language_statistic(\n",
    "                0, split_size, label_fixed, model_name, splits)\n",
    "            statistics.append(statistic)\n",
    "            print(\"llm:\", model_name, \"|synth_size:\", split_size, \"|few-shot-fixed:\", label_fixed, \"|n_synth:\", len(splits[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "724d8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"language_statistics.json\", 'w') as json_file:\n",
    "    json.dump(statistics, json_file, indent=4) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
