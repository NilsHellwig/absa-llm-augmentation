{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d22bba-6bd8-4cf5-a82d-8902e533973c",
   "metadata": {},
   "source": [
    "# Notebook: Analyse Language Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dcfb1-6eab-4730-bda2-ca3a90d80987",
   "metadata": {},
   "source": [
    "This notebook is used to analyse the language diversity of all conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa234d8-c34b-4aa5-a156-a7faf6555811",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbaa62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../07 train classifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e001435c-ddc2-4522-9c2f-5cb7ec941aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from load_dataset_folds import load_dataset_folds\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c272eaaa-e283-4a2d-89d0-65cfa446bd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nils_hellwig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20356b0",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9691d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_CATEGORIES = [\"FOOD\", \"SERVICE\", \"PRICE\", \"GENERAL-IMPRESSION\", \"AMBIENCE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba7e45-2617-4526-8bd4-f479848f628f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eeb129b-44e0-455f-ba71-83ac329f2abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_tokens(texts):\n",
    "    token_counts = [] \n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        token_counts.append(len(tokens))\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e2254fe-973c-4513-a4ea-7040202381a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_lemmas(texts):\n",
    "    unique_lemmas = set()\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            unique_lemmas.add(token.lemma_)\n",
    "    return len(unique_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "935d6b48-6643-47b7-a634-bd454375fe53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords_and_punctuation(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if token.text.lower() not in STOP_WORDS and token.text not in string.punctuation]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def count_top_n_lemmas(texts, n):\n",
    "    lemma_counts = {}\n",
    "    for text in texts:\n",
    "        cleaned_text = remove_stopwords_and_punctuation(text)\n",
    "        doc = nlp(cleaned_text)\n",
    "        for token in doc:\n",
    "            lemma = token.lemma_\n",
    "            if lemma in lemma_counts:\n",
    "                lemma_counts[lemma] += 1\n",
    "            else:\n",
    "                lemma_counts[lemma] = 1\n",
    "    \n",
    "    sorted_lemmas = sorted(lemma_counts, key=lambda lemma: lemma_counts[lemma], reverse=True)\n",
    "    top_n_lemmas = sorted_lemmas[:n]\n",
    "    \n",
    "    return ', '.join(top_n_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47b0424-7629-4b12-a941-d3008fb1132d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_language_statistic(n_synth, n_real, labels_fixed, model_name, splits):\n",
    "    total_texts = []\n",
    "    unique_lemma_counts = []\n",
    "    texts_token_counts = []\n",
    "    n_unique_explicit_aspects_total = []\n",
    "    total_llm_invalid_xml_schema = 0\n",
    "    total_llm_invalid_xml_tags = 0\n",
    "    total_llm_aspect_polarity_in_text_but_not_in_label = 0\n",
    "    total_llm_more_than_one_sentences = 0\n",
    "    total_llm_no_german_language = 0\n",
    "    total_llm_no_quotes = 0\n",
    "\n",
    "    n_unique_explicit_aspects_total_aspect = {}\n",
    "\n",
    "    for i in range(0, len(splits)):\n",
    "        examples_in_split = splits[i]\n",
    "\n",
    "        texts = [example[\"text\"] for example in examples_in_split]\n",
    "\n",
    "        # Calculate n tokens in texts\n",
    "        texts_token_count = count_tokens(texts)\n",
    "        for count in texts_token_count:\n",
    "            texts_token_counts.append(count)\n",
    "\n",
    "        # Calcuate unique lemmas in text\n",
    "        unique_lemma_count = count_unique_lemmas(texts)\n",
    "        unique_lemma_counts.append(unique_lemma_count)\n",
    "\n",
    "        # Calculate number of unique aspect terms\n",
    "        explicit_aspects = [\n",
    "            tag[\"text\"] for example in examples_in_split for tag in example[\"tags\"] if tag[\"type\"] == \"explicit\"]\n",
    "        n_unique_aspect_terms = len(set(explicit_aspects))\n",
    "        n_unique_explicit_aspects_total.append(n_unique_aspect_terms)\n",
    "\n",
    "        for ac in ASPECT_CATEGORIES:\n",
    "            explicit_aspects = [tag[\"text\"] for example in examples_in_split for tag in example[\"tags\"] if tag[\"type\"] == \"explicit\" and tag[\"label\"] == ac]\n",
    "            n_unique_aspect_terms = len(set(explicit_aspects))\n",
    "            if f\"n_unique_aspect_terms_{ac}\" in n_unique_explicit_aspects_total_aspect:\n",
    "                n_unique_explicit_aspects_total_aspect[f\"n_unique_aspect_terms_{ac}\"].append(n_unique_aspect_terms)\n",
    "            else:\n",
    "                n_unique_explicit_aspects_total_aspect[f\"n_unique_aspect_terms_{ac}\"] = [n_unique_aspect_terms]\n",
    "\n",
    "\n",
    "\n",
    "        if model_name != None:\n",
    "            # Calculate number of retries for example\n",
    "            for example in examples_in_split:\n",
    "                total_llm_invalid_xml_schema += example[\"llm_invalid_xml_schema\"]\n",
    "                total_llm_invalid_xml_tags += example[\"llm_invalid_xml_tags\"]\n",
    "                total_llm_aspect_polarity_in_text_but_not_in_label += example[\n",
    "                \"llm_aspect_polarity_in_text_but_not_in_label\"]\n",
    "                total_llm_more_than_one_sentences += example[\"llm_more_than_one_sentences\"]\n",
    "                total_llm_no_german_language += example[\"llm_no_german_language\"]\n",
    "                total_llm_no_quotes += example[\"llm_no_quotes\"]\n",
    "\n",
    "        # Add to total text collection\n",
    "        total_texts.extend(texts)\n",
    "\n",
    "    top_n_lemmas = count_top_n_lemmas(total_texts, 5)\n",
    "    unique_lemmas_avg = np.mean(unique_lemma_counts)\n",
    "    texts_token_counts_avg = np.mean(texts_token_counts)\n",
    "    texts_token_counts_sd = np.std(texts_token_counts)\n",
    "\n",
    "    statistic = {\n",
    "        \"n_real\": n_real,\n",
    "        \"n_synth\": n_synth,\n",
    "        \"llm\": model_name,\n",
    "        \"few-shot examples\": \"fixed\" if labels_fixed else \"random\",\n",
    "        \"top_n_lemmas\": top_n_lemmas,\n",
    "        \"unique_lemmas_avg\": unique_lemmas_avg,\n",
    "        \"avg_number_of_tokens_in_example_text\": texts_token_counts_avg,\n",
    "        \"sd_number_of_tokens_in_example_text\": texts_token_counts_sd,\n",
    "        \"n_unique_explicit_aspects_total\": np.mean(n_unique_explicit_aspects_total),\n",
    "        # Summe aller invaliden retries Ã¼ber alle 5 folds hinweg\n",
    "        \"total_llm_invalid_xml_schema\": total_llm_invalid_xml_schema,\n",
    "        \"total_llm_invalid_xml_tags\": total_llm_invalid_xml_tags,\n",
    "        \"total_llm_aspect_polarity_in_text_but_not_in_label\": total_llm_aspect_polarity_in_text_but_not_in_label,\n",
    "        \"total_llm_more_than_one_sentences\": total_llm_more_than_one_sentences,\n",
    "        \"total_llm_no_german_language\": total_llm_no_german_language,\n",
    "        \"total_llm_no_quotes\": total_llm_no_quotes,\n",
    "        \"total_llm_retries\": total_llm_invalid_xml_schema + total_llm_invalid_xml_tags + total_llm_aspect_polarity_in_text_but_not_in_label + total_llm_more_than_one_sentences + total_llm_no_german_language + total_llm_no_quotes\n",
    "    }\n",
    "\n",
    "    for key in n_unique_explicit_aspects_total_aspect.keys():\n",
    "        statistic[key] = np.mean(n_unique_explicit_aspects_total_aspect[key])\n",
    "\n",
    "    return statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d6e56e-3bfd-4218-b38b-048d224d6f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "859883f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 5\n",
      "500 5\n"
     ]
    }
   ],
   "source": [
    "# Load real examples and save statistics\n",
    "train_dataset, test_dataset = load_dataset_folds(\"Llama13B\", 2000, 0, \"random\")\n",
    "real_split_counts = [500, 1000, 1500]\n",
    "\n",
    "for i in range(len(real_split_counts)):\n",
    "    splits = train_dataset[:1]\n",
    "    splits[0] = splits[0][0: real_split_counts[i]]\n",
    "\n",
    "    statistic = get_language_statistic(0, real_split_counts[i], False, None, splits)\n",
    "    statistics.append(statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "265032a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../07 train classifier/synth/Llama13B/fixed/split_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nils_hellwig/Desktop/Studium/Masterstudium WS 2023:24/Masterarbeit/absa-llm-augmentation/06 language diversity analysis/01_analyse_language_diversity.ipynb Zelle 16\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nils_hellwig/Desktop/Studium/Masterstudium%20WS%202023%3A24/Masterarbeit/absa-llm-augmentation/06%20language%20diversity%20analysis/01_analyse_language_diversity.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m model_name \u001b[39min\u001b[39;00m llms:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nils_hellwig/Desktop/Studium/Masterstudium%20WS%202023%3A24/Masterarbeit/absa-llm-augmentation/06%20language%20diversity%20analysis/01_analyse_language_diversity.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m label_fixed \u001b[39min\u001b[39;00m few_shot_examples_conditions:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nils_hellwig/Desktop/Studium/Masterstudium%20WS%202023%3A24/Masterarbeit/absa-llm-augmentation/06%20language%20diversity%20analysis/01_analyse_language_diversity.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m# 1975 is the number of synthetic examples that were synthesized for each of the five real splits\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nils_hellwig/Desktop/Studium/Masterstudium%20WS%202023%3A24/Masterarbeit/absa-llm-augmentation/06%20language%20diversity%20analysis/01_analyse_language_diversity.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         train_dataset, test_dataset \u001b[39m=\u001b[39m load_dataset_folds(model_name, \u001b[39m0\u001b[39;49m, \u001b[39m1975\u001b[39;49m \u001b[39mif\u001b[39;49;00m label_fixed \u001b[39melse\u001b[39;49;00m \u001b[39m1500\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfixed\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m label_fixed \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mrandom\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nils_hellwig/Desktop/Studium/Masterstudium%20WS%202023%3A24/Masterarbeit/absa-llm-augmentation/06%20language%20diversity%20analysis/01_analyse_language_diversity.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m# exclude real samples from training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nils_hellwig/Desktop/Studium/Masterstudium%20WS%202023%3A24/Masterarbeit/absa-llm-augmentation/06%20language%20diversity%20analysis/01_analyse_language_diversity.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         train_dataset \u001b[39m=\u001b[39m [[example \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m split \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m example]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nils_hellwig/Desktop/Studium/Masterstudium%20WS%202023%3A24/Masterarbeit/absa-llm-augmentation/06%20language%20diversity%20analysis/01_analyse_language_diversity.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                          \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m train_dataset]\n",
      "File \u001b[0;32m~/Desktop/Studium/Masterstudium WS 2023:24/Masterarbeit/absa-llm-augmentation/07 train classifier/load_dataset_folds.py:69\u001b[0m, in \u001b[0;36mload_dataset_folds\u001b[0;34m(LLM_NAME, N_REAL, N_SYNTH, LLM_SAMPLING)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m N_SYNTH \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     68\u001b[0m     \u001b[39mfor\u001b[39;00m split_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(constants\u001b[39m.\u001b[39mN_FOLDS):\n\u001b[0;32m---> 69\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../07 train classifier/synth/\u001b[39m\u001b[39m{\u001b[39;00mLLM_NAME\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mLLM_SAMPLING\u001b[39m}\u001b[39;00m\u001b[39m/split_\u001b[39m\u001b[39m{\u001b[39;00mconstants\u001b[39m.\u001b[39mSPLIT_LOOP[split_id\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m json_datei:\n\u001b[1;32m     70\u001b[0m             synth_split \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(json_datei)[:N_SYNTH]\n\u001b[1;32m     71\u001b[0m             \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m synth_split:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../07 train classifier/synth/Llama13B/fixed/split_1.json'"
     ]
    }
   ],
   "source": [
    "llms = [\"Llama13B\"]\n",
    "synth_splits = [500, 1000, 1500]\n",
    "few_shot_examples_conditions = [True, False]\n",
    "\n",
    "\n",
    "for model_name in llms:\n",
    "    for label_fixed in few_shot_examples_conditions:\n",
    "        # 1975 is the number of synthetic examples that were synthesized for each of the five real splits\n",
    "        train_dataset, test_dataset = load_dataset_folds(model_name, 0, 1975 if label_fixed else 1500, \"fixed\" if label_fixed else \"random\")\n",
    "        # exclude real samples from training\n",
    "        train_dataset = [[example for example in split if \"model\" in example]\n",
    "                         for split in train_dataset]\n",
    "        print(len(train_dataset))\n",
    "\n",
    "        for split_size in synth_splits:\n",
    "            splits = [subset[0:split_size] for subset in train_dataset]\n",
    "            statistic = get_language_statistic(\n",
    "                0, split_size, label_fixed, model_name, splits)\n",
    "            statistics.append(statistic)\n",
    "            print(\"llm:\", model_name, \"|synth_size:\", split_size, \"|few-shot-fixed:\", label_fixed, \"|n_synth:\", len(splits[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"language_statistics.json\", 'w') as json_file:\n",
    "    json.dump(statistics, json_file, indent=4) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
