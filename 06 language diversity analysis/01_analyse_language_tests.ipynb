{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Analyse Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_CATEGORIES = [\"GENERAL-IMPRESSION\",\n",
    "                     \"FOOD\", \"SERVICE\", \"AMBIENCE\", \"PRICE\"]\n",
    "LLMS = [\"GPT-3\", \"Llama70B\"]\n",
    "FS_CONDITIONS = [\"fixed\", \"random\"]\n",
    "PROMPTING_ENCODING = {\"fixed\": \"25 fixed examples\",\n",
    "                      \"random\": \"25 random examples\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(texts):\n",
    "    token_counts = [] \n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        token_counts.append(len(tokens))\n",
    "    return token_counts\n",
    "\n",
    "def count_unique_lemmas(texts):\n",
    "    unique_lemmas = set()\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            unique_lemmas.add(token.lemma_)\n",
    "    return len(unique_lemmas)\n",
    "\n",
    "def remove_stopwords_and_punctuation(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if token.text.lower() not in STOP_WORDS and token.text not in string.punctuation and token.text.isalpha()]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def count_top_n_lemmas(texts, n):\n",
    "    lemma_counts = {}\n",
    "    for text in texts:\n",
    "        cleaned_text = remove_stopwords_and_punctuation(text)\n",
    "        doc = nlp(cleaned_text)\n",
    "        for token in doc:\n",
    "            lemma = token.lemma_\n",
    "            if lemma in lemma_counts:\n",
    "                lemma_counts[lemma] += 1\n",
    "            else:\n",
    "                lemma_counts[lemma] = 1\n",
    "    \n",
    "    sorted_lemmas = sorted(lemma_counts, key=lambda lemma: lemma_counts[lemma], reverse=True)\n",
    "    top_n_lemmas = sorted_lemmas[:n]\n",
    "    \n",
    "    return ', '.join(top_n_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\"synth\": {}, \"real\": []}\n",
    "\n",
    "# Load Synth\n",
    "for llm in LLMS:\n",
    "    dataset[\"synth\"][llm] = {}\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        dataset[\"synth\"][llm][prompting] = []\n",
    "        for split in range(5):\n",
    "            with open(f\"../07 train models/synth/{llm}/{prompting}/split_{split}.json\", 'r', encoding='utf-8') as json_file:\n",
    "                split_data = json.load(json_file)\n",
    "            for example in split_data:\n",
    "                example[\"tokenized_text\"] = nlp(example[\"text\"])\n",
    "            dataset[\"synth\"][llm][prompting].append(split_data)\n",
    "\n",
    "# Load Real\n",
    "for split in range(5):\n",
    "    with open(f\"../07 train models/real/split_{split}.json\", 'r', encoding='utf-8') as json_file:\n",
    "        split_data = json.load(json_file)\n",
    "    for example in split_data:\n",
    "        example[\"tokenized_text\"] = nlp(example[\"text\"])\n",
    "    dataset[\"real\"].append(split_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Avg Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for llm in LLMS:\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        print(\"-----\")\n",
    "        print(llm, prompting, round(np.mean(count_tokens(\n",
    "            [example[\"text\"] for split_data in dataset[\"synth\"][llm][prompting] for example in split_data])), 2))\n",
    "        print(llm, prompting, [round(np.mean(count_tokens(\n",
    "            [example[\"text\"] for example in dataset[\"synth\"][llm][prompting][split_id]])), 2) for split_id in range(0, 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Real\", round(np.mean(count_tokens([example[\"text\"] for split_examples in dataset[\"real\"] for example in split_examples])), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Text Similarity: Sampling Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token.text for split_examples in dataset[\"real\"] for example in split_examples for token in example[\"tokenized_text\"]]\n",
    "word_distribution = Counter(tokenized_texts)\n",
    "len(word_distribution.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def calculate_average_unique_words(word_distribution, num_draws):\n",
    "    total_words = sum(word_distribution.values())\n",
    "    all_words = list(word_distribution.keys())\n",
    "\n",
    "    possible_combinations = list(combinations(all_words, num_draws))\n",
    "\n",
    "    unique_combinations = set()\n",
    "    for combination in possible_combinations:\n",
    "        unique_combinations.add(tuple(sorted(combination)))\n",
    "\n",
    "    num_possible_combinations = len(possible_combinations)\n",
    "    num_unique_combinations = len(unique_combinations)\n",
    "\n",
    "    average_unique_words = num_unique_combinations / num_possible_combinations\n",
    "\n",
    "    return average_unique_words\n",
    "\n",
    "num_draws = 10\n",
    "\n",
    "#average_unique_words = calculate_average_unique_words(word_distribution, num_draws)\n",
    "\n",
    "#print(f\"Durchschnittliche Anzahl einzigartiger Wörter bei {num_draws} Zügen: {average_unique_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Text Similarity: Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_level_levenshtein_distance(docs):\n",
    "    tokenized_texts = [[token.text for token in doc] for doc in docs]\n",
    "\n",
    "    total_distance = 0\n",
    "    pair_count = 0\n",
    "\n",
    "    for i in range(len(tokenized_texts)):\n",
    "        for j in range(i + 1, len(tokenized_texts)):\n",
    "            tokens1 = tokenized_texts[i]\n",
    "            tokens2 = tokenized_texts[j]\n",
    "\n",
    "            if len(tokens1) >= len(tokens2):\n",
    "                max_tokens = len(tokens1)\n",
    "            else:\n",
    "                max_tokens = len(tokens2)\n",
    "\n",
    "            distance = Levenshtein.distance(tokens1, tokens2)\n",
    "            total_distance += distance\n",
    "            pair_count += 1\n",
    "\n",
    "    average_distance = total_distance / pair_count if pair_count > 0 else 0\n",
    "    return average_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for llm in LLMS:\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        for split_idx in range(5):\n",
    "            texts_in_split = [example[\"tokenized_text\"] for example in dataset[\"synth\"][llm][prompting][split_idx]]\n",
    "            print(llm, prompting, split_idx, average_word_level_levenshtein_distance(texts_in_split))\n",
    "\n",
    "            # for ac in ASPECT_CATEGORIES:\n",
    "            #     tags_in_split = [nlp(tag[\"text\"]) for example in dataset[\"synth\"][llm][prompting][split_idx] for tag in example[\"tags\"] if tag[\"type\"] == \"label-explicit\" and tag[\"label\"] == ac]\n",
    "            #     print(llm, prompting, split_idx, ac, average_word_level_cosine_similarity(tags_in_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent terms\n",
    "\n",
    "Ähnlich wie bei den realen Daten Aspekte, die das Aspekt selber benennen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(456.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for llm in LLMS:\n",
    "    print(llm)\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        for ac_idx, aspect_category in enumerate(ASPECT_CATEGORIES):\n",
    "            aspect_terms_all_splits = []\n",
    "            n_aspects_in_splits = []\n",
    "            n_aspect_terms_in_splits = []\n",
    "            n_unique_aspects_in_split = []\n",
    "\n",
    "            for split_idx in range(5):\n",
    "                aspects_in_split = [tag for example in dataset[\"synth\"][llm][prompting][split_idx]\n",
    "                                    for tag in example[\"tags\"] if tag[\"label\"] == aspect_category]\n",
    "                aspect_terms_in_split = [\n",
    "                    tag[\"text\"] for tag in aspects_in_split if tag[\"type\"] == \"label-explicit\"]\n",
    "                aspect_terms_all_splits += aspect_terms_in_split\n",
    "                n_aspects_in_splits.append(len(aspects_in_split))\n",
    "                n_aspect_terms_in_splits.append(len(aspect_terms_in_split))\n",
    "                n_unique_aspects_in_split.append(\n",
    "                    len(list(set(aspect_terms_in_split))))\n",
    "\n",
    "            aspect_term_counts = Counter(aspect_terms_all_splits)\n",
    "            most_common_aspect_terms = aspect_term_counts.most_common(5)\n",
    "\n",
    "            term_list = [\n",
    "                f\"\\\\textit{{{term}}} ({round(count/5,2)})\" for term, count in most_common_aspect_terms]\n",
    "            term_string = \", \".join(term_list)\n",
    "\n",
    "            if ac_idx == 0:\n",
    "                print(\n",
    "                    f\"\\n {PROMPTING_ENCODING[prompting]} & {aspect_category} & {round(np.mean(n_aspect_terms_in_splits))} & {round(np.mean(n_unique_aspects_in_split), 2)} & {term_string} \\\\\\\\\")\n",
    "            elif ac_idx == 4:\n",
    "                print(\n",
    "                    f\"\\n & {aspect_category}  & {round(np.mean(n_aspect_terms_in_splits))} & {round(np.mean(n_unique_aspects_in_split), 2)} & {term_string} \\\\\\\\ \\\\hline\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"\\n & {aspect_category}  & {round(np.mean(n_aspect_terms_in_splits))} & {round(np.mean(n_unique_aspects_in_split), 2)} & {term_string} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prozentualer Anteil an Aspektbegriffen je Aspekt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for llm in LLMS:\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        aspect_terms_total = []\n",
    "        n_aspects_total = 0\n",
    "        ratio_splits = []\n",
    "        for split_idx in range(5):\n",
    "            aspect_terms_split = []\n",
    "            n_aspects_split = 0\n",
    "            for example in dataset[\"synth\"][llm][prompting][split_idx]:\n",
    "                aspect_terms = [tag[\"text\"] for tag in example[\"tags\"] if tag[\"type\"] == \"label-explicit\"]\n",
    "                n_aspects_example = len([tag for tag in example[\"tags\"]])\n",
    "\n",
    "                aspect_terms_split += aspect_terms\n",
    "                n_aspects_split += n_aspects_example\n",
    "\n",
    "        \n",
    "            aspect_terms_total += aspect_terms_split\n",
    "            n_aspects_total += n_aspects_split\n",
    "            ratio_splits.append(len(aspect_terms_split) / n_aspects_split)\n",
    "                \n",
    "        print(llm, prompting, \"total:\", len(aspect_terms_total) / n_aspects_total, \"splits:\", ratio_splits, np.std(ratio_splits), np.var(ratio_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erster Token im Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: für normale daten berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for llm in LLMS:\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        first_pos = [example[\"tokenized_text\"][0].pos_ for split_idx in range(5) for example in dataset[\"synth\"][llm][prompting][split_idx]]\n",
    "        pos_counts = Counter(first_pos)\n",
    "        article_percentage = (pos_counts[\"DET\"] / len(first_pos)) * 100\n",
    "        print(f\"Prozentsatz der Artikel ({llm}, {prompting}): {article_percentage}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
