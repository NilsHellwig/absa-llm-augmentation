{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Analyse Language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_CATEGORIES = [\"GENERAL-IMPRESSION\",\n",
    "                     \"FOOD\", \"SERVICE\", \"AMBIENCE\", \"PRICE\"]\n",
    "LLMS = [\"Llama70B\", \"GPT-3\"]\n",
    "FS_CONDITIONS = [\"fixed\", \"random\"]\n",
    "PROMPTING_ENCODING = {\"fixed\": \"25 fixed examples\",\n",
    "                      \"random\": \"25 random examples\"}\n",
    "N_FOLDS = 3\n",
    "CRITERIA_RS = \"tag_with_polarity\"\n",
    "POLARITIES = [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]\n",
    "MENTIONING_TYPE = [\"implicit\", \"explicit\"]\n",
    "COMBINATIONS = [f\"{aspect}-{polarity}\" for aspect in [\"SERVICE\", \"FOOD\",\n",
    "                                                      \"GENERAL-IMPRESSION\", \"AMBIENCE\", \"PRICE\"] for polarity in POLARITIES]\n",
    "RANDOM_STATE = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMS_ENCODED = {\"GPT-3\": \"GPT-3.5-turbo\", \"Llama70B\":\"Llama-2-70B\"}\n",
    "ENCODE_CONDITION = {\"fixed\": \"25 fixed examples\", \"random\": \"25 random examples\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nils_hellwig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "nltk.download('punkt')\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_sentences(sentences):\n",
    "    unique_sentences = set(sentences)\n",
    "    return len(unique_sentences)\n",
    "\n",
    "\n",
    "def count_unique_tokens(tokens):\n",
    "    unique_tokens = set(token.text for token in tokens)\n",
    "    return len(unique_tokens)\n",
    "\n",
    "\n",
    "def count_unique_lemmas(tokens):\n",
    "    unique_lemmas = set(token.lemma_ for token in tokens)\n",
    "    return len(unique_lemmas)\n",
    "\n",
    "\n",
    "def get_avg_unique_words_in_k_words(tokens, n_selection=100, n_repetitions=1000):\n",
    "    iterations_n_unique_words = []\n",
    "    for i in range(n_repetitions):\n",
    "        random_indices = random.sample(range(len(tokens)), n_selection)\n",
    "        random_words = [tokens[index] for index in random_indices]\n",
    "        n_unique_words = len(set(random_words))\n",
    "        iterations_n_unique_words.append(n_unique_words)\n",
    "    return np.mean(iterations_n_unique_words)\n",
    "\n",
    "\n",
    "def average_word_level_levenshtein_distance(docs, norm=False):\n",
    "    tokenized_texts = [\n",
    "        [token.text for token in doc[\"tokenized_text\"]] for doc in docs]\n",
    "\n",
    "    total_distance = 0\n",
    "    pair_count = 0\n",
    "\n",
    "    for i in range(len(tokenized_texts)):\n",
    "        for j in range(i + 1, len(tokenized_texts)):\n",
    "            tokens1 = tokenized_texts[i]\n",
    "            tokens2 = tokenized_texts[j]\n",
    "\n",
    "            if len(tokens1) >= len(tokens2):\n",
    "                max_tokens = len(tokens1)\n",
    "            else:\n",
    "                max_tokens = len(tokens2)\n",
    "\n",
    "            distance = Levenshtein.distance(tokens1, tokens2)\n",
    "            if norm:\n",
    "                distance = distance / max_tokens\n",
    "            total_distance += distance\n",
    "            pair_count += 1\n",
    "\n",
    "    average_distance = total_distance / pair_count if pair_count > 0 else 0\n",
    "    return average_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_thousand_dots(n_sample):\n",
    "    return f\"{n_sample:,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\"synth\": {}, \"real\": []}\n",
    "\n",
    "# Load Synth\n",
    "for llm in LLMS:\n",
    "    dataset[\"synth\"][llm] = {}\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        dataset[\"synth\"][llm][prompting] = []\n",
    "        for split in range(5):\n",
    "            with open(f\"../07 train models/synth/{llm}/{prompting}/split_{split}.json\", 'r', encoding='utf-8') as json_file:\n",
    "                split_data = json.load(json_file)\n",
    "            for example in split_data:\n",
    "                example[\"tokenized_text\"] = nlp(example[\"text\"])\n",
    "            dataset[\"synth\"][llm][prompting].append(split_data)\n",
    "\n",
    "# Load Real\n",
    "for split in range(6):\n",
    "    with open(f\"../07 train models/real/split_{split}.json\", 'r', encoding='utf-8') as json_file:\n",
    "        split_data = json.load(json_file)\n",
    "    for example in split_data:\n",
    "        example[\"tokenized_text\"] = nlp(example[\"text\"])\n",
    "    dataset[\"real\"].append(split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(subset):\n",
    "    labels_one_hot = []\n",
    "    for i in range(len(subset)):\n",
    "        tags_in_example = list(set([tag[CRITERIA_RS]\n",
    "                               for tag in subset[i][\"tags\"]]))\n",
    "        one_hot_encoded_combination = np.array(\n",
    "            [1 if tag in tags_in_example else 0 for tag in COMBINATIONS])\n",
    "        labels_one_hot.append(one_hot_encoded_combination)\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for llm in LLMS:\n",
    "    for few_shot_condition in FS_CONDITIONS:\n",
    "        for iteration in range(5):\n",
    "            if few_shot_condition == \"random\":\n",
    "                subset = dataset[\"synth\"][llm][few_shot_condition][iteration]\n",
    "            else:\n",
    "                subset = dataset[\"synth\"][llm][few_shot_condition][iteration][475:]\n",
    "\n",
    "            found_5_split = False\n",
    "            restart_idx = 0\n",
    "            while found_5_split == False:\n",
    "                mskf = MultilabelStratifiedKFold(\n",
    "                    n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE+restart_idx)\n",
    "                section = []\n",
    "                for train_index, test_index in mskf.split(subset, get_one_hot(subset)):\n",
    "                    split_500 = [subset[i] for i in test_index]\n",
    "                    section.append(split_500)\n",
    "\n",
    "                if len(section[0]) == 500 and len(section[1]) == 500 and len(section[2]) == 500:\n",
    "                    found_5_split = True\n",
    "\n",
    "                restart_idx += 1\n",
    "\n",
    "            dataset[\"synth\"][llm][few_shot_condition][iteration] = section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_examples = []\n",
    "for i in [0, 1, 2, 3, 4, 5]:\n",
    "    real_examples.append([])\n",
    "    for k in [0, 1, 2]:\n",
    "        if (i+k) < 6:\n",
    "            t = i+k\n",
    "        else:\n",
    "            t = i+k - 6\n",
    "        real_examples[i].append(dataset[\"real\"][t])\n",
    "dataset[\"real\"] = real_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{6}{*}{\\textbf{Llama-2-70B}} & \\multirow{3}{*}{25 fixed examples} & 500 & 480.0 & 694.4 & 536.6 \\\\\n",
      " &  & 1,000 & 934.0 & 1,023.8 & 787.0 \\\\\n",
      " &  & 1,500 & 1,383.2 & 1,269.2 & 973.0 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-6}\\arrayrulecolor{black}\n",
      " & \\multirow{3}{*}{25 random examples} & 500 & 485.4 & 751.8 & 580.2 \\\\\n",
      " &  & 1,000 & 949.4 & 1,103.0 & 846.8 \\\\\n",
      " &  & 1,500 & 1,400.0 & 1,380.4 & 1,054.0 \\\\\n",
      "\\hline\n",
      "\\multirow{6}{*}{\\textbf{GPT-3.5-turbo}} & \\multirow{3}{*}{25 fixed examples} & 500 & 307.6 & 289.6 & 208.2 \\\\\n",
      " &  & 1,000 & 549.0 & 369.6 & 264.4 \\\\\n",
      " &  & 1,500 & 769.8 & 428.4 & 307.0 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-6}\\arrayrulecolor{black}\n",
      " & \\multirow{3}{*}{25 random examples} & 500 & 317.0 & 295.4 & 217.0 \\\\\n",
      " &  & 1,000 & 561.2 & 389.4 & 281.2 \\\\\n",
      " &  & 1,500 & 782.2 & 456.6 & 328.4 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for idx_llm, llm in enumerate(LLMS):\n",
    "    for idx_fsc, few_shot_condition in enumerate(FS_CONDITIONS):\n",
    "        for idx_sample, n_sample in enumerate([500, 1000, 1500]):\n",
    "            iterations_n_unique_tokens = []\n",
    "            iterations_n_lemmas = []\n",
    "            iterations_avg_unique_sentences = []\n",
    "            for it in range(5):\n",
    "                samples = [item for k in range(\n",
    "                    int(n_sample / 500)) for item in dataset[\"synth\"][llm][few_shot_condition][it][k]]\n",
    "                n_unique_tokens = count_unique_tokens(\n",
    "                    [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "                n_unique_lemmas = count_unique_lemmas(\n",
    "                    [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "                n_unique_sentences = count_unique_sentences(\n",
    "                    [example[\"text\"] for example in samples])\n",
    "\n",
    "                iterations_n_unique_tokens.append(n_unique_tokens)\n",
    "                iterations_n_lemmas.append(n_unique_lemmas)\n",
    "                iterations_avg_unique_sentences.append(n_unique_sentences)\n",
    "\n",
    "            llm_print = \"\\multirow{6}{*}{\\\\textbf{\" + \\\n",
    "                LLMS_ENCODED[llm] + \\\n",
    "                \"}}\" if idx_sample == 0 and idx_fsc == 0 else \"\"\n",
    "            fs_condition_print = \"\\multirow{3}{*}{\" + \\\n",
    "                ENCODE_CONDITION[few_shot_condition] + \\\n",
    "                \"}\" if idx_sample == 0 else \"\"\n",
    "\n",
    "            print(llm_print, \"&\", fs_condition_print, \"&\", add_thousand_dots(n_sample), \"&\",\n",
    "                  add_thousand_dots(\n",
    "                      round(np.mean(iterations_avg_unique_sentences), 2)), \"&\",\n",
    "                  add_thousand_dots(\n",
    "                      round(np.mean(iterations_n_unique_tokens), 2)), \"&\",\n",
    "                  add_thousand_dots(round(np.mean(iterations_n_lemmas), 2)), \"\\\\\\\\\")\n",
    "        if idx_fsc == 0:\n",
    "            print(\"\\\\arrayrulecolor{gray}\\cline{2-6}\\\\arrayrulecolor{black}\")\n",
    "        else:\n",
    "            print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{6}{*}{\\textbf{Llama-2-70B}} & \\multirow{3}{*}{25 fixed examples} & 500 & 59.2 & 11.08 & 0.85\n",
      " &  & 1,000 & 59.67 & 11.09 & 0.85\n",
      " &  & 1,500 & 59.63 & 11.06 & 0.85\n",
      "\\arrayrulecolor{gray}\\cline{2-6}\\arrayrulecolor{black}\n",
      " & \\multirow{3}{*}{25 random examples} & 500 & 61.46 & 11.06 & 0.86\n",
      " &  & 1,000 & 61.33 & 11.04 & 0.86\n",
      " &  & 1,500 & 61.26 & 10.98 & 0.86\n",
      "\\hline\n",
      "\\multirow{6}{*}{\\textbf{GPT-3.5-turbo}} & \\multirow{3}{*}{25 fixed examples} & 500 & 47.82 & 9.79 & 0.79\n",
      " &  & 1,000 & 47.86 & 9.79 & 0.79\n",
      " &  & 1,500 & 48.01 & 9.8 & 0.79\n",
      "\\arrayrulecolor{gray}\\cline{2-6}\\arrayrulecolor{black}\n",
      " & \\multirow{3}{*}{25 random examples} & 500 & 48.11 & 8.81 & 0.77\n",
      " &  & 1,000 & 48.24 & 8.84 & 0.77\n",
      " &  & 1,500 & 48.25 & 8.85 & 0.77\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for idx_llm, llm in enumerate(LLMS):\n",
    "    for idx_fsc, few_shot_condition in enumerate(FS_CONDITIONS):\n",
    "        for idx_sample, n_sample in enumerate([500, 1000, 1500]):\n",
    "            iterations_avg_unique_words_in_k_words = []\n",
    "            iterations_avg_levenshtein_distance = []\n",
    "            iterations_avg_levenshtein_distance_norm = []\n",
    "            for it in range(5):\n",
    "                samples = [item for k in range(\n",
    "                    int(n_sample / 500)) for item in dataset[\"synth\"][llm][few_shot_condition][it][k]]\n",
    "                n_unique_words_in_k_words = get_avg_unique_words_in_k_words(\n",
    "                    [token.text for example in samples for token in example[\"tokenized_text\"]])\n",
    "                avg_levenshtein_distance = average_word_level_levenshtein_distance(\n",
    "                    samples)\n",
    "                avg_levenshtein_distance_norm = average_word_level_levenshtein_distance(\n",
    "                    samples, norm=True)\n",
    "\n",
    "                iterations_avg_unique_words_in_k_words.append(\n",
    "                    n_unique_words_in_k_words)\n",
    "                iterations_avg_levenshtein_distance.append(\n",
    "                    avg_levenshtein_distance)\n",
    "                iterations_avg_levenshtein_distance_norm.append(\n",
    "                    avg_levenshtein_distance_norm)\n",
    "                \n",
    "            llm_print = \"\\multirow{6}{*}{\\\\textbf{\" + \\\n",
    "                LLMS_ENCODED[llm] + \\\n",
    "                \"}}\" if idx_sample == 0 and idx_fsc == 0 else \"\"\n",
    "            fs_condition_print = \"\\multirow{3}{*}{\" + \\\n",
    "                ENCODE_CONDITION[few_shot_condition] + \\\n",
    "                \"}\" if idx_sample == 0 else \"\"\n",
    "\n",
    "            print(llm_print, \"&\", fs_condition_print, \"&\", add_thousand_dots(n_sample), \"&\",\n",
    "                  add_thousand_dots(round(np.mean(iterations_avg_unique_words_in_k_words), 2)), \"&\",\n",
    "                  add_thousand_dots(round(np.mean(iterations_avg_levenshtein_distance), 2)), \"&\",\n",
    "                  add_thousand_dots(round(np.mean(iterations_avg_levenshtein_distance_norm), 2)))\n",
    "        \n",
    "        if idx_fsc == 0:\n",
    "            print(\"\\\\arrayrulecolor{gray}\\cline{2-6}\\\\arrayrulecolor{black}\")\n",
    "        else:\n",
    "            print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{3}{*}{\\textbf{Real Examples}} & \\multirow{3}{*}{-} & 500 & 496.4 & 1,914.8 & 1,492.8 \\\\\n",
      " &  & 1,000 & 988.8 & 3,064.2 & 2,352.4 \\\\\n",
      " &  & 1,500 & 1,480.8 & 3,998.6 & 3,041.0 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for idx_sample, n_sample in enumerate([500, 1000, 1500]):\n",
    "    iterations_n_unique_tokens = []\n",
    "    iterations_n_lemmas = []\n",
    "    iterations_avg_unique_sentences = []\n",
    "\n",
    "    for it in range(5):\n",
    "        samples = [item for k in range(\n",
    "            int(n_sample / 500)) for item in dataset[\"real\"][it][k]]\n",
    "        n_unique_tokens = count_unique_tokens(\n",
    "            [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "        n_unique_lemmas = count_unique_lemmas(\n",
    "            [token for example in samples for token in example[\"tokenized_text\"]])\n",
    "        n_unique_sentences = count_unique_sentences(\n",
    "            [example[\"text\"] for example in samples])\n",
    "\n",
    "        iterations_n_unique_tokens.append(n_unique_tokens)\n",
    "        iterations_n_lemmas.append(n_unique_lemmas)\n",
    "        iterations_avg_unique_sentences.append(n_unique_sentences)\n",
    "\n",
    "    data_source_print = \"\\multirow{3}{*}{\\\\textbf{Real Examples}}\" if idx_sample == 0 else \"\"\n",
    "\n",
    "    fs_condition_print = \"\\multirow{3}{*}{-}\" if idx_sample == 0 else \"\"\n",
    "\n",
    "    print(data_source_print, \"&\", fs_condition_print, \"&\", add_thousand_dots(n_sample), \"&\",\n",
    "          add_thousand_dots(round(np.mean(iterations_avg_unique_sentences), 2)), \"&\",\n",
    "          add_thousand_dots(round(np.mean(iterations_n_unique_tokens), 2)), \"&\",\n",
    "          add_thousand_dots(round(np.mean(iterations_n_lemmas), 2)), \"\\\\\\\\\")\n",
    "print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{3}{*}{\\textbf{Real Examples}} & \\multirow{3}{*}{-} & 500 & 78.32 & 16.37 & 0.93 \\\\\n",
      " &  & 1,000 & 78.35 & 16.42 & 0.93 \\\\\n",
      " &  & 1,500 & 78.22 & 16.39 & 0.93 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for idx_sample, n_sample in enumerate([500, 1000, 1500]):\n",
    "    iterations_avg_unique_words_in_k_words = []\n",
    "    iterations_avg_levenshtein_distance = []\n",
    "    iterations_avg_levenshtein_distance_norm = []\n",
    "    for it in range(5):\n",
    "        samples = [item for k in range(\n",
    "            int(n_sample / 500)) for item in dataset[\"real\"][it][k]]\n",
    "\n",
    "        n_unique_words_in_k_words = get_avg_unique_words_in_k_words(\n",
    "            [token.text for example in samples for token in example[\"tokenized_text\"]])\n",
    "        avg_levenshtein_distance = average_word_level_levenshtein_distance(\n",
    "            samples)\n",
    "        avg_levenshtein_distance_norm = average_word_level_levenshtein_distance(\n",
    "            samples, norm=True)\n",
    "\n",
    "        iterations_avg_unique_words_in_k_words.append(\n",
    "            n_unique_words_in_k_words)\n",
    "        iterations_avg_levenshtein_distance.append(\n",
    "            avg_levenshtein_distance)\n",
    "        iterations_avg_levenshtein_distance_norm.append(\n",
    "            avg_levenshtein_distance_norm)\n",
    "        \n",
    "    data_source_print = \"\\multirow{3}{*}{\\\\textbf{Real Examples}}\" if idx_sample == 0 else \"\"\n",
    "    fs_condition_print = \"\\multirow{3}{*}{-}\" if idx_sample == 0 else \"\"\n",
    "\n",
    "    print(data_source_print, \"&\", fs_condition_print, \"&\", add_thousand_dots(n_sample), \"&\",\n",
    "          add_thousand_dots(\n",
    "              round(np.mean(iterations_avg_unique_words_in_k_words), 2)), \"&\",\n",
    "          add_thousand_dots(\n",
    "              round(np.mean(iterations_avg_levenshtein_distance), 2)), \"&\",\n",
    "          add_thousand_dots(round(np.mean(iterations_avg_levenshtein_distance_norm), 2)), \"\\\\\\\\\")\n",
    "print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect Term Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{6}{*}{\\textbf{Llama-2-70B}} & \\multirow{3}{*}{25 fixed examples} & 500 & 795.6 & 27.2 \\% \\textit{(SD = 4.25)} & 578.8 & 104.8 & 34.1 \\\\\n",
      " &  & 1,000 & 1,591.4 & 27.8 \\% \\textit{(SD = 4.24)} & 1,149.4 & 170.0 & 33.9 \\\\\n",
      " &  & 1,500 & 2,388.0 & 28.0 \\% \\textit{(SD = 4.22)} & 1,720.4 & 220.0 & 33.4 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-8}\\arrayrulecolor{black}\n",
      " & \\multirow{3}{*}{25 random examples} & 500 & 704.6 & 26.6 \\% \\textit{(SD = 1.99)} & 517.2 & 116.8 & 38.0 \\\\\n",
      " &  & 1,000 & 1,407.8 & 26.3 \\% \\textit{(SD = 1.77)} & 1,037.4 & 194.4 & 39.0 \\\\\n",
      " &  & 1,500 & 2,109.6 & 26.5 \\% \\textit{(SD = 1.14)} & 1,550.0 & 256.6 & 38.9 \\\\\n",
      "\\hline\n",
      "\\multirow{6}{*}{\\textbf{GPT-3.5-turbo}} & \\multirow{3}{*}{25 fixed examples} & 500 & 795.6 & 35.1 \\% \\textit{(SD = 9.18)} & 516.2 & 38.8 & 17.9 \\\\\n",
      " &  & 1,000 & 1,591.4 & 35.1 \\% \\textit{(SD = 9.28)} & 1,032.4 & 50.6 & 17.5 \\\\\n",
      " &  & 1,500 & 2,388.0 & 35.1 \\% \\textit{(SD = 8.77)} & 1,550.4 & 61.8 & 17.4 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-8}\\arrayrulecolor{black}\n",
      " & \\multirow{3}{*}{25 random examples} & 500 & 704.6 & 44.6 \\% \\textit{(SD = 3.91)} & 390.2 & 33.0 & 18.1 \\\\\n",
      " &  & 1,000 & 1,407.8 & 44.5 \\% \\textit{(SD = 2.91)} & 781.0 & 43.8 & 17.6 \\\\\n",
      " &  & 1,500 & 2,109.6 & 44.1 \\% \\textit{(SD = 2.6)} & 1,180.2 & 54.2 & 17.6 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for idx_llm, llm in enumerate(LLMS):\n",
    "    for idx_fsc, few_shot_condition in enumerate(FS_CONDITIONS):\n",
    "        for idx_sample, n_sample in enumerate([500, 1000, 1500]):\n",
    "            tags_from_splits_count = []\n",
    "            tags_from_splits_count_implicit = []\n",
    "            tags_from_splits_count_explicit = []\n",
    "            count_unique_aspect_terms_in_split = []\n",
    "            count_unique_aspect_terms_in_k_aspect_terms = []\n",
    "            for it in range(5):\n",
    "                tags = [tag for k in range(\n",
    "                    int(n_sample / 500)) for example in dataset[\"synth\"][llm][few_shot_condition][it][k] for tag in example[\"tags\"]]\n",
    "                tags_explicit = [tag[\"text\"]\n",
    "                                 for tag in tags if tag[\"type\"] == \"label-explicit\"]\n",
    "                tags_from_splits_count.append(len(tags))\n",
    "                tags_from_splits_count_explicit.append(\n",
    "                    len([tag for tag in tags if tag[\"type\"] == \"label-explicit\"]))\n",
    "                tags_from_splits_count_implicit.append(\n",
    "                    len([tag for tag in tags if tag[\"type\"] == \"label-implicit\"]))\n",
    "\n",
    "                unique_tags = len(set(tags_explicit))\n",
    "\n",
    "                # Calculate number of unique tokens in 100 aspect terms\n",
    "                count_unique_aspect_terms_in_k_aspect_terms.append(\n",
    "                    get_avg_unique_words_in_k_words(tags_explicit))\n",
    "\n",
    "                count_unique_aspect_terms_in_split.append(unique_tags)\n",
    "\n",
    "            llm_print = \"\\multirow{6}{*}{\\\\textbf{\" + \\\n",
    "                LLMS_ENCODED[llm] + \\\n",
    "                \"}}\" if idx_sample == 0 and idx_fsc == 0 else \"\"\n",
    "            fs_condition_print = \"\\multirow{3}{*}{\" + \\\n",
    "                ENCODE_CONDITION[few_shot_condition] + \\\n",
    "                \"}\" if idx_sample == 0 else \"\"\n",
    "\n",
    "            print(llm_print, \"&\", fs_condition_print,\n",
    "                  \"&\", add_thousand_dots(n_sample),  # n samples\n",
    "                  \"&\", add_thousand_dots(\n",
    "                      round(np.mean(tags_from_splits_count), 2)),  # n aspects\n",
    "                  \"&\", add_thousand_dots(\n",
    "                      round(np.mean(tags_from_splits_count_implicit) / np.mean(tags_from_splits_count) * 100, 1)) + \" \\\\%\",  # % implicit\n",
    "                  \"\\\\textit{(SD = \" + add_thousand_dots(round(np.std([a / b * 100 for a, b in zip(\n",
    "                      tags_from_splits_count_implicit, tags_from_splits_count)]), 2)) + \")}\",\n",
    "                  \"&\", add_thousand_dots(\n",
    "                      round(np.mean(tags_from_splits_count_explicit), 2)),  # n aspects\n",
    "                  \"&\", add_thousand_dots(\n",
    "                      round(np.mean(count_unique_aspect_terms_in_split), 2)),  # n unique\n",
    "                  \"&\", add_thousand_dots(round(np.mean(count_unique_aspect_terms_in_k_aspect_terms), 1)), \"\\\\\\\\\")\n",
    "        \n",
    "        if idx_fsc == 0:\n",
    "            print(\"\\\\arrayrulecolor{gray}\\cline{2-8}\\\\arrayrulecolor{black}\")\n",
    "        else:\n",
    "            print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{3}{*}{\\textbf{Real Examples}} & \\multirow{3}{*}{-} & 500 & 703.2 & 26.5 \\% \\textit{(SD = 1.06)} & 517.2 & 256.4 & 68.3 \\\\\n",
      " &  & 1,000 & 1,403.8 & 26.6 \\% \\textit{(SD = 0.56)} & 1,029.8 & 438.4 & 68.2 \\\\\n",
      " &  & 1,500 & 2,107.2 & 26.8 \\% \\textit{(SD = 0.19)} & 1,542.4 & 595.6 & 68.0 \\\\\n"
     ]
    }
   ],
   "source": [
    "for idx_sample, n_sample in enumerate([500, 1000, 1500]):\n",
    "    tags_from_splits_count = []\n",
    "    tags_from_splits_count_implicit = []\n",
    "    tags_from_splits_count_explicit = []\n",
    "    count_unique_aspect_terms_in_split = []\n",
    "    count_unique_aspect_terms_in_k_aspect_terms = []\n",
    "    for it in range(5):\n",
    "        tags = [tag for k in range(int(n_sample / 500))\n",
    "                for example in dataset[\"real\"][it][k] for tag in example[\"tags\"]]\n",
    "        tags_explicit = [tag[\"text\"]\n",
    "                         for tag in tags if tag[\"type\"] == \"label-explicit\"]\n",
    "        tags_from_splits_count.append(len(tags))\n",
    "        tags_from_splits_count_explicit.append(\n",
    "            len([tag for tag in tags if tag[\"type\"] == \"label-explicit\"]))\n",
    "        tags_from_splits_count_implicit.append(\n",
    "            len([tag for tag in tags if tag[\"type\"] == \"label-implicit\"]))\n",
    "\n",
    "        unique_tags = len(set(tags_explicit))\n",
    "\n",
    "        # Calculate number of unique tokens in 100 aspect terms\n",
    "        count_unique_aspect_terms_in_k_aspect_terms.append(\n",
    "            get_avg_unique_words_in_k_words(tags_explicit))\n",
    "\n",
    "        count_unique_aspect_terms_in_split.append(unique_tags)\n",
    "\n",
    "    data_source_print = \"\\multirow{3}{*}{\\\\textbf{Real Examples}}\" if idx_sample == 0 else \"\"\n",
    "    fs_condition_print = \"\\multirow{3}{*}{-}\" if idx_sample == 0 else \"\"\n",
    "\n",
    "    print(data_source_print, \"&\", fs_condition_print,\n",
    "          \"&\", add_thousand_dots(n_sample),  # n samples\n",
    "          \"&\", add_thousand_dots(\n",
    "              round(np.mean(tags_from_splits_count), 2)),  # n aspects\n",
    "          \"&\", add_thousand_dots(\n",
    "              round(np.mean(tags_from_splits_count_implicit) / np.mean(tags_from_splits_count) * 100, 1)) + \" \\\\%\",  # % implicit\n",
    "          \"\\\\textit{(SD = \" + add_thousand_dots(round(np.std([a / b * 100 for a, b in zip(\n",
    "              tags_from_splits_count_implicit, tags_from_splits_count)]), 2)) + \")}\",\n",
    "          \"&\", add_thousand_dots(\n",
    "              round(np.mean(tags_from_splits_count_explicit), 2)),  # n aspects\n",
    "          \"&\", add_thousand_dots(\n",
    "              round(np.mean(count_unique_aspect_terms_in_split), 2)),  # n unique\n",
    "          \"&\", add_thousand_dots(round(np.mean(count_unique_aspect_terms_in_k_aspect_terms), 1)), \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect Term Analysis (With Aspect Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synth Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textbf{Llama-2-70B} & 25 fixed examples & GENERAL-IMPRESSION & 500 & 155.4 & 43.0 \\% \\textit{(SD = 4.8)} & 34.6 & 7.51 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & GENERAL-IMPRESSION & 1,000 & 312.2 & 44.1 \\% \\textit{(SD = 5.24)} & 57.0 & 7.5 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & GENERAL-IMPRESSION & 1,500 & 468.0 & 42.6 \\% \\textit{(SD = 6.39)} & 78.4 & 7.52 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & FOOD & 500 & 157.8 & 21.3 \\% \\textit{(SD = 4.72)} & 33.6 & 5.29 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & FOOD & 1,000 & 316.4 & 22.4 \\% \\textit{(SD = 5.52)} & 56.0 & 5.2 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & FOOD & 1,500 & 474.0 & 22.9 \\% \\textit{(SD = 5.09)} & 71.0 & 5.14 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & SERVICE & 500 & 158.8 & 17.5 \\% \\textit{(SD = 3.74)} & 18.4 & 4.57 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & SERVICE & 1,000 & 316.2 & 18.3 \\% \\textit{(SD = 4.48)} & 26.4 & 4.55 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & SERVICE & 1,500 & 474.6 & 19.0 \\% \\textit{(SD = 3.92)} & 32.6 & 4.53 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & AMBIENCE & 500 & 158.4 & 21.1 \\% \\textit{(SD = 4.54)} & 24.8 & 4.58 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & AMBIENCE & 1,000 & 314.4 & 20.2 \\% \\textit{(SD = 5.09)} & 40.4 & 4.53 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & AMBIENCE & 1,500 & 471.8 & 21.5 \\% \\textit{(SD = 4.72)} & 52.8 & 4.52 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & PRICE & 500 & 156.8 & 29.8 \\% \\textit{(SD = 9.7)} & 21.0 & 5.5 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & PRICE & 1,000 & 315.0 & 30.0 \\% \\textit{(SD = 7.75)} & 32.8 & 5.35 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 fixed examples & PRICE & 1,500 & 471.6 & 29.6 \\% \\textit{(SD = 7.37)} & 41.0 & 5.27 \\\\\n",
      "\\hline\n",
      "\\textbf{Llama-2-70B} & 25 random examples & GENERAL-IMPRESSION & 500 & 139.2 & 47.6 \\% \\textit{(SD = 3.11)} & 35.4 & 8.15 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & GENERAL-IMPRESSION & 1,000 & 277.6 & 47.2 \\% \\textit{(SD = 1.65)} & 56.4 & 8.05 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & GENERAL-IMPRESSION & 1,500 & 417.2 & 48.2 \\% \\textit{(SD = 1.4)} & 75.8 & 8.12 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & FOOD & 500 & 138.2 & 18.7 \\% \\textit{(SD = 1.89)} & 33.6 & 5.58 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & FOOD & 1,000 & 279.4 & 19.9 \\% \\textit{(SD = 1.98)} & 54.8 & 5.57 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & FOOD & 1,500 & 418.8 & 20.2 \\% \\textit{(SD = 1.24)} & 76.2 & 5.57 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & SERVICE & 500 & 139.6 & 18.2 \\% \\textit{(SD = 4.5)} & 19.2 & 4.97 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & SERVICE & 1,000 & 278.8 & 17.4 \\% \\textit{(SD = 2.12)} & 30.6 & 5.03 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & SERVICE & 1,500 & 418.4 & 18.0 \\% \\textit{(SD = 1.43)} & 41.0 & 5.11 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & AMBIENCE & 500 & 139.2 & 17.7 \\% \\textit{(SD = 3.43)} & 26.8 & 4.71 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & AMBIENCE & 1,000 & 277.8 & 16.9 \\% \\textit{(SD = 2.38)} & 47.8 & 4.94 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & AMBIENCE & 1,500 & 418.0 & 17.1 \\% \\textit{(SD = 2.03)} & 62.8 & 4.9 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & PRICE & 500 & 139.0 & 25.9 \\% \\textit{(SD = 4.74)} & 22.8 & 5.35 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & PRICE & 1,000 & 277.2 & 25.8 \\% \\textit{(SD = 4.28)} & 38.8 & 5.7 \\\\\n",
      "\\textbf{Llama-2-70B} & 25 random examples & PRICE & 1,500 & 415.8 & 25.5 \\% \\textit{(SD = 3.76)} & 47.6 & 5.6 \\\\\n",
      "\\hline\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & GENERAL-IMPRESSION & 500 & 155.4 & 71.7 \\% \\textit{(SD = 12.97)} & 12.0 & 5.16 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & GENERAL-IMPRESSION & 1,000 & 311.6 & 72.3 \\% \\textit{(SD = 12.8)} & 17.2 & 5.17 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & GENERAL-IMPRESSION & 1,500 & 467.6 & 72.9 \\% \\textit{(SD = 13.15)} & 20.4 & 5.21 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & FOOD & 500 & 158.2 & 24.3 \\% \\textit{(SD = 9.96)} & 12.2 & 2.61 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & FOOD & 1,000 & 317.0 & 24.0 \\% \\textit{(SD = 8.86)} & 16.2 & 2.45 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & FOOD & 1,500 & 476.2 & 23.2 \\% \\textit{(SD = 7.89)} & 20.4 & 2.44 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & SERVICE & 500 & 158.8 & 20.2 \\% \\textit{(SD = 8.74)} & 4.6 & 2.01 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & SERVICE & 1,000 & 316.8 & 19.5 \\% \\textit{(SD = 7.99)} & 6.2 & 2.2 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & SERVICE & 1,500 & 474.8 & 19.0 \\% \\textit{(SD = 7.1)} & 7.8 & 2.18 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & AMBIENCE & 500 & 158.6 & 22.2 \\% \\textit{(SD = 9.6)} & 10.2 & 3.48 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & AMBIENCE & 1,000 & 315.6 & 22.9 \\% \\textit{(SD = 10.28)} & 12.8 & 3.34 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & AMBIENCE & 1,500 & 474.0 & 22.8 \\% \\textit{(SD = 9.43)} & 16.2 & 3.29 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & PRICE & 500 & 156.6 & 34.7 \\% \\textit{(SD = 15.65)} & 6.2 & 2.74 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & PRICE & 1,000 & 315.0 & 34.3 \\% \\textit{(SD = 15.3)} & 8.2 & 2.74 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 fixed examples & PRICE & 1,500 & 472.8 & 35.0 \\% \\textit{(SD = 14.8)} & 9.8 & 2.75 \\\\\n",
      "\\hline\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & GENERAL-IMPRESSION & 500 & 137.6 & 81.5 \\% \\textit{(SD = 4.4)} & 6.8 & 3.94 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & GENERAL-IMPRESSION & 1,000 & 274.6 & 81.7 \\% \\textit{(SD = 3.32)} & 9.8 & 3.72 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & GENERAL-IMPRESSION & 1,500 & 413.4 & 81.2 \\% \\textit{(SD = 3.36)} & 13.0 & 3.77 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & FOOD & 500 & 138.0 & 37.2 \\% \\textit{(SD = 2.37)} & 8.4 & 2.5 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & FOOD & 1,000 & 278.8 & 35.2 \\% \\textit{(SD = 1.88)} & 14.6 & 2.52 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & FOOD & 1,500 & 418.6 & 34.1 \\% \\textit{(SD = 2.08)} & 18.6 & 2.54 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & SERVICE & 500 & 139.6 & 25.9 \\% \\textit{(SD = 3.27)} & 4.8 & 2.31 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & SERVICE & 1,000 & 279.0 & 26.4 \\% \\textit{(SD = 1.68)} & 5.4 & 2.31 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & SERVICE & 1,500 & 418.0 & 26.1 \\% \\textit{(SD = 1.63)} & 7.4 & 2.27 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & AMBIENCE & 500 & 140.0 & 23.4 \\% \\textit{(SD = 6.6)} & 8.2 & 3.53 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & AMBIENCE & 1,000 & 278.0 & 24.2 \\% \\textit{(SD = 3.65)} & 9.6 & 3.46 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & AMBIENCE & 1,500 & 417.8 & 24.7 \\% \\textit{(SD = 2.89)} & 11.6 & 3.47 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & PRICE & 500 & 139.2 & 51.4 \\% \\textit{(SD = 8.71)} & 8.2 & 3.71 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & PRICE & 1,000 & 277.4 & 51.7 \\% \\textit{(SD = 7.88)} & 10.0 & 3.53 \\\\\n",
      "\\textbf{GPT-3.5-turbo} & 25 random examples & PRICE & 1,500 & 415.0 & 51.1 \\% \\textit{(SD = 7.44)} & 12.2 & 3.58 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for few_shot_condition in FS_CONDITIONS:\n",
    "        for aspect_category in ASPECT_CATEGORIES:\n",
    "            for n_sample in [500, 1000, 1500]:\n",
    "                tags_from_splits_count = []\n",
    "                tags_from_splits_count_implicit = []\n",
    "                tags_from_splits_count_explicit = []\n",
    "                count_unique_aspect_terms_in_split = []\n",
    "                count_unique_aspect_terms_in_k_aspect_terms = []\n",
    "                for it in range(5):\n",
    "                    tags = [tag for k in range(\n",
    "                        int(n_sample / 500)) for example in dataset[\"synth\"][llm][few_shot_condition][it][k] for tag in example[\"tags\"] if tag[\"label\"] == aspect_category]\n",
    "                    tags_explicit = [tag[\"text\"]\n",
    "                                     for tag in tags if tag[\"type\"] == \"label-explicit\"]\n",
    "                    tags_from_splits_count.append(len(tags))\n",
    "                    tags_from_splits_count_explicit.append(\n",
    "                        len([tag for tag in tags if tag[\"type\"] == \"label-explicit\"]))\n",
    "                    tags_from_splits_count_implicit.append(\n",
    "                        len([tag for tag in tags if tag[\"type\"] == \"label-implicit\"]))\n",
    "\n",
    "                    unique_tags = len(set(tags_explicit))\n",
    "\n",
    "                    # Calculate number of unique tokens in 100 aspect terms\n",
    "                    count_unique_aspect_terms_in_k_aspect_terms.append(\n",
    "                        get_avg_unique_words_in_k_words(tags_explicit, n_selection=10))\n",
    "\n",
    "                    count_unique_aspect_terms_in_split.append(unique_tags)\n",
    "                print(\"\\\\textbf{\"+LLMS_ENCODED[llm]+\"}\", \"&\", ENCODE_CONDITION[few_shot_condition], \"&\", aspect_category,\n",
    "                      \"&\", add_thousand_dots(n_sample),\n",
    "                      \"&\", add_thousand_dots(\n",
    "                          round(np.mean(tags_from_splits_count), 2)),\n",
    "                      \"&\", add_thousand_dots(\n",
    "                    round(np.mean(tags_from_splits_count_implicit) / np.mean(tags_from_splits_count) * 100, 1)) + \" \\\\%\",  # % implicit\n",
    "                    \"\\\\textit{(SD = \" + add_thousand_dots(round(np.std([a / b * 100 for a, b in zip(\n",
    "                        tags_from_splits_count_implicit, tags_from_splits_count)]), 2)) + \")}\",\n",
    "                    \"&\", add_thousand_dots(round(\n",
    "                        np.mean(count_unique_aspect_terms_in_split), 2)),\n",
    "                    \"&\", add_thousand_dots(round(np.mean(count_unique_aspect_terms_in_k_aspect_terms), 2)), \"\\\\\\\\\")\n",
    "        print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textbf{Real Examples} & GENERAL-IMPRESSION & 500 & 124.6 & 96.8 & 27.8 & 14.0 & 6.92 \\\\\n",
      "\\textbf{Real Examples} & GENERAL-IMPRESSION & 1,000 & 249.2 & 194.0 & 55.2 & 24.0 & 7.07 \\\\\n",
      "\\textbf{Real Examples} & GENERAL-IMPRESSION & 1,500 & 376.8 & 294.8 & 82.0 & 32.4 & 7.05 \\\\\n",
      "\\textbf{Real Examples} & FOOD & 500 & 281.2 & 30.8 & 250.4 & 144.8 & 8.71 \\\\\n",
      "\\textbf{Real Examples} & FOOD & 1,000 & 561.2 & 62.6 & 498.6 & 249.6 & 8.7 \\\\\n",
      "\\textbf{Real Examples} & FOOD & 1,500 & 839.4 & 94.6 & 744.8 & 339.8 & 8.69 \\\\\n",
      "\\textbf{Real Examples} & SERVICE & 500 & 174.8 & 39.6 & 135.2 & 51.4 & 7.17 \\\\\n",
      "\\textbf{Real Examples} & SERVICE & 1,000 & 349.8 & 79.6 & 270.2 & 87.2 & 7.14 \\\\\n",
      "\\textbf{Real Examples} & SERVICE & 1,500 & 525.8 & 118.8 & 407.0 & 116.8 & 7.13 \\\\\n",
      "\\textbf{Real Examples} & AMBIENCE & 500 & 80.0 & 12.0 & 68.0 & 38.0 & 7.79 \\\\\n",
      "\\textbf{Real Examples} & AMBIENCE & 1,000 & 159.0 & 23.4 & 135.6 & 66.4 & 7.81 \\\\\n",
      "\\textbf{Real Examples} & AMBIENCE & 1,500 & 238.8 & 34.4 & 204.4 & 93.2 & 7.8 \\\\\n",
      "\\textbf{Real Examples} & PRICE & 500 & 42.6 & 6.8 & 35.8 & 24.6 & 8.47 \\\\\n",
      "\\textbf{Real Examples} & PRICE & 1,000 & 84.6 & 14.4 & 70.2 & 41.4 & 8.48 \\\\\n",
      "\\textbf{Real Examples} & PRICE & 1,500 & 126.4 & 22.2 & 104.2 & 55.2 & 8.42 \\\\\n"
     ]
    }
   ],
   "source": [
    "for aspect_category in ASPECT_CATEGORIES:\n",
    "    for n_sample in [500, 1000, 1500]:\n",
    "        tags_from_splits_count = []\n",
    "        tags_from_splits_count_implicit = []\n",
    "        tags_from_splits_count_explicit = []\n",
    "        count_unique_aspect_terms_in_split = []\n",
    "        count_unique_aspect_terms_in_k_aspect_terms = []\n",
    "        for it in range(5):\n",
    "            tags = [tag for k in range(int(n_sample / 500))\n",
    "                    for example in dataset[\"real\"][it][k] for tag in example[\"tags\"] if tag[\"label\"] == aspect_category]\n",
    "            tags_explicit = [tag[\"text\"]\n",
    "                             for tag in tags if tag[\"type\"] == \"label-explicit\"]\n",
    "            tags_from_splits_count.append(len(tags))\n",
    "            tags_from_splits_count_explicit.append(\n",
    "                len([tag for tag in tags if tag[\"type\"] == \"label-explicit\"]))\n",
    "            tags_from_splits_count_implicit.append(\n",
    "                len([tag for tag in tags if tag[\"type\"] == \"label-implicit\"]))\n",
    "\n",
    "            unique_tags = len(set(tags_explicit))\n",
    "\n",
    "            # Calculate number of unique tokens in 100 aspect terms\n",
    "            count_unique_aspect_terms_in_k_aspect_terms.append(\n",
    "                get_avg_unique_words_in_k_words(tags_explicit, n_selection=10))\n",
    "\n",
    "            count_unique_aspect_terms_in_split.append(unique_tags)\n",
    "        print(\"\\\\textbf{Real Examples} &\", aspect_category,\n",
    "              \"&\", add_thousand_dots(n_sample),\n",
    "              \"&\", add_thousand_dots(\n",
    "                  round(np.mean(tags_from_splits_count), 2)),\n",
    "              \"&\", add_thousand_dots(\n",
    "                  round(np.mean(tags_from_splits_count_implicit), 2)),\n",
    "              \"&\", add_thousand_dots(\n",
    "                  round(np.mean(tags_from_splits_count_explicit), 2)),\n",
    "              \"&\", add_thousand_dots(round(\n",
    "                  np.mean(count_unique_aspect_terms_in_split), 2)),\n",
    "              \"&\", add_thousand_dots(round(np.mean(count_unique_aspect_terms_in_k_aspect_terms), 2)), \"\\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
