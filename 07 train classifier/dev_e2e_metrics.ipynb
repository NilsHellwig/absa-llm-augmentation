{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import constants\n",
    "from helper import divide\n",
    "\n",
    "\n",
    "def one_hot_to_label(one_hot):\n",
    "    return next(constants.ID_TO_LABEL_E2E[idx] for idx in range(len(one_hot)) if one_hot[idx] == 1)\n",
    "\n",
    "\n",
    "def add_end_to_tag(tag, i):\n",
    "    tag[\"end\"] = i\n",
    "    return tag\n",
    "\n",
    "def compute_metrics_for_subset(examples_predictions, examples_labels):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(examples_predictions)):\n",
    "        example_metrics = calculate_tp_tn_fp_fn_spans(examples_predictions[i], examples_labels[i])\n",
    "        tp += example_metrics[0]\n",
    "        tn += example_metrics[1]\n",
    "        fp += example_metrics[2]\n",
    "        fn += example_metrics[3]\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def calculate_tp_tn_fp_fn_spans(pred, label):\n",
    "    pred_set = set(\n",
    "        f\"{range['start']}_{range['end']}_{range['polarity']}\" for range in pred)\n",
    "    label_set = set(\n",
    "        f\"{range['start']}_{range['end']}_{range['polarity']}\" for range in label)\n",
    "\n",
    "    # Calculate true positives by finding the intersection of the sets.\n",
    "    tp_set = pred_set & label_set\n",
    "    tp = len(tp_set)\n",
    "\n",
    "    # Calculate false positives by subtracting the intersection from the predicted set.\n",
    "    fp_set = pred_set - tp_set\n",
    "    fp = len(fp_set)\n",
    "\n",
    "    # Calculate false negatives by subtracting the intersection from the label set.\n",
    "    fn_set = label_set - tp_set\n",
    "    fn = len(fn_set)\n",
    "\n",
    "    return tp, 0, fp, fn\n",
    "\n",
    "\n",
    "def get_predicted_phrases(example):\n",
    "    total_tags = []\n",
    "    for polarity in constants.POLARITIES:\n",
    "        tags_for_polarity = []\n",
    "\n",
    "        for i in range(len(example)):\n",
    "            I_tag_val = example[i][constants.LABEL_TO_ID_E2E[f\"I_{polarity}\"]]\n",
    "            if I_tag_val == 0 and len(tags_for_polarity) > 0:\n",
    "                tags_for_polarity = [add_end_to_tag(\n",
    "                    tag, i) for tag in tags_for_polarity]\n",
    "                total_tags += tags_for_polarity\n",
    "                tags_for_polarity = []\n",
    "\n",
    "            B_tag_val = example[i][constants.LABEL_TO_ID_E2E[f\"B_{polarity}\"]]\n",
    "            if B_tag_val == 1:\n",
    "                tags_for_polarity.append({\"start\": i, \"polarity\": polarity})\n",
    "\n",
    "    # print([d for d in total_tags if 'end' not in d])\n",
    "    return total_tags\n",
    "\n",
    "\n",
    "def calculate_popular_metrics(predictions, true_labels):\n",
    "    tp, tn, fp, fn = compute_metrics_for_subset(predictions, true_labels)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    metrics[\"tp\"] = tp\n",
    "    metrics[\"tn\"] = tn\n",
    "    metrics[\"fp\"] = fp\n",
    "    metrics[\"fn\"] = fn\n",
    "    metrics[\"accuracy\"] = divide((tp+tn), (tp+tn+fp+fn))\n",
    "    metrics[\"precision\"] = divide(tp, tp + fp)\n",
    "    metrics[\"recall\"] = divide(tp, tp + fn)\n",
    "    metrics[\"f1\"] = divide(2 * metrics[\"precision\"] *\n",
    "                           metrics[\"recall\"], metrics[\"precision\"] + metrics[\"recall\"])\n",
    "    return metrics\n",
    "\n",
    "def calculate_f1_micro(metrics):\n",
    "    tp_total = sum([metrics[f\"tp_{pol}\"] for pol in constants.POLARITIES])\n",
    "    fp_total = sum([metrics[f\"fp_{pol}\"] for pol in constants.POLARITIES])\n",
    "    fn_total = sum([metrics[f\"fn_{pol}\"] for pol in constants.POLARITIES])\n",
    "    tn_total = sum([metrics[f\"tn_{pol}\"] for pol in constants.POLARITIES])\n",
    "\n",
    "    precision_total = tp_total / (tp_total + fp_total)\n",
    "    recall_total = tp_total / (tp_total + fn_total)\n",
    "\n",
    "    return 2 * (precision_total * recall_total) / (precision_total + recall_total)\n",
    "\n",
    "def compute_metrics(p=None):\n",
    "    predictions = np.load('predictions.npy')\n",
    "    predictions = np.where(predictions > 0, np.ones(\n",
    "        predictions.shape), np.zeros(predictions.shape))\n",
    "    true_labels = np.load('true_labels.npy')\n",
    "\n",
    "    # Convert prediction to phrases\n",
    "    predictions = [get_predicted_phrases(example) for example in predictions]\n",
    "    true_labels = [get_predicted_phrases(example) for example in true_labels]\n",
    "\n",
    "    metrics = {}\n",
    "    # Calculate metrics for each polarity\n",
    "    for polarity in constants.POLARITIES:\n",
    "        pol_metrics = calculate_popular_metrics([[tag for tag in pred if tag[\"polarity\"] == polarity] for pred in predictions], [[tag for tag in label if tag[\"polarity\"] == polarity] for label in true_labels])\n",
    "        metrics.update({f\"{metric}_{polarity}\":pol_metrics[metric] for metric in pol_metrics.keys()})\n",
    "    \n",
    "    # Total metrics\n",
    "    metrics[\"f1_micro\"] = calculate_f1_micro(metrics)\n",
    "    metrics[\"f1_macro\"] = sum(metrics[key] for key in [f\"f1_{pol}\" for pol in constants.POLARITIES]) / len(constants.POLARITIES)\n",
    "    metrics.update(calculate_popular_metrics(predictions, true_labels))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tp_POSITIVE': 2,\n",
       " 'tn_POSITIVE': 0,\n",
       " 'fp_POSITIVE': 5403,\n",
       " 'fn_POSITIVE': 298,\n",
       " 'accuracy_POSITIVE': 0.0003506926179203928,\n",
       " 'precision_POSITIVE': 0.0003700277520814061,\n",
       " 'recall_POSITIVE': 0.006666666666666667,\n",
       " 'f1_POSITIVE': 0.0007011393514460998,\n",
       " 'tp_NEUTRAL': 0,\n",
       " 'tn_NEUTRAL': 0,\n",
       " 'fp_NEUTRAL': 188,\n",
       " 'fn_NEUTRAL': 17,\n",
       " 'accuracy_NEUTRAL': 0.0,\n",
       " 'precision_NEUTRAL': 0.0,\n",
       " 'recall_NEUTRAL': 0.0,\n",
       " 'f1_NEUTRAL': 0,\n",
       " 'tp_NEGATIVE': 6,\n",
       " 'tn_NEGATIVE': 0,\n",
       " 'fp_NEGATIVE': 2191,\n",
       " 'fn_NEGATIVE': 204,\n",
       " 'accuracy_NEGATIVE': 0.0024989587671803417,\n",
       " 'precision_NEGATIVE': 0.0027309968138370506,\n",
       " 'recall_NEGATIVE': 0.02857142857142857,\n",
       " 'f1_NEGATIVE': 0.004985459077690072,\n",
       " 'f1_micro': 0.0019237705903571002,\n",
       " 'f1_macro': 0.001895532809712057,\n",
       " 'tp': 8,\n",
       " 'tn': 0,\n",
       " 'fp': 7782,\n",
       " 'fn': 519,\n",
       " 'accuracy': 0.0009628114093152004,\n",
       " 'precision': 0.0010269576379974327,\n",
       " 'recall': 0.015180265654648957,\n",
       " 'f1': 0.0019237705903571002}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.load('test_data.npy', allow_pickle=True)\n",
    "compute_metrics(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
