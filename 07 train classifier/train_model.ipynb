{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1c00eb-3042-4e94-8e47-e082bcb29577",
   "metadata": {},
   "source": [
    "# Notebook: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e96d9f-4b8d-41c6-bd36-6a4dda52903a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e64ba35-a40d-44e6-bdc3-771db5285113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ACD import aspect_category_labels_to_one_hot, CustomDatasetACD, preprocess_data_ACD, create_model_ACD, compute_metrics_ACD, get_trainer_ACD\n",
    "from OTE import create_model_OTE, get_preprocessed_data_OTE, compute_metrics_OTE, get_trainer_OTE\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score, accuracy_score, hamming_loss, precision_score, recall_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from helper import format_seconds_to_time_string\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import set_seed\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5cb7e4-ff64-44d3-a5e6-d9386eb7bc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.optimization\")\n",
    "torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af370b-cdd6-4733-92c2-067358ea0736",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0356f-40dc-404f-9c82-2a4112545f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete Later:\n",
    "TEST_FOLDS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4c2f77-2f1c-40c8-9244-28b546a38584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLM_NAME = \"Llama13B\"\n",
    "N_REAL = 500\n",
    "N_SYNTH = 0\n",
    "TARGET = \"aspect_term\" # \"aspect_term\", \"aspect_category\"\n",
    "LLM_SAMPLING = \"fixed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42300f-d3ea-4578-8450-02b2639c0af2",
   "metadata": {},
   "source": [
    "## Settings (do not change!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de0ce2d7-a009-43ca-9b99-5b874a25a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c48866c-3549-4d05-b0a9-f0b737ee6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_LOOP = [0, 1, 2, 3, 4, 0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4a5385-2a90-4033-9c12-b662f07d0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 43\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c55d7f95-4c76-4352-9f66-96a55a0caf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_CATEGORIES  = [\"GENERAL-IMPRESSION\", \"FOOD\", \"SERVICE\", \"AMBIENCE\", \"PRICE\"]\n",
    "POLARITIES = [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7fecf01-0c40-40ff-840d-d0c4b7f57cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8986e14-70e2-4a52-9cba-1456bc9be94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4853cd8-c455-4f03-9b67-589fdd620768",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c67dad-6b5e-464c-9928-277c9c2bc844",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c32e3b-2258-4244-8d1e-9fecd81372cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Real Dataset\n",
    "splits_real = []\n",
    "for i in range(N_FOLDS):\n",
    "    with open(f'../03 dataset split/real/real_{i}.json', 'r') as json_datei:\n",
    "        real_split = json.load(json_datei)[:N_REAL]\n",
    "        splits_real.append(real_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb0500e-e89d-4ebb-afba-53c3d3867161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Synth Dataset\n",
    "splits_synth = []\n",
    "for i in range(N_FOLDS):\n",
    "    with open(f'../04 llm synthesis/synth/{LLM_NAME}/{LLM_SAMPLING}/split_{i}.json', 'r') as json_datei:\n",
    "        synth_split = json.load(json_datei)[:N_SYNTH]\n",
    "        splits_synth.append(synth_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3466979-b9c5-421d-887e-ebebadafcd42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_splits_map = {\n",
    "    500: 1,\n",
    "    1000: 2,\n",
    "    2000: 4\n",
    "}\n",
    "n_splits_required_real = n_splits_map.get(N_REAL, 0)\n",
    "n_splits_required_synth = n_splits_map.get(N_SYNTH, 0)\n",
    "n_splits_required_real, n_splits_required_synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee0e5093-b0dc-4c6a-818a-26f2346c781f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Five indexes, each for one cross valdiation run\n",
    "train_dataset = []\n",
    "test_dataset = []\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    test_data = splits_real[i]\n",
    "    train_data = []\n",
    "    \n",
    "    if N_REAL > 0:\n",
    "        for split_idx in SPLIT_LOOP[i+1: i+1+n_splits_required_real]:\n",
    "            for example in splits_real[split_idx]:\n",
    "                train_data.append(example)\n",
    " \n",
    "    if N_SYNTH > 0:\n",
    "        for split_idx in SPLIT_LOOP[i+1: i+1+n_splits_required_synth]:\n",
    "            for example in splits_synth[split_idx]:\n",
    "                train_data.append(example)\n",
    "                \n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    train_dataset.append(train_data)\n",
    "    test_dataset.append(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbcbc1-5c95-4ccb-b90e-fa14b4268439",
   "metadata": {},
   "source": [
    "### ACD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d82fdd55-f02d-49e9-9b70-d9e487e301f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ACD_model():\n",
    "    results = {\n",
    "        \"LLM_NAME\": LLM_NAME,\n",
    "        \"N_REAL\": N_REAL,\n",
    "        \"N_SYNTH\": N_SYNTH,\n",
    "        \"TARGET\": TARGET,\n",
    "        \"LLM_SAMPLING\": LLM_SAMPLING,\n",
    "    }\n",
    "\n",
    "    f1_micro_scores = []\n",
    "    f1_macro_scores = []\n",
    "    f1_weighted_scores = []\n",
    "    accuracy_scores = []\n",
    "    class_f1_scores = []\n",
    "    loss = []\n",
    "    hamming = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepset/gbert-large\")\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    for cross_idx in range(N_FOLDS):\n",
    "        # Load Data\n",
    "        train_data = preprocess_data_ACD(train_dataset[cross_idx], tokenizer)\n",
    "        test_data = preprocess_data_ACD(test_dataset[cross_idx], tokenizer)\n",
    "\n",
    "        # Load Model\n",
    "        model_ACD = create_model_ACD()\n",
    "\n",
    "        # Train Model\n",
    "        trainer = get_trainer_ACD(model_ACD, train_data, test_data, tokenizer)\n",
    "        trainer.train()\n",
    "\n",
    "        # Save Evaluation of Test Data\n",
    "        eval_metrics = trainer.evaluate()\n",
    "\n",
    "        # Save Metrics for fold\n",
    "        f1_micro_scores.append(eval_metrics[\"eval_f1_micro\"])\n",
    "        f1_macro_scores.append(eval_metrics[\"eval_f1_macro\"])\n",
    "        f1_weighted_scores.append(eval_metrics[\"eval_f1_weighted\"])\n",
    "        accuracy_scores.append(eval_metrics[\"eval_accuracy\"])\n",
    "        class_f1_scores.append(eval_metrics[\"eval_class_f1_scores\"])\n",
    "        loss.append(eval_metrics[\"eval_loss\"])\n",
    "        hamming.append(eval_metrics[\"eval_hamming_loss\"])\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    results[\"loss\"] = np.mean(loss)\n",
    "    results[\"hamming\"] = np.mean(hamming)\n",
    "    results[\"accuracy\"] = np.mean(accuracy_scores)\n",
    "    results[\"f1_micro\"] = np.mean(f1_micro_scores)\n",
    "    results[\"f1_macro\"] = np.mean(f1_macro_scores)\n",
    "    results[\"f1_weighted\"] = np.mean(f1_weighted_scores)\n",
    "    results[\"runtime\"] = runtime\n",
    "    results[\"runtime_formatted\"] = format_seconds_to_time_string(runtime)\n",
    "    return results\n",
    "\n",
    "if TARGET == \"aspect_category\":\n",
    "   results = train_ACD_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0739ccd-edd1-45af-aa28-9f56f3247211",
   "metadata": {},
   "source": [
    "### OTE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5f2ca26-1f66-4c0f-b201-8852cfa9b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_OTE_model():\n",
    "    model_name = \"deepset/gbert-large\"\n",
    "    results = {\n",
    "       \"LLM_NAME\": LLM_NAME,\n",
    "       \"N_REAL\": N_REAL,\n",
    "       \"N_SYNTH\": N_SYNTH,\n",
    "       \"TARGET\": TARGET,\n",
    "       \"LLM_SAMPLING\": LLM_SAMPLING,\n",
    "    }\n",
    "\n",
    "    f1_micro_scores = []\n",
    "    eval_loss = []\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for cross_idx in range(N_FOLDS)[0:TEST_FOLDS]:\n",
    "        # Load Data\n",
    "        train_data = train_dataset[cross_idx]\n",
    "        test_data = test_dataset[cross_idx]\n",
    "        train_data, test_data = get_preprocessed_data_OTE(train_data, test_data, tokenizer)\n",
    "        trainer = get_trainer_OTE(train_data, test_data, tokenizer)\n",
    "        trainer.train()\n",
    "\n",
    "        # Save Evaluation Metrics\n",
    "        eval_metrics = trainer.evaluate()\n",
    "        f1_micro_scores.append(eval_metrics[\"eval_f1_micro\"])\n",
    "        eval_loss.append(eval_metrics[\"eval_loss\"])\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "    results[\"runtime\"] = runtime\n",
    "    results[\"runtime_formatted\"] = format_seconds_to_time_string(runtime)\n",
    "    results[\"eval_loss\"] = np.mean(eval_loss)\n",
    "    results[\"f1_micro\"] = np.mean(f1_micro_scores)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0c7c133-58cd-4394-aac8-2ec0b1581efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSpanCategorizationOTE: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSpanCategorizationOTE from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSpanCategorizationOTE from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSpanCategorizationOTE were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSpanCategorizationOTE: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSpanCategorizationOTE from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSpanCategorizationOTE from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSpanCategorizationOTE were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 B</th>\n",
       "      <th>F1 I</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.256900</td>\n",
       "      <td>0.070314</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if TARGET == \"aspect_term\":\n",
    "   results = train_OTE_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24030dcd-dcdb-4dfa-943d-77df2419a437",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a605e1c-0838-4362-baff-ac314887b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results_json/results_{LLM_NAME}_real{N_REAL}_synth{N_SYNTH}_{TARGET}_{LLM_SAMPLING}.json', 'w') as json_file:\n",
    "    json.dump(results, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4361c90b-9974-4482-b122-580cc52c7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([results])\n",
    "df.to_csv(f'results_csv/results_{LLM_NAME}_real{N_REAL}_synth{N_SYNTH}_{TARGET}_{LLM_SAMPLING}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c568fc9-bab5-431f-8640-3549d39929c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LLM_NAME': 'Llama13B',\n",
       " 'N_REAL': 500,\n",
       " 'N_SYNTH': 0,\n",
       " 'TARGET': 'aspect_term',\n",
       " 'LLM_SAMPLING': 'fixed',\n",
       " 'runtime': 57.751405000686646,\n",
       " 'runtime_formatted': '57s',\n",
       " 'eval_loss': 0.07031434774398804,\n",
       " 'f1_micro': 0.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75196213-705c-4490-845d-139c9bcb2303",
   "metadata": {},
   "source": [
    "### Remove useless folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a769b8c7-8103-4ab0-a472-a7fab305f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_to_delete = ['output', 'output2', 'token_classifier']\n",
    "for folder in folders_to_delete:\n",
    "   try:\n",
    "       shutil.rmtree(folder)\n",
    "   except:\n",
    "       pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
