{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1c00eb-3042-4e94-8e47-e082bcb29577",
   "metadata": {},
   "source": [
    "# Notebook: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e96d9f-4b8d-41c6-bd36-6a4dda52903a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e64ba35-a40d-44e6-bdc3-771db5285113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ACD import aspect_category_labels_to_one_hot, CustomDatasetACD, preprocess_data_ACD, create_model_ACD, compute_metrics_ACD, get_trainer_ACD\n",
    "from OTE import create_model_OTE, get_preprocessed_data_OTE, compute_metrics_OTE, get_trainer_OTE, divide\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score, accuracy_score, hamming_loss, precision_score, recall_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from helper import format_seconds_to_time_string\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import set_seed\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5cb7e4-ff64-44d3-a5e6-d9386eb7bc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.optimization\")\n",
    "torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af370b-cdd6-4733-92c2-067358ea0736",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b0356f-40dc-404f-9c82-2a4112545f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete Later:\n",
    "TEST_FOLDS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4c2f77-2f1c-40c8-9244-28b546a38584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLM_NAME = \"Llama13B\"\n",
    "N_REAL = 500\n",
    "N_SYNTH = 0\n",
    "TARGET = \"aspect_term\" # \"aspect_term\", \"aspect_category\"\n",
    "LLM_SAMPLING = \"fixed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42300f-d3ea-4578-8450-02b2639c0af2",
   "metadata": {},
   "source": [
    "## Settings (do not change!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0ce2d7-a009-43ca-9b99-5b874a25a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c48866c-3549-4d05-b0a9-f0b737ee6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_LOOP = [0, 1, 2, 3, 4, 0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4a5385-2a90-4033-9c12-b662f07d0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 43\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55d7f95-4c76-4352-9f66-96a55a0caf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_CATEGORIES  = [\"GENERAL-IMPRESSION\", \"FOOD\", \"SERVICE\", \"AMBIENCE\", \"PRICE\"]\n",
    "POLARITIES = [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7fecf01-0c40-40ff-840d-d0c4b7f57cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8986e14-70e2-4a52-9cba-1456bc9be94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4853cd8-c455-4f03-9b67-589fdd620768",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c67dad-6b5e-464c-9928-277c9c2bc844",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77c32e3b-2258-4244-8d1e-9fecd81372cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Real Dataset\n",
    "splits_real = []\n",
    "for i in range(N_FOLDS):\n",
    "    with open(f'../03 dataset split/real/real_{i}.json', 'r') as json_datei:\n",
    "        real_split = json.load(json_datei)[:N_REAL]\n",
    "        splits_real.append(real_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bb0500e-e89d-4ebb-afba-53c3d3867161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Synth Dataset\n",
    "splits_synth = []\n",
    "for i in range(N_FOLDS):\n",
    "    with open(f'../04 llm synthesis/synth/{LLM_NAME}/{LLM_SAMPLING}/split_{i}.json', 'r') as json_datei:\n",
    "        synth_split = json.load(json_datei)[:N_SYNTH]\n",
    "        splits_synth.append(synth_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3466979-b9c5-421d-887e-ebebadafcd42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_splits_map = {\n",
    "    500: 1,\n",
    "    1000: 2,\n",
    "    2000: 4\n",
    "}\n",
    "n_splits_required_real = n_splits_map.get(N_REAL, 0)\n",
    "n_splits_required_synth = n_splits_map.get(N_SYNTH, 0)\n",
    "n_splits_required_real, n_splits_required_synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee0e5093-b0dc-4c6a-818a-26f2346c781f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Five indexes, each for one cross valdiation run\n",
    "train_dataset = []\n",
    "test_dataset = []\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    test_data = splits_real[i]\n",
    "    train_data = []\n",
    "    \n",
    "    if N_REAL > 0:\n",
    "        for split_idx in SPLIT_LOOP[i+1: i+1+n_splits_required_real]:\n",
    "            for example in splits_real[split_idx]:\n",
    "                train_data.append(example)\n",
    " \n",
    "    if N_SYNTH > 0:\n",
    "        for split_idx in SPLIT_LOOP[i+1: i+1+n_splits_required_synth]:\n",
    "            for example in splits_synth[split_idx]:\n",
    "                train_data.append(example)\n",
    "                \n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    train_dataset.append(train_data)\n",
    "    test_dataset.append(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbcbc1-5c95-4ccb-b90e-fa14b4268439",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ACD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d82fdd55-f02d-49e9-9b70-d9e487e301f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ACD_model():\n",
    "    results = {\n",
    "        \"LLM_NAME\": LLM_NAME,\n",
    "        \"N_REAL\": N_REAL,\n",
    "        \"N_SYNTH\": N_SYNTH,\n",
    "        \"TARGET\": TARGET,\n",
    "        \"LLM_SAMPLING\": LLM_SAMPLING,\n",
    "    }\n",
    "\n",
    "    f1_micro_scores = []\n",
    "    f1_macro_scores = []\n",
    "    f1_weighted_scores = []\n",
    "    accuracy_scores = []\n",
    "    class_f1_scores = []\n",
    "    loss = []\n",
    "    hamming = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepset/gbert-large\")\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    for cross_idx in range(N_FOLDS):\n",
    "        # Load Data\n",
    "        train_data = preprocess_data_ACD(train_dataset[cross_idx], tokenizer)\n",
    "        test_data = preprocess_data_ACD(test_dataset[cross_idx], tokenizer)\n",
    "\n",
    "        # Load Model\n",
    "        model_ACD = create_model_ACD()\n",
    "\n",
    "        # Train Model\n",
    "        trainer = get_trainer_ACD(model_ACD, train_data, test_data, tokenizer)\n",
    "        trainer.train()\n",
    "\n",
    "        # Save Evaluation of Test Data\n",
    "        eval_metrics = trainer.evaluate()\n",
    "\n",
    "        # Save Metrics for fold\n",
    "        f1_micro_scores.append(eval_metrics[\"eval_f1_micro\"])\n",
    "        f1_macro_scores.append(eval_metrics[\"eval_f1_macro\"])\n",
    "        f1_weighted_scores.append(eval_metrics[\"eval_f1_weighted\"])\n",
    "        accuracy_scores.append(eval_metrics[\"eval_accuracy\"])\n",
    "        class_f1_scores.append(eval_metrics[\"eval_class_f1_scores\"])\n",
    "        loss.append(eval_metrics[\"eval_loss\"])\n",
    "        hamming.append(eval_metrics[\"eval_hamming_loss\"])\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    results[\"loss\"] = np.mean(loss)\n",
    "    results[\"hamming\"] = np.mean(hamming)\n",
    "    results[\"accuracy\"] = np.mean(accuracy_scores)\n",
    "    results[\"f1_micro\"] = np.mean(f1_micro_scores)\n",
    "    results[\"f1_macro\"] = np.mean(f1_macro_scores)\n",
    "    results[\"f1_weighted\"] = np.mean(f1_weighted_scores)\n",
    "    results[\"runtime\"] = runtime\n",
    "    results[\"runtime_formatted\"] = format_seconds_to_time_string(runtime)\n",
    "    return results\n",
    "\n",
    "if TARGET == \"aspect_category\":\n",
    "   results = train_ACD_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a7373-3ef8-41d2-9169-926882a65359",
   "metadata": {},
   "source": [
    "### tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "752f150a-44bf-4f61-a1d8-1a1e00c6dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"deepset/gbert-base\"\n",
    "MAX_TOKENS = 256\n",
    "RANDOM_SEED = 43\n",
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 1\n",
    "LEARNING_RATE = 5e-6\n",
    "\n",
    "label2id = {\n",
    "    'O': 0,\n",
    "    'B': 1,\n",
    "    'I': 2,\n",
    "}\n",
    "id2label = {\n",
    "    0: 'O',\n",
    "    1: 'B',\n",
    "    2: 'I',\n",
    "}\n",
    "\n",
    "n_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7a396a6-d922-4ac6-a258-d90dce229491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/huggingface/transformers/issues/17971\n",
    "class TrainingArgumentsWithMPSSupport(TrainingArguments):\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "def get_trainer_OTE(train_data, test_data, tokenizer):\n",
    "    training_args = TrainingArgumentsWithMPSSupport(\n",
    "        output_dir=\"output2\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=N_EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"logs\",\n",
    "        logging_steps=100,\n",
    "        logging_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_micro\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        seed=RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "    compute_metrics_OTE = prepare_compute_metrics(test_data)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model_init=create_model_OTE,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_OTE\n",
    "    )\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0739ccd-edd1-45af-aa28-9f56f3247211",
   "metadata": {},
   "source": [
    "### OTE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd95558a-ccf2-450f-8385-565bc4ba4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_to_label(one_hot):\n",
    "    return next(id2label[idx] for idx in range(len(one_hot)) if one_hot[idx] == 1)   \n",
    "def find_bio_phrases(bio_list):\n",
    "    phrases = []\n",
    "    phrase_start = None\n",
    "\n",
    "    for i in range(len(bio_list)):\n",
    "        if bio_list[i] == 'B':\n",
    "            if phrase_start is not None:\n",
    "                phrase_end = i - 1\n",
    "                phrases.append({\"start\": phrase_start, \"end\": phrase_end})\n",
    "            phrase_start = i\n",
    "        elif bio_list[i] == 'O':\n",
    "            if phrase_start is not None:\n",
    "                phrase_end = i - 1\n",
    "                phrases.append({\"start\": phrase_start, \"end\": phrase_end})\n",
    "                phrase_start = None\n",
    "\n",
    "    if phrase_start is not None:\n",
    "        phrases.append({\"start\": phrase_start, \"end\": len(bio_list) - 1})\n",
    "\n",
    "    return phrases\n",
    "\n",
    "def calculate_tp_tn_fp_fn_spans(pred, label):\n",
    "    \"\"\"\n",
    "    Calculate true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) based on the provided\n",
    "    lists of predicted and actual label ranges.\n",
    "\n",
    "    Args:\n",
    "        pred (list of dict): A list containing dictionaries representing predicted ranges with 'start' and 'end' values.\n",
    "        label (list of dict): A list containing dictionaries representing actual label ranges with 'start' and 'end' values.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing four values - TP (true positives), FP (false positives), and FN (false negatives).\n",
    "    \"\"\"\n",
    "    # Convert ranges to string representations and create sets.\n",
    "    pred_set = set(f\"{range['start']}_{range['end']}\" for range in pred)\n",
    "    label_set = set(f\"{range['start']}_{range['end']}\" for range in label)\n",
    "\n",
    "    # Calculate true positives by finding the intersection of the sets.\n",
    "    tp_set = pred_set & label_set\n",
    "    tp = len(tp_set)\n",
    "\n",
    "    # Calculate false positives by subtracting the intersection from the predicted set.\n",
    "    fp_set = pred_set - tp_set\n",
    "    fp = len(fp_set)\n",
    "\n",
    "    # Calculate false negatives by subtracting the intersection from the label set.\n",
    "    fn_set = label_set - tp_set\n",
    "    fn = len(fn_set)\n",
    "\n",
    "    # Calculate true negatives by considering all possible pairs and subtracting TP, FP, and FN.\n",
    "    total_possible_pairs = len(pred) * len(label)\n",
    "\n",
    "    return tp, 0, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a458f0ce-cd95-4907-8785-2d156785a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_compute_metrics(test_data):\n",
    "    \n",
    "    def compute_metrics_OTE(p):\n",
    "        aspect_categories = test_data.aspect_categories\n",
    "        predictions, true_labels = p\n",
    "\n",
    "        #np.save(\"metrics_data.npy\", { \"aspect_categories\": aspect_categories, \"predictions\": predictions, \"true_labels\": true_labels})\n",
    "\n",
    "        loaded_data = np.load(\"metrics_data.npy\", allow_pickle=True)\n",
    "        aspect_categories = loaded_data.item()[\"aspect_categories\"]\n",
    "        predictions = loaded_data.item()[\"predictions\"]\n",
    "        true_labels = loaded_data.item()[\"true_labels\"]\n",
    "\n",
    "        metrics = {}\n",
    "\n",
    "        for ac in ASPECT_CATEGORIES:\n",
    "            examples_predictions = np.array([predictions[i] for i in range(len(predictions)) if aspect_categories[i] == ac])\n",
    "            examples_labels = np.array([true_labels[i] for i in range(len(true_labels)) if aspect_categories[i] == ac])\n",
    "\n",
    "            examples_predictions = (examples_predictions == examples_predictions.max(axis=2)[:,:,np.newaxis]).astype(int)\n",
    "\n",
    "            examples_predictions = find_bio_phrases([one_hot_to_label(p) for p in examples_predictions[i]])\n",
    "            examples_labels = find_bio_phrases([one_hot_to_label(p) for p in examples_labels[i]])\n",
    "            \n",
    "            tp, tn, fp, fn = calculate_tp_tn_fp_fn_spans(examples_predictions, examples_labels)\n",
    "            precision = divide(tp, tp + fp)\n",
    "            recall = divide(tp, tp + fn)\n",
    "            f1 = divide(2 * precision * recall, precision + recall)\n",
    "            metrics[f\"f1_{ac}\"] = f1\n",
    "    \n",
    "        metrics[\"f1_micro\"] = sum(metrics[key] for key in [f\"f1_{ac}\" for ac in ASPECT_CATEGORIES]) / (len(ASPECT_CATEGORIES) - 1)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    return compute_metrics_OTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5f2ca26-1f66-4c0f-b201-8852cfa9b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_OTE_model():\n",
    "    model_name = \"deepset/gbert-base\"\n",
    "    results = {\n",
    "       \"LLM_NAME\": LLM_NAME,\n",
    "       \"N_REAL\": N_REAL,\n",
    "       \"N_SYNTH\": N_SYNTH,\n",
    "       \"TARGET\": TARGET,\n",
    "       \"LLM_SAMPLING\": LLM_SAMPLING,\n",
    "    }\n",
    "\n",
    "    f1_micro_scores = []\n",
    "    eval_loss = []\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for cross_idx in range(N_FOLDS)[0:TEST_FOLDS]:\n",
    "        # Load Data\n",
    "        train_data = train_dataset[cross_idx]\n",
    "        test_data = test_dataset[cross_idx]\n",
    "        train_data, test_data = get_preprocessed_data_OTE(train_data, test_data, tokenizer)\n",
    "        trainer = get_trainer_OTE(train_data, test_data, tokenizer)\n",
    "        trainer.train()\n",
    "        \n",
    "\n",
    "        # Save Evaluation Metrics\n",
    "        eval_metrics = trainer.evaluate()\n",
    "        f1_micro_scores.append(eval_metrics[\"eval_f1_micro\"])\n",
    "        eval_loss.append(eval_metrics[\"eval_loss\"])\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "    results[\"runtime\"] = runtime\n",
    "    results[\"runtime_formatted\"] = format_seconds_to_time_string(runtime)\n",
    "    results[\"eval_loss\"] = np.mean(eval_loss)\n",
    "    results[\"f1_micro\"] = np.mean(f1_micro_scores)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0c7c133-58cd-4394-aac8-2ec0b1581efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSpanCategorizationOTE: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSpanCategorizationOTE from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSpanCategorizationOTE from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSpanCategorizationOTE were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSpanCategorizationOTE: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSpanCategorizationOTE from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSpanCategorizationOTE from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSpanCategorizationOTE were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 General-impression</th>\n",
       "      <th>F1 Food</th>\n",
       "      <th>F1 Service</th>\n",
       "      <th>F1 Ambience</th>\n",
       "      <th>F1 Price</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>473.256300</td>\n",
       "      <td>473.127045</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if TARGET == \"aspect_term\":\n",
    "   results = train_OTE_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24030dcd-dcdb-4dfa-943d-77df2419a437",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a605e1c-0838-4362-baff-ac314887b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results_json/results_{LLM_NAME}_real{N_REAL}_synth{N_SYNTH}_{TARGET}_{LLM_SAMPLING}.json', 'w') as json_file:\n",
    "    json.dump(results, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4361c90b-9974-4482-b122-580cc52c7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([results])\n",
    "df.to_csv(f'results_csv/results_{LLM_NAME}_real{N_REAL}_synth{N_SYNTH}_{TARGET}_{LLM_SAMPLING}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c568fc9-bab5-431f-8640-3549d39929c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LLM_NAME': 'Llama13B',\n",
       " 'N_REAL': 500,\n",
       " 'N_SYNTH': 0,\n",
       " 'TARGET': 'aspect_term',\n",
       " 'LLM_SAMPLING': 'fixed',\n",
       " 'runtime': 58.23281717300415,\n",
       " 'runtime_formatted': '58s',\n",
       " 'eval_loss': 473.1270446777344,\n",
       " 'f1_micro': 0.058823529411764705}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75196213-705c-4490-845d-139c9bcb2303",
   "metadata": {},
   "source": [
    "### Remove useless folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a769b8c7-8103-4ab0-a472-a7fab305f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_to_delete = ['output', 'output2', 'token_classifier']\n",
    "for folder in folders_to_delete:\n",
    "   try:\n",
    "       shutil.rmtree(folder)\n",
    "   except:\n",
    "       pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
