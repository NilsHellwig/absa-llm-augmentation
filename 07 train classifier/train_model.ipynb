{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1c00eb-3042-4e94-8e47-e082bcb29577",
   "metadata": {},
   "source": [
    "# Notebook: Train Model for a given Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e96d9f-4b8d-41c6-bd36-6a4dda52903a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e64ba35-a40d-44e6-bdc3-771db5285113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score, accuracy_score, hamming_loss, precision_score, recall_score\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from scipy.special import expit\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5cb7e4-ff64-44d3-a5e6-d9386eb7bc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af370b-cdd6-4733-92c2-067358ea0736",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4c2f77-2f1c-40c8-9244-28b546a38584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLM_NAME = \"Llama13B\"\n",
    "N_REAL = 500\n",
    "N_SYNTH = 0\n",
    "TARGET = \"aspect_category\"\n",
    "LLM_SAMPLING = \"fixed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42300f-d3ea-4578-8450-02b2639c0af2",
   "metadata": {},
   "source": [
    "## Settings (do not change!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de0ce2d7-a009-43ca-9b99-5b874a25a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c48866c-3549-4d05-b0a9-f0b737ee6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_LOOP = [0, 1, 2, 3, 4, 0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4a5385-2a90-4033-9c12-b662f07d0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 43\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c55d7f95-4c76-4352-9f66-96a55a0caf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_CATEGORIES  = [\"GENERAL-IMPRESSION\", \"FOOD\", \"SERVICE\", \"AMBIENCE\", \"PRICE\"]\n",
    "POLARITIES = [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4853cd8-c455-4f03-9b67-589fdd620768",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c67dad-6b5e-464c-9928-277c9c2bc844",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77c32e3b-2258-4244-8d1e-9fecd81372cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Real Dataset\n",
    "splits_real = []\n",
    "for i in range(N_FOLDS):\n",
    "    with open(f'../03 dataset split/real/real_{i}.json', 'r') as json_datei:\n",
    "        real_split = json.load(json_datei)[:N_REAL]\n",
    "        splits_real.append(real_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb0500e-e89d-4ebb-afba-53c3d3867161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Synth Dataset\n",
    "splits_synth = []\n",
    "for i in range(N_FOLDS):\n",
    "    with open(f'../04 llm synthesis/synth/{LLM_NAME}/{LLM_SAMPLING}/split_{i}.json', 'r') as json_datei:\n",
    "        synth_split = json.load(json_datei)[:N_SYNTH]\n",
    "        splits_synth.append(synth_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3466979-b9c5-421d-887e-ebebadafcd42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_splits_map = {\n",
    "    500: 1,\n",
    "    1000: 2,\n",
    "    2000: 4\n",
    "}\n",
    "n_splits_required_real = n_splits_map.get(N_REAL, 0)\n",
    "n_splits_required_synth = n_splits_map.get(N_SYNTH, 0)\n",
    "n_splits_required_real, n_splits_required_synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee0e5093-b0dc-4c6a-818a-26f2346c781f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Five indexes, each for one cross valdiation run\n",
    "train_dataset = []\n",
    "test_dataset = []\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    test_data = splits_real[i]\n",
    "    train_data = []\n",
    "    \n",
    "    if N_REAL > 0:\n",
    "        for split_idx in SPLIT_LOOP[i+1: i+1+n_splits_required_real]:\n",
    "            for example in splits_real[split_idx]:\n",
    "                train_data.append(example)\n",
    " \n",
    "    if N_SYNTH > 0:\n",
    "        for split_idx in SPLIT_LOOP[i+1: i+1+n_splits_required_synth]:\n",
    "            for example in splits_synth[split_idx]:\n",
    "                train_data.append(example)\n",
    "                \n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    train_dataset.append(train_data)\n",
    "    test_dataset.append(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92021f1-6bdb-4d85-b2d3-f94e0793b1e5",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbcbc1-5c95-4ccb-b90e-fa14b4268439",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbb7855e-484c-42b5-a963-3f31f7c64d65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_ACD = \"deepset/gbert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_ACD)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f368bca-eddf-4a55-bc6e-226b846c235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_category_labels_to_one_hot(labels):\n",
    "    one_hot = []\n",
    "    for label in ASPECT_CATEGORIES:\n",
    "        if label in labels:\n",
    "            one_hot.append(1)\n",
    "        else:\n",
    "            one_hot.append(0)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f569a7f8-fe32-4deb-b9b4-2fb4613838f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetACD(TorchDataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"label\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeb79830-0f1b-4ec2-8ddc-3762a8fa4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_ACD(dataset, tokenizer):\n",
    "    texts = [example[\"text\"] for example in dataset]\n",
    "\n",
    "    #list(set(original_list))\n",
    "    \n",
    "    labels = [list(set([tag[\"label\"] for tag in example[\"tags\"]])) for example in dataset]\n",
    "    labels = [aspect_category_labels_to_one_hot(label) for label in labels]\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    return CustomDatasetACD(encodings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8f893bd-b15b-45f2-8113-b954ee0a0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ACD():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name_ACD,\n",
    "        num_labels=len(ASPECT_CATEGORIES),\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68c757c7-8de9-4a75-abf4-d33fb6101a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_ACD(eval_pred):\n",
    "    predictions, lab = eval_pred\n",
    "\n",
    "    predictions = (expit(predictions) > 0.5)\n",
    "    labels = [l==1 for l in lab]\n",
    "\n",
    "    print(labels[0], predictions[0])\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    f1_macro = f1_score(labels, predictions, average=\"macro\")\n",
    "    f1_micro = f1_score(labels, predictions, average=\"micro\")\n",
    "    f1_weighted = f1_score(labels, predictions, average=\"weighted\")\n",
    "\n",
    "    class_f1_scores = f1_score(labels, predictions, average=None)\n",
    "\n",
    "    hamming = hamming_loss(labels, predictions)\n",
    "\n",
    "    metrics = {\n",
    "        \"hamming_loss\": hamming,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"class_f1_scores\": class_f1_scores.tolist(),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de377411-437e-4af7-818c-c07e00c8b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer_ACD(model, train_data, test_data, tokenizer):\n",
    "    batch_size = 16\n",
    "    epochs = 5\n",
    "    learning_rate = 5e-06\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"output\",\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"logs\",\n",
    "        logging_steps=5,\n",
    "        logging_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_micro\",\n",
    "      # ONLY CUDA:  fp16=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model.to(torch.device(\"mps\")),\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_ACD\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85ad004e-9864-4870-a608-b5b56c2f958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/var/folders/qy/5gtwsk6s7jgbknbqgb533x9w0000gn/T/ipykernel_76137/2976016068.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/qy/5gtwsk6s7jgbknbqgb533x9w0000gn/T/ipykernel_76137/2976016068.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item[\"label\"] = torch.tensor(self.labels[idx])\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='123' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [123/160 01:51 < 00:33, 1.09 it/s, Epoch 3.81/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Class F1 Scores</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.568300</td>\n",
       "      <td>0.119600</td>\n",
       "      <td>0.443236</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.459431</td>\n",
       "      <td>0.691434</td>\n",
       "      <td>0.601071</td>\n",
       "      <td>[0.0, 0.8970099667774087, 0.9076086956521738, 0.4925373134328358, 0.0]</td>\n",
       "      <td>6.145200</td>\n",
       "      <td>81.364000</td>\n",
       "      <td>5.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.319253</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.759524</td>\n",
       "      <td>0.944213</td>\n",
       "      <td>0.915709</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.7976190476190476, 0.0]</td>\n",
       "      <td>6.131500</td>\n",
       "      <td>81.545000</td>\n",
       "      <td>5.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.310400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247975</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>6.119100</td>\n",
       "      <td>81.711000</td>\n",
       "      <td>5.230000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False False] [False False  True False False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qy/5gtwsk6s7jgbknbqgb533x9w0000gn/T/ipykernel_76137/2976016068.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/qy/5gtwsk6s7jgbknbqgb533x9w0000gn/T/ipykernel_76137/2976016068.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item[\"label\"] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False False] [False False  True False False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qy/5gtwsk6s7jgbknbqgb533x9w0000gn/T/ipykernel_76137/2976016068.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/qy/5gtwsk6s7jgbknbqgb533x9w0000gn/T/ipykernel_76137/2976016068.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item[\"label\"] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False False] [False False  True False False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qy/5gtwsk6s7jgbknbqgb533x9w0000gn/T/ipykernel_76137/2976016068.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/qy/5gtwsk6s7jgbknbqgb533x9w0000gn/T/ipykernel_76137/2976016068.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item[\"label\"] = torch.tensor(self.labels[idx])\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cross_idx in range(5):\n",
    "    # Load Data\n",
    "    train_data = preprocess_data_ACD(train_dataset[cross_idx], tokenizer)\n",
    "    test_data = preprocess_data_ACD(test_dataset[cross_idx], tokenizer)\n",
    "\n",
    "    # Load Model\n",
    "    model_ACD = create_model_ACD()\n",
    "\n",
    "    trainer = get_trainer_ACD(model_ACD, train_data, test_data, tokenizer)\n",
    "    trainer.train()\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6a72b-d8bb-4e85-8b21-d27a1972316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b03d9-4387-4e2a-a831-d228951d6fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
