{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1c00eb-3042-4e94-8e47-e082bcb29577",
   "metadata": {},
   "source": [
    "# Notebook: Train Model for a given Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e96d9f-4b8d-41c6-bd36-6a4dda52903a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e64ba35-a40d-44e6-bdc3-771db5285113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ACD import aspect_category_labels_to_one_hot, CustomDatasetACD, preprocess_data_ACD, create_model_ACD, compute_metrics_ACD, get_trainer_ACD\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score, accuracy_score, hamming_loss, precision_score, recall_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from helper import format_seconds_to_time_string\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import set_seed\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5cb7e4-ff64-44d3-a5e6-d9386eb7bc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.optimization\")\n",
    "torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af370b-cdd6-4733-92c2-067358ea0736",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4c2f77-2f1c-40c8-9244-28b546a38584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLM_NAME = \"Llama13B\"\n",
    "N_REAL = 500\n",
    "N_SYNTH = 0\n",
    "TARGET = \"aspect_term\" # \"aspect_term\", \"aspect_category\"\n",
    "LLM_SAMPLING = \"fixed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42300f-d3ea-4578-8450-02b2639c0af2",
   "metadata": {},
   "source": [
    "## Settings (do not change!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de0ce2d7-a009-43ca-9b99-5b874a25a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c48866c-3549-4d05-b0a9-f0b737ee6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_LOOP = [0, 1, 2, 3, 4, 0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4a5385-2a90-4033-9c12-b662f07d0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 43\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c55d7f95-4c76-4352-9f66-96a55a0caf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_CATEGORIES  = [\"GENERAL-IMPRESSION\", \"FOOD\", \"SERVICE\", \"AMBIENCE\", \"PRICE\"]\n",
    "POLARITIES = [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7fecf01-0c40-40ff-840d-d0c4b7f57cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8986e14-70e2-4a52-9cba-1456bc9be94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4853cd8-c455-4f03-9b67-589fdd620768",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c67dad-6b5e-464c-9928-277c9c2bc844",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c32e3b-2258-4244-8d1e-9fecd81372cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Real Dataset\n",
    "splits_real = []\n",
    "for i in range(N_FOLDS):\n",
    "    with open(f'../03 dataset split/real/real_{i}.json', 'r') as json_datei:\n",
    "        real_split = json.load(json_datei)[:N_REAL]\n",
    "        splits_real.append(real_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb0500e-e89d-4ebb-afba-53c3d3867161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Synth Dataset\n",
    "splits_synth = []\n",
    "for i in range(N_FOLDS):\n",
    "    with open(f'../04 llm synthesis/synth/{LLM_NAME}/{LLM_SAMPLING}/split_{i}.json', 'r') as json_datei:\n",
    "        synth_split = json.load(json_datei)[:N_SYNTH]\n",
    "        splits_synth.append(synth_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3466979-b9c5-421d-887e-ebebadafcd42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_splits_map = {\n",
    "    500: 1,\n",
    "    1000: 2,\n",
    "    2000: 4\n",
    "}\n",
    "n_splits_required_real = n_splits_map.get(N_REAL, 0)\n",
    "n_splits_required_synth = n_splits_map.get(N_SYNTH, 0)\n",
    "n_splits_required_real, n_splits_required_synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee0e5093-b0dc-4c6a-818a-26f2346c781f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Five indexes, each for one cross valdiation run\n",
    "train_dataset = []\n",
    "test_dataset = []\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    test_data = splits_real[i]\n",
    "    train_data = []\n",
    "    \n",
    "    if N_REAL > 0:\n",
    "        for split_idx in SPLIT_LOOP[i+1: i+1+n_splits_required_real]:\n",
    "            for example in splits_real[split_idx]:\n",
    "                train_data.append(example)\n",
    " \n",
    "    if N_SYNTH > 0:\n",
    "        for split_idx in SPLIT_LOOP[i+1: i+1+n_splits_required_synth]:\n",
    "            for example in splits_synth[split_idx]:\n",
    "                train_data.append(example)\n",
    "                \n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    train_dataset.append(train_data)\n",
    "    test_dataset.append(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbcbc1-5c95-4ccb-b90e-fa14b4268439",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ACD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d82fdd55-f02d-49e9-9b70-d9e487e301f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ACD_model():\n",
    "    results = {\n",
    "        \"LLM_NAME\": LLM_NAME,\n",
    "        \"N_REAL\": N_REAL,\n",
    "        \"N_SYNTH\": N_SYNTH,\n",
    "        \"TARGET\": TARGET,\n",
    "        \"LLM_SAMPLING\": LLM_SAMPLING,\n",
    "    }\n",
    "\n",
    "    f1_micro_scores = []\n",
    "    f1_macro_scores = []\n",
    "    f1_weighted_scores = []\n",
    "    accuracy_scores = []\n",
    "    class_f1_scores = []\n",
    "    loss = []\n",
    "    hamming = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepset/gbert-large\")\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    for cross_idx in range(N_FOLDS):\n",
    "        # Load Data\n",
    "        train_data = preprocess_data_ACD(train_dataset[cross_idx], tokenizer)\n",
    "        test_data = preprocess_data_ACD(test_dataset[cross_idx], tokenizer)\n",
    "\n",
    "        # Load Model\n",
    "        model_ACD = create_model_ACD()\n",
    "\n",
    "        # Train Model\n",
    "        trainer = get_trainer_ACD(model_ACD, train_data, test_data, tokenizer)\n",
    "        trainer.train()\n",
    "\n",
    "        # Save Evaluation of Test Data\n",
    "        eval_metrics = trainer.evaluate()\n",
    "\n",
    "        # Save Metrics for fold\n",
    "        f1_micro_scores.append(eval_metrics[\"eval_f1_micro\"])\n",
    "        f1_macro_scores.append(eval_metrics[\"eval_f1_macro\"])\n",
    "        f1_weighted_scores.append(eval_metrics[\"eval_f1_weighted\"])\n",
    "        accuracy_scores.append(eval_metrics[\"eval_accuracy\"])\n",
    "        class_f1_scores.append(eval_metrics[\"eval_class_f1_scores\"])\n",
    "        loss.append(eval_metrics[\"eval_loss\"])\n",
    "        hamming.append(eval_metrics[\"eval_hamming_loss\"])\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    results[\"loss\"] = np.mean(loss)\n",
    "    results[\"hamming\"] = np.mean(hamming)\n",
    "    results[\"accuracy\"] = np.mean(accuracy_scores)\n",
    "    results[\"f1_micro\"] = np.mean(f1_micro_scores)\n",
    "    results[\"f1_macro\"] = np.mean(f1_macro_scores)\n",
    "    results[\"f1_weighted\"] = np.mean(f1_weighted_scores)\n",
    "    results[\"runtime\"] = runtime\n",
    "    results[\"runtime_formatted\"] = format_seconds_to_time_string(runtime)\n",
    "    return results\n",
    "\n",
    "if TARGET == \"aspect_category\":\n",
    "   results = train_ACD_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0739ccd-edd1-45af-aa28-9f56f3247211",
   "metadata": {},
   "source": [
    "### OTE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "124c8d9b-d400-40cc-a068-fc9601c55262",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cross_idx in range(N_FOLDS)[0:]:\n",
    "    # Load Data\n",
    "    train_data = train_dataset[cross_idx]\n",
    "    test_data = test_dataset[cross_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d887e6da-0438-4ba8-9cf6-78e3ac47a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    " 'O': 0,\n",
    " 'B': 1,\n",
    " 'I': 2,\n",
    "}\n",
    "id2label = {\n",
    "  0: 'O',\n",
    "  1: 'B',\n",
    "  2: 'I',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24246e83-717f-4191-af5f-f11f17c342f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2d33092-5687-45a0-a143-c2dbfd7f1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_aspect_level_OTE(dataset):\n",
    "    aspect_dataset = []\n",
    "    for example in dataset:\n",
    "        aspect_categories = list(set([tag[\"label\"] for tag in example[\"tags\"] if tag[\"type\"] == \"label-explicit\"]))\n",
    "        for ac in aspect_categories:\n",
    "            aspect_dataset.append({\n",
    "                \"text\": example[\"text\"],\n",
    "                \"aspect_category\": ac,\n",
    "                \"tags\": [tag for tag in example[\"tags\"] if tag[\"type\"] == \"label-explicit\"],\n",
    "                \"id\": example[\"id\"]\n",
    "            })\n",
    "    return aspect_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2fd2bbf-6a51-4fae-b748-1fd9f4ca6a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_role_in_span_OTE(token_start: int, token_end: int, span_start: int, span_end: int):\n",
    "    if token_end <= token_start:\n",
    "        return \"N\"\n",
    "    if token_start < span_start or token_end > span_end:\n",
    "        return \"O\"\n",
    "    if token_start > span_start:\n",
    "        return \"I\"\n",
    "    else:\n",
    "        return \"B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9527d865-fe59-4f3f-8428-33aa1539f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_example_OTE(example, tokenizer):\n",
    "    input_text = example[\"text\"] + \"[SEP]\" + example[\"aspect_category\"]\n",
    "    one_hot_output = [[0 for _ in label2id.keys()] for _ in range(256)]\n",
    "\n",
    "    tokenized_input_text = tokenizer(input_text, \n",
    "                             return_offsets_mapping=True, \n",
    "                             padding=\"max_length\", \n",
    "                             max_length=256,\n",
    "                             truncation=True)\n",
    "\n",
    "    for (token_start, token_end), token_labels in zip(tokenized_input_text[\"offset_mapping\"], one_hot_output):\n",
    "        for span in example[\"tags\"]:\n",
    "            role = get_token_role_in_span_OTE(token_start, token_end, span[\"start\"], span[\"end\"])\n",
    "            if role == \"B\":\n",
    "                token_labels[label2id[\"B\"]] = 1\n",
    "            elif role == \"I\":\n",
    "                token_labels[label2id[\"I\"]] = 1\n",
    "\n",
    "    # 'input_ids', 'attention_mask', 'offset_mapping', 'labels'\n",
    "    return {\n",
    "        \"input_ids\": tokenized_input_text[\"input_ids\"], \n",
    "        \"attention_mask\": tokenized_input_text[\"attention_mask\"],\n",
    "        \"offset_mapping\": tokenized_input_text[\"offset_mapping\"],\n",
    "        \"labels\": one_hot_output\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e054a69-fc07-4adb-a70d-f30ecd8c5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetOTE(TorchDataset):\n",
    "    def __init__(self, input_ids, attention_mask, offset_mapping, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.offset_mapping = offset_mapping\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #item = {key: val[idx].clone().detach() for key, val in self.input_ids.items()}\n",
    "        item = {}\n",
    "        item[\"input_ids\"] = self.input_ids[idx]\n",
    "        item[\"attention_mask\"] = self.attention_mask[idx]\n",
    "        item[\"offset_mapping\"] = self.offset_mapping[idx]\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be5929d2-414f-4159-8318-22b47be6263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset_to_aspect_level_OTE(train_data)\n",
    "test_data = dataset_to_aspect_level_OTE(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "365fa78b-93cd-4ff3-a8b7-b30595aff42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/gbert-large\")\n",
    "\n",
    "train_data = [preprocess_example_OTE(example, tokenizer) for example in train_data]\n",
    "test_data = [preprocess_example_OTE(example, tokenizer) for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cad5aac-63ec-43be-890e-d0f52bb6fef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'labels'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67af1ada-3ce6-48c0-999e-628e06e2737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDatasetOTE([example[\"input_ids\"] for example in train_data], \n",
    "                              [example[\"attention_mask\"] for example in train_data],\n",
    "                              [example[\"offset_mapping\"] for example in train_data],\n",
    "                              [example[\"labels\"] for example in train_data])\n",
    "test_data = CustomDatasetOTE([example[\"input_ids\"] for example in test_data], \n",
    "                              [example[\"attention_mask\"] for example in test_data],\n",
    "                              [example[\"offset_mapping\"] for example in test_data],\n",
    "                              [example[\"labels\"] for example in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "237470af-c8ec-40c7-b202-3d8b02902163",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 1\n",
    "learning_rate = 5e-06\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=100,\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_micro\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    seed=RANDOM_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c191f5e-db20-40e3-97e6-fb648a85e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(a: int, b: int):\n",
    "    return a / b if b > 0 else 0\n",
    "\n",
    "def compute_metrics_OTE(p):\n",
    "    predictions, true_labels = p\n",
    "    predicted_labels = np.where(predictions > 0, np.ones(predictions.shape), np.zeros(predictions.shape))\n",
    "    metrics = {}\n",
    "    cm = multilabel_confusion_matrix(true_labels.reshape(-1, n_labels), predicted_labels.reshape(-1, n_labels))\n",
    "    \n",
    "    for label_idx, matrix in enumerate(cm):\n",
    "        if label_idx == 0:\n",
    "            continue # We don't care about the label \"O\"\n",
    "        tp, fp, fn = matrix[1, 1], matrix[0, 1], matrix[1, 0]\n",
    "        precision = divide(tp, tp + fp)\n",
    "        recall = divide(tp, tp + fn)\n",
    "        f1 = divide(2 * precision * recall, precision + recall)\n",
    "        metrics[f\"f1_{id2label[label_idx]}\"] = f1\n",
    "        \n",
    "    f1_micro = sum(list(metrics.values())) / (n_labels - 1)\n",
    "    metrics[\"f1_micro\"] = f1_micro\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6f64d4f-5e10-422f-b8d8-60454a2b8ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertPreTrainedModel\n",
    "from transformers import RobertaPreTrainedModel, RobertaModel\n",
    "from transformers.utils import (\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    ROBERTA_INPUTS_DOCSTRING,\n",
    "    ROBERTA_START_DOCSTRING,\n",
    "    RobertaEmbeddings\n",
    ")\n",
    "\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BERT_INPUTS_DOCSTRING,\n",
    "    BERT_START_DOCSTRING,\n",
    "    BertEmbeddings\n",
    ")\n",
    "from typing import Optional, Union, Tuple\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01af977c-a1a7-43fa-80b0-ee7e02018f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSpanCategorization(RobertaPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "    \n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels.float())\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0632437d-8a06-4c4a-b807-4f379963e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_OTE():\n",
    "    model_path = \"uklfr/gottbert-base\" # \"uklfr/gottbert-base\" # \"deepset/gbert-large\"\n",
    "    return BertForSpanCategorization.from_pretrained(model_path, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed71da2c-3972-4198-a15d-d97dba4ce24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uklfr/gottbert-base were not used when initializing BertForSpanCategorization: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSpanCategorization from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSpanCategorization from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSpanCategorization were not initialized from the model checkpoint at uklfr/gottbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=create_model_OTE, \n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_OTE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d545620-96a9-4350-a3e5-2880f3d91127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uklfr/gottbert-base were not used when initializing BertForSpanCategorization: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSpanCategorization from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSpanCategorization from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSpanCategorization were not initialized from the model checkpoint at uklfr/gottbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 02:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 B</th>\n",
       "      <th>F1 I</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>0.088065</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32, training_loss=0.21620553731918335, metrics={'train_runtime': 171.7505, 'train_samples_per_second': 2.911, 'train_steps_per_second': 0.186, 'total_flos': 65324779776000.0, 'train_loss': 0.21620553731918335, 'epoch': 1.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24030dcd-dcdb-4dfa-943d-77df2419a437",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a7fe16c-9a30-4cb7-9a41-2c7268441a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[{}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a605e1c-0838-4362-baff-ac314887b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results_json/results_{LLM_NAME}_real{N_REAL}_synth{N_SYNTH}_{TARGET}_{LLM_SAMPLING}.json', 'w') as json_file:\n",
    "    json.dump(results, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4361c90b-9974-4482-b122-580cc52c7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([results])\n",
    "df.to_csv(f'results_csv/results_{LLM_NAME}_real{N_REAL}_synth{N_SYNTH}_{TARGET}_{LLM_SAMPLING}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c568fc9-bab5-431f-8640-3549d39929c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
