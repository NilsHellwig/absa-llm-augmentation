{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Convert Model Results to Latex\n",
    "\n",
    "This notebook is used to load the .json files with the model performance in order to convert them into a latex table for the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings / Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH_BASE = \"../07 train classifier/results_json/results_\"\n",
    "LLMS = [\"Llama70B\", \"GPT-3\"]\n",
    "LLM_PAPER_TITLE = {\"Llama70B\": \"Llama2-70B\", \"GPT-3\": \"GPT-3.5-turbo\"}\n",
    "# , \"aspect_category_sentiment\", \"end_2_end_absa\" ,\"target_aspect_sentiment_detection\"]\n",
    "ABSA_TASKS = [\"aspect_category\"]\n",
    "SYNTH_COMBINATIONS = {\n",
    "    \"random\": [\n",
    "        {\"real\": 500, \"synth\": 500},\n",
    "        {\"real\": 500, \"synth\": 1000},\n",
    "        {\"real\": 500, \"synth\": 1500}\n",
    "    ], \"fixed\": [\n",
    "        {\"real\": 25, \"synth\": 475},\n",
    "        {\"real\": 25, \"synth\": 975},\n",
    "        {\"real\": 25, \"synth\": 1975}\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOT_STRATEGY = {25: \"25 fixed examples\", 500: \"25 random examples\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_thousands(number):\n",
    "    formatted_number = \"{:,}\".format(number)\n",
    "    return formatted_number\n",
    "\n",
    "def round_to_three_decimals(number):\n",
    "    rounded_number = round(number, 3)\n",
    "    return rounded_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect_category :\n",
      "\n",
      "- & - & 500 & 0 & 0.061 & 0.033 & 0.026 \\\\\n",
      "- & - & 1,000 & 0 & 0.061 & 0.033 & 0.026 \\\\\n",
      "- & - & 2,000 & 0 & 0.061 & 0.033 & 0.026 \\\\\n",
      "\\hline\n",
      "Llama2-70B & 25 random examples & 500 & 500 & 0.061 & 0.026 & 0.026 \\\\\n",
      "Llama2-70B & 25 random examples & 500 & 1,000 & 0.061 & 0.026 & 0.026 \\\\\n",
      "Llama2-70B & 25 random examples & 500 & 1,500 & 0.061 & 0.026 & 0.026 \\\\\n",
      "GPT-3.5-turbo & 25 random examples & 500 & 500 & 0.061 & 0.026 & 0.026 \\\\\n",
      "GPT-3.5-turbo & 25 random examples & 500 & 1,000 & 0.061 & 0.026 & 0.026 \\\\\n",
      "GPT-3.5-turbo & 25 random examples & 500 & 1,500 & 0.061 & 0.026 & 0.026 \\\\\n",
      "\\hline\n",
      "Llama2-70B & 25 fixed examples & 25 & 475 & 0.061 & 0.026 & 0.026 \\\\\n",
      "Llama2-70B & 25 fixed examples & 25 & 975 & 0.061 & 0.026 & 0.026 \\\\\n",
      "Llama2-70B & 25 fixed examples & 25 & 1,975 & 0.061 & 0.026 & 0.026 \\\\\n",
      "GPT-3.5-turbo & 25 fixed examples & 25 & 475 & 0.061 & 0.026 & 0.026 \\\\\n",
      "GPT-3.5-turbo & 25 fixed examples & 25 & 975 & 0.061 & 0.026 & 0.026 \\\\\n",
      "GPT-3.5-turbo & 25 fixed examples & 25 & 1,975 & 0.061 & 0.026 & 0.026 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for absa_task in ABSA_TASKS:\n",
    "    print(absa_task, \":\\n\")\n",
    "    for n_real in [500, 1000, 2000]:\n",
    "        json_path = RESULTS_PATH_BASE + \\\n",
    "            f\"only_real_real{n_real}_synth0_{absa_task}_random.json\"\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            results = json.load(json_file)\n",
    "        print(\n",
    "            f\"- & - & {format_thousands(n_real)} & 0 & {round_to_three_decimals(results['eval_f1_micro'])} & {round_to_three_decimals(results['eval_f1_macro'])} & {round_to_three_decimals(results['eval_accuracy'])} \\\\\\\\\")\n",
    "    print(\"\\\\hline\")\n",
    "    for few_shot_condition in SYNTH_COMBINATIONS.keys():\n",
    "        for llm in LLMS:\n",
    "            for freq in SYNTH_COMBINATIONS[few_shot_condition]:\n",
    "                n_real = freq[\"real\"]\n",
    "                n_synth = freq[\"synth\"]\n",
    "                json_path = RESULTS_PATH_BASE + llm + \\\n",
    "                    f\"_real{n_real}_synth{n_synth}_{absa_task}_{few_shot_condition}.json\"\n",
    "                with open(json_path, 'r') as json_file:\n",
    "                    results = json.load(json_file)\n",
    "                # print(f\"results: {absa_task}, {llm}, {few_shot_condition}, n_real: {n_real}, n_synth: {n_synth}\", results)\n",
    "                if absa_task == \"TASD\":\n",
    "                    f1_metrics = f\"{round_to_three_decimals(results['eval_f1_micro'])} & {round_to_three_decimals(results['eval_f1_macro'])}\"\n",
    "                else:\n",
    "                    f1_metrics = f\"{round_to_three_decimals(results['eval_f1_micro'])}\"\n",
    "                print(\n",
    "                    f\"{LLM_PAPER_TITLE[llm]} & {FEW_SHOT_STRATEGY[n_real]} & {format_thousands(n_real)} & {format_thousands(n_synth)} & {f1_metrics} & {round_to_three_decimals(results['eval_accuracy'])} & {round_to_three_decimals(results['eval_accuracy'])} \\\\\\\\\")\n",
    "        print(\"\\hline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Schauen, ob es die Metriken auch bei anderen Modellen gibt\n",
    "# Todo: 1.000 <- Punkte einfÃ¼gen\n",
    "# Todo: Soll bei nur Real gehen\n",
    "# Todo: Soll bei allen Tasks gehen\n",
    "# Todo: bei f1 micro etc 3 nachkommastellen\n",
    "# Schauen, dass es bei jedem Task \"eval_f1_micro\", \"eval_f1_macro\", \"accuracy\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
