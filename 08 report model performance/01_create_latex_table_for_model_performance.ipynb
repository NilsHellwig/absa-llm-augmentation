{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Convert Model Results to Latex\n",
    "\n",
    "This notebook is used to load the .json files with the model performance in order to convert them into a latex table for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Schauen, ob es die Metriken auch bei anderen Modellen gibt\n",
    "# Todo: 1.000 <- Punkte einfÃ¼gen\n",
    "# Todo: Soll bei nur Real gehen\n",
    "# Todo: Soll bei allen Tasks gehen\n",
    "# Todo: bei f1 micro etc 3 nachkommastellen\n",
    "# Schauen, dass es bei jedem Task \"eval_f1_micro\", \"eval_f1_macro\", \"accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings / Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH_BASE = \"../07 train models/results_json/results_\"\n",
    "LLMS = [\"Llama70B\", \"GPT-3\"]\n",
    "LLM_PAPER_TITLE = {\"Llama70B\": \"Llama-2-70B\", \"GPT-3\": \"GPT-3.5-turbo\"}\n",
    "# , \"aspect_category_sentiment\", \"end_2_end_absa\" ,\"target_aspect_sentiment_detection\"]\n",
    "ABSA_TASKS = [\"aspect_category\", \"aspect_category_sentiment\", \"end_2_end_absa\", \"target_aspect_sentiment_detection\"]\n",
    "SYNTH_COMBINATIONS = {\n",
    "    \"random\": [\n",
    "        {\"real\": 500, \"synth\": 500},\n",
    "        {\"real\": 500, \"synth\": 1000},\n",
    "        {\"real\": 500, \"synth\": 1500}\n",
    "    ], \"fixed\": [\n",
    "        {\"real\": 25, \"synth\": 475},\n",
    "        {\"real\": 25, \"synth\": 975},\n",
    "        {\"real\": 25, \"synth\": 1975}\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOT_STRATEGY = {25: \"LRS\\\\textsubscript{25}\",\n",
    "                     500: \"LRS\\\\textsubscript{500}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_number(num, decimal_places):\n",
    "    formatted_num = \"{:.{}f}\".format(num, decimal_places)\n",
    "    rounded_num_str = \"{:.{}f}\".format(float(formatted_num), decimal_places)\n",
    "    return rounded_num_str\n",
    "\n",
    "def add_thousand_dots(n_sample):\n",
    "    if isinstance(n_sample, str):\n",
    "        if '.' in n_sample:\n",
    "            integer_part, decimal_part = n_sample.split('.')\n",
    "            formatted_integer_part = \"{:,}\".format(int(integer_part))\n",
    "            result = f\"{formatted_integer_part}.{decimal_part}\"\n",
    "        else:\n",
    "            result = \"{:,}\".format(int(n_sample))\n",
    "    elif isinstance(n_sample, np.float64):\n",
    "        result = \"{:,}\".format(round(n_sample, 1))\n",
    "    else:\n",
    "        result = n_sample\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Main Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect_category :\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../07 train models/results_json/results_only_real_real500_synth0_aspect_category_random.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_real \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m2000\u001b[39m]:\n\u001b[1;32m      4\u001b[0m     json_path \u001b[38;5;241m=\u001b[39m RESULTS_PATH_BASE \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_real_real\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_real\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_synth0_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabsa_task\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_random.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m      8\u001b[0m         results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m absa_task \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_aspect_sentiment_detection\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../07 train models/results_json/results_only_real_real500_synth0_aspect_category_random.json'"
     ]
    }
   ],
   "source": [
    "for absa_task in ABSA_TASKS:\n",
    "    print(absa_task, \":\\n\")\n",
    "    for n_real in [500, 1000, 2000]:\n",
    "        json_path = RESULTS_PATH_BASE + \\\n",
    "            f\"only_real_real{n_real}_synth0_{absa_task}_random.json\"\n",
    "\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            results = json.load(json_file)\n",
    "        if absa_task != \"target_aspect_sentiment_detection\":\n",
    "            print(\n",
    "                f\"- & - & {add_thousand_dots(str(n_real))} & 0 & {add_thousand_dots(round_number(results['eval_f1_micro'], 3))} & {add_thousand_dots(round_number(results['eval_f1_macro'], 3))} & {add_thousand_dots(round_number(results['eval_accuracy'], 3))} \\\\\\\\\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"- & - & {add_thousand_dots(str(n_real))} & 0 & {add_thousand_dots(round_number(results['eval_f1'], 3))} & {add_thousand_dots(round_number(results['eval_accuracy'], 3))} \\\\\\\\\")\n",
    "\n",
    "    print(\"\\\\hline\")\n",
    "    for llm in LLMS:\n",
    "        for few_shot_condition in [\"fixed\", \"random\"]:\n",
    "            for freq in SYNTH_COMBINATIONS[few_shot_condition]:\n",
    "                n_real = freq[\"real\"]\n",
    "                n_synth = freq[\"synth\"]\n",
    "                json_path = RESULTS_PATH_BASE + llm + \\\n",
    "                    f\"_real{n_real}_synth{n_synth}_{absa_task}_{few_shot_condition}.json\"\n",
    "                with open(json_path, 'r') as json_file:\n",
    "                    results = json.load(json_file)\n",
    "                # print(f\"results: {absa_task}, {llm}, {few_shot_condition}, n_real: {n_real}, n_synth: {n_synth}\", results)\n",
    "                if absa_task != \"target_aspect_sentiment_detection\":\n",
    "                    f1_metrics = f\"{add_thousand_dots(round_number(results['eval_f1_micro'], 3))} & {add_thousand_dots(round_number(results['eval_f1_macro'], 3))}\"\n",
    "                else:\n",
    "                    f1_metrics = f\"{add_thousand_dots(round_number(results['eval_f1'], 3))}\"\n",
    "                print(\n",
    "                    f\"{LLM_PAPER_TITLE[llm]} & {FEW_SHOT_STRATEGY[n_real]} & {add_thousand_dots(str(n_real))} & {add_thousand_dots(str(n_synth))} & {f1_metrics} & {add_thousand_dots(round_number(results['eval_accuracy'], 3))} \\\\\\\\\")\n",
    "\n",
    "        print(\"\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Metrics Fine-Grained Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- & - & 500 & 0 & 0.769 &0.900 &0.905 &0.920 &0.868 &0.918 &0.588 &0.916 &0.242 &0.937  \\\\\n",
      "- & - & 1,000 & 0 & 0.800 &0.910 &0.926 &0.938 &0.902 &0.939 &0.820 &0.951 &0.862 &0.981  \\\\\n",
      "- & - & 2,000 & 0 & 0.834 &0.921 &0.934 &0.945 &0.925 &0.952 &0.838 &0.956 &0.900 &0.985  \\\\\n",
      "\\hline\n",
      "Llama-2-70B & LRS\\textsubscript{500} & 500 & 500 & 0.792 &0.905 &0.904 &0.922 &0.869 &0.921 &0.767 &0.938 &0.866 &0.980  \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{500} & 500 & 1000 & 0.796 &0.901 &0.884 &0.906 &0.881 &0.928 &0.781 &0.944 &0.855 &0.979  \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{500} & 500 & 1500 & 0.790 &0.895 &0.850 &0.885 &0.872 &0.922 &0.781 &0.940 &0.795 &0.962  \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{500} & 500 & 500 & 0.774 &0.896 &0.895 &0.913 &0.880 &0.925 &0.759 &0.936 &0.804 &0.970  \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{500} & 500 & 1000 & 0.804 &0.902 &0.898 &0.916 &0.891 &0.933 &0.795 &0.945 &0.851 &0.978  \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{500} & 500 & 1500 & 0.789 &0.890 &0.909 &0.925 &0.891 &0.932 &0.804 &0.949 &0.812 &0.968  \\\\\n",
      "\\hline\n",
      "Llama-2-70B & LRS\\textsubscript{25} & 25 & 475 & 0.581 &0.795 &0.706 &0.806 &0.699 &0.850 &0.618 &0.902 &0.652 &0.935  \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{25} & 25 & 975 & 0.645 &0.817 &0.824 &0.864 &0.768 &0.877 &0.642 &0.915 &0.706 &0.948  \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{25} & 25 & 1975 & 0.694 &0.813 &0.842 &0.876 &0.748 &0.869 &0.730 &0.932 &0.771 &0.965  \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{25} & 25 & 475 & 0.549 &0.736 &0.716 &0.808 &0.607 &0.813 &0.492 &0.875 &0.598 &0.928  \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{25} & 25 & 975 & 0.658 &0.790 &0.833 &0.867 &0.664 &0.837 &0.689 &0.919 &0.619 &0.915  \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{25} & 25 & 1975 & 0.675 &0.795 &0.855 &0.882 &0.703 &0.853 &0.734 &0.926 &0.699 &0.942  \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "absa_task = \"aspect_category\"\n",
    "for n_real in [500, 1000, 2000]:\n",
    "    json_path = RESULTS_PATH_BASE + \\\n",
    "        f\"only_real_real{n_real}_synth0_{absa_task}_random.json\"\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        results = json.load(json_file)\n",
    "    class_wise_metrics = \"\"\n",
    "    for ac in [\"GENERAL-IMPRESSION\", \"FOOD\", \"SERVICE\", \"AMBIENCE\", \"PRICE\"]:\n",
    "       for metric in [\"f1\", \"accuracy\"]:\n",
    "           class_wise_metrics += f\"{add_thousand_dots(round_number(results[f'eval_{metric}_{ac}'], 3))} &\"\n",
    "    print(f\"- & - & {add_thousand_dots(str(n_real))} & 0 & {class_wise_metrics[:-1]} \\\\\\\\\")\n",
    "print(\"\\\\hline\")\n",
    "for few_shot_condition in SYNTH_COMBINATIONS.keys():\n",
    "    for llm in LLMS:\n",
    "        for freq in SYNTH_COMBINATIONS[few_shot_condition]:\n",
    "            n_real = freq[\"real\"]\n",
    "            n_synth = freq[\"synth\"]\n",
    "            json_path = RESULTS_PATH_BASE + llm + \\\n",
    "                f\"_real{n_real}_synth{n_synth}_{absa_task}_{few_shot_condition}.json\"\n",
    "            with open(json_path, 'r') as json_file:\n",
    "                results = json.load(json_file)\n",
    "            # print(f\"results: {absa_task}, {llm}, {few_shot_condition}, n_real: {n_real}, n_synth: {n_synth}\", results)\n",
    "            if absa_task == \"TASD\":\n",
    "                f1_metrics = f\"{add_thousand_dots(results['eval_f1_micro'])} & {add_thousand_dots(results['eval_f1_macro'])}\"\n",
    "            else:\n",
    "                f1_metrics = f\"{add_thousand_dots(results['eval_f1_micro'])}\"\n",
    "\n",
    "            class_wise_metrics = \"\"\n",
    "            for ac in [\"GENERAL-IMPRESSION\", \"FOOD\", \"SERVICE\", \"AMBIENCE\", \"PRICE\"]:\n",
    "                for metric in [\"f1\", \"accuracy\"]:\n",
    "                   class_wise_metrics += f\"{add_thousand_dots(round_number(results[f'eval_{metric}_{ac}'], 3))} &\"\n",
    "\n",
    "            print(\n",
    "                    f\"{LLM_PAPER_TITLE[llm]} & {FEW_SHOT_STRATEGY[n_real]} & {add_thousand_dots(n_real)} & {add_thousand_dots(n_synth)} & {class_wise_metrics[:-1]} \\\\\\\\\")\n",
    "    print(\"\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table for: ['GENERAL-IMPRESSION', 'FOOD'] \n",
      "\n",
      "\n",
      "- & - & 500 & 0 & 0.722 & 0.938 & 0.000 & 0.992 & 0.678 & 0.942 & 0.816 & 0.913 & 0.000 & 0.964 & 0.662 & 0.905 \\\\\n",
      "- & - & 1000 & 0 & 0.786 & 0.948 & 0.000 & 0.992 & 0.747 & 0.950 & 0.877 & 0.939 & 0.168 & 0.966 & 0.723 & 0.922 \\\\\n",
      "- & - & 2000 & 0 & 0.806 & 0.953 & 0.000 & 0.992 & 0.789 & 0.957 & 0.894 & 0.949 & 0.602 & 0.976 & 0.793 & 0.932 \\\\\n",
      "\\hline\n",
      "Llama-2-70B & LRS\\textsubscript{500} & 500 & 500 & 0.743 & 0.940 & 0.000 & 0.992 & 0.693 & 0.944 & 0.782 & 0.912 & 0.090 & 0.965 & 0.676 & 0.910 \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{500} & 500 & 1000 & 0.682 & 0.936 & 0.000 & 0.992 & 0.662 & 0.940 & 0.809 & 0.917 & 0.498 & 0.974 & 0.722 & 0.920 \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{500} & 500 & 1500 & 0.747 & 0.940 & 0.259 & 0.991 & 0.728 & 0.943 & 0.734 & 0.896 & 0.453 & 0.972 & 0.739 & 0.923 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{500} & 500 & 500 & 0.766 & 0.942 & 0.250 & 0.992 & 0.744 & 0.950 & 0.825 & 0.920 & 0.567 & 0.977 & 0.714 & 0.918 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{500} & 500 & 1000 & 0.766 & 0.938 & 0.356 & 0.992 & 0.734 & 0.941 & 0.844 & 0.929 & 0.665 & 0.982 & 0.745 & 0.924 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{500} & 500 & 1500 & 0.773 & 0.944 & 0.551 & 0.994 & 0.751 & 0.950 & 0.846 & 0.929 & 0.656 & 0.980 & 0.739 & 0.922 \\\\\n",
      "\\hline\n",
      "Llama-2-70B & LRS\\textsubscript{25} & 25 & 475 & 0.051 & 0.870 & 0.000 & 0.992 & 0.011 & 0.890 & 0.096 & 0.771 & 0.000 & 0.964 & 0.045 & 0.844 \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{25} & 25 & 975 & 0.395 & 0.899 & 0.000 & 0.992 & 0.206 & 0.900 & 0.383 & 0.819 & 0.174 & 0.962 & 0.433 & 0.876 \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{25} & 25 & 1975 & 0.470 & 0.904 & 0.075 & 0.984 & 0.464 & 0.902 & 0.536 & 0.847 & 0.387 & 0.947 & 0.551 & 0.890 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{25} & 25 & 475 & 0.418 & 0.895 & 0.000 & 0.992 & 0.220 & 0.901 & 0.670 & 0.870 & 0.307 & 0.969 & 0.345 & 0.867 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{25} & 25 & 975 & 0.585 & 0.901 & 0.141 & 0.984 & 0.461 & 0.903 & 0.705 & 0.885 & 0.470 & 0.964 & 0.524 & 0.891 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{25} & 25 & 1975 & 0.631 & 0.893 & 0.185 & 0.970 & 0.569 & 0.909 & 0.774 & 0.895 & 0.485 & 0.949 & 0.576 & 0.893 \\\\\n",
      "\\hline\n",
      "Table for: ['SERVICE', 'AMBIENCE'] \n",
      "\n",
      "\n",
      "- & - & 500 & 0 & 0.821 & 0.944 & 0.000 & 0.996 & 0.667 & 0.922 & 0.649 & 0.947 & 0.000 & 0.998 & 0.000 & 0.958 \\\\\n",
      "- & - & 1000 & 0 & 0.893 & 0.964 & 0.000 & 0.996 & 0.799 & 0.944 & 0.810 & 0.966 & 0.000 & 0.998 & 0.529 & 0.973 \\\\\n",
      "- & - & 2000 & 0 & 0.907 & 0.968 & 0.000 & 0.996 & 0.845 & 0.953 & 0.840 & 0.971 & 0.000 & 0.998 & 0.696 & 0.978 \\\\\n",
      "\\hline\n",
      "Llama-2-70B & LRS\\textsubscript{500} & 500 & 500 & 0.748 & 0.929 & 0.178 & 0.993 & 0.772 & 0.937 & 0.717 & 0.959 & 0.000 & 0.996 & 0.317 & 0.966 \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{500} & 500 & 1000 & 0.792 & 0.940 & 0.247 & 0.994 & 0.740 & 0.930 & 0.739 & 0.959 & 0.062 & 0.996 & 0.545 & 0.972 \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{500} & 500 & 1500 & 0.791 & 0.940 & 0.036 & 0.988 & 0.779 & 0.940 & 0.644 & 0.950 & 0.000 & 0.996 & 0.635 & 0.974 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{500} & 500 & 500 & 0.861 & 0.956 & 0.493 & 0.997 & 0.772 & 0.936 & 0.762 & 0.962 & 0.312 & 0.996 & 0.612 & 0.975 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{500} & 500 & 1000 & 0.887 & 0.964 & 0.433 & 0.997 & 0.766 & 0.935 & 0.786 & 0.965 & 0.310 & 0.997 & 0.681 & 0.977 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{500} & 500 & 1500 & 0.879 & 0.962 & 0.693 & 0.998 & 0.806 & 0.941 & 0.789 & 0.964 & 0.417 & 0.998 & 0.667 & 0.976 \\\\\n",
      "\\hline\n",
      "Llama-2-70B & LRS\\textsubscript{25} & 25 & 475 & 0.353 & 0.868 & 0.024 & 0.990 & 0.173 & 0.866 & 0.309 & 0.924 & 0.000 & 0.997 & 0.108 & 0.961 \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{25} & 25 & 975 & 0.552 & 0.900 & 0.036 & 0.988 & 0.490 & 0.897 & 0.430 & 0.931 & 0.042 & 0.990 & 0.427 & 0.968 \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{25} & 25 & 1975 & 0.662 & 0.915 & 0.087 & 0.952 & 0.606 & 0.910 & 0.333 & 0.923 & 0.138 & 0.991 & 0.498 & 0.965 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{25} & 25 & 475 & 0.534 & 0.891 & 0.320 & 0.997 & 0.176 & 0.864 & 0.400 & 0.929 & 0.125 & 0.998 & 0.209 & 0.964 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{25} & 25 & 975 & 0.707 & 0.922 & 0.272 & 0.990 & 0.460 & 0.894 & 0.445 & 0.931 & 0.450 & 0.995 & 0.470 & 0.971 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{25} & 25 & 1975 & 0.729 & 0.927 & 0.265 & 0.988 & 0.480 & 0.898 & 0.510 & 0.936 & 0.375 & 0.994 & 0.545 & 0.969 \\\\\n",
      "\\hline\n",
      "Table for: ['PRICE'] \n",
      "\n",
      "\n",
      "- & - & 500 & 0 & 0.000 & 0.986 & 0.000 & 0.994 & 0.249 & 0.952 \\\\\n",
      "- & - & 1000 & 0 & 0.056 & 0.986 & 0.000 & 0.994 & 0.753 & 0.976 \\\\\n",
      "- & - & 2000 & 0 & 0.310 & 0.988 & 0.000 & 0.994 & 0.729 & 0.975 \\\\\n",
      "\\hline\n",
      "Llama-2-70B & LRS\\textsubscript{500} & 500 & 500 & 0.534 & 0.989 & 0.159 & 0.994 & 0.596 & 0.965 \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{500} & 500 & 1000 & 0.507 & 0.988 & 0.288 & 0.993 & 0.660 & 0.971 \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{500} & 500 & 1500 & 0.653 & 0.992 & 0.358 & 0.990 & 0.728 & 0.972 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{500} & 500 & 500 & 0.386 & 0.988 & 0.222 & 0.994 & 0.722 & 0.972 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{500} & 500 & 1000 & 0.611 & 0.990 & 0.361 & 0.995 & 0.708 & 0.971 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{500} & 500 & 1500 & 0.499 & 0.985 & 0.469 & 0.993 & 0.687 & 0.970 \\\\\n",
      "\\hline\n",
      "Llama-2-70B & LRS\\textsubscript{25} & 25 & 475 & 0.056 & 0.986 & 0.000 & 0.994 & 0.192 & 0.951 \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{25} & 25 & 975 & 0.458 & 0.987 & 0.000 & 0.992 & 0.486 & 0.960 \\\\\n",
      "Llama-2-70B & LRS\\textsubscript{25} & 25 & 1975 & 0.623 & 0.990 & 0.206 & 0.994 & 0.577 & 0.964 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{25} & 25 & 475 & 0.089 & 0.985 & 0.111 & 0.993 & 0.253 & 0.952 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{25} & 25 & 975 & 0.366 & 0.988 & 0.000 & 0.993 & 0.536 & 0.962 \\\\\n",
      "GPT-3.5-turbo & LRS\\textsubscript{25} & 25 & 1975 & 0.454 & 0.986 & 0.248 & 0.993 & 0.498 & 0.957 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "absa_task = \"aspect_category_sentiment\"\n",
    "\n",
    "idx = 0\n",
    "for aspect_categories in [[\"GENERAL-IMPRESSION\", \"FOOD\"], [\"SERVICE\", \"AMBIENCE\"], [\"PRICE\"]]:\n",
    "    print(\"Table for:\", aspect_categories, \"\\n\\n\")\n",
    "    for n_real in [500, 1000, 2000]:\n",
    "        json_path = RESULTS_PATH_BASE + \\\n",
    "            f\"only_real_real{n_real}_synth0_{absa_task}_random.json\"\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            results = json.load(json_file)\n",
    "\n",
    "        condition_string = f\"- & - & {add_thousand_dots(n_real)} & 0 &\"\n",
    "        metrics_class_wise = \"\"\n",
    "        for ac in aspect_categories:\n",
    "            for polarity in [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"]:\n",
    "                for metric in [\"eval_f1\", \"eval_accuracy\"]:\n",
    "                    metrics_class_wise += f\" {add_thousand_dots(round_number(results[f'{metric}_{ac}-{polarity}'], 3))} &\"\n",
    "\n",
    "        print(condition_string + metrics_class_wise[:-1] + \"\\\\\\\\\")\n",
    "\n",
    "    print(\"\\\\hline\")\n",
    "    for few_shot_condition in SYNTH_COMBINATIONS.keys():\n",
    "        for llm in LLMS:\n",
    "            for freq in SYNTH_COMBINATIONS[few_shot_condition]:\n",
    "                n_real = freq[\"real\"]\n",
    "                n_synth = freq[\"synth\"]\n",
    "                json_path = RESULTS_PATH_BASE + llm + \\\n",
    "                    f\"_real{n_real}_synth{n_synth}_{absa_task}_{few_shot_condition}.json\"\n",
    "                with open(json_path, 'r') as json_file:\n",
    "                    results = json.load(json_file)\n",
    "\n",
    "                condition_string = f\"{LLM_PAPER_TITLE[llm]} & {FEW_SHOT_STRATEGY[n_real]} & {add_thousand_dots(n_real)} & {add_thousand_dots(n_synth)} &\"\n",
    "                metrics_class_wise = \"\"\n",
    "                for ac in aspect_categories:\n",
    "                    for polarity in [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"]:\n",
    "                        for metric in [\"eval_f1\", \"eval_accuracy\"]:\n",
    "                            metrics_class_wise += f\" {add_thousand_dots(round_number(results[f'{metric}_{ac}-{polarity}'], 3))} &\"\n",
    "\n",
    "                print(condition_string + metrics_class_wise[:-1] + \"\\\\\\\\\")\n",
    "        print(\"\\hline\")\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
