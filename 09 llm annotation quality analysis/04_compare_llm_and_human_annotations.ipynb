{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Compare LLM and Human Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../07 train models/'))\n",
    "from TASD.evaluation import calculate_metrics_for_examples\n",
    "import constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMS = [\"Llama3_70B\", \"GPT-3\"]\n",
    "FEW_SHOT_CONDITIONS = [\"fixed\", \"random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMS_ENCODED = {\"GPT-3\": \"\\\\textbf{GPT-3.5-turbo}\", \"Llama3_70B\": \"\\\\textbf{Llama-3-70B}\"}\n",
    "ENCODE_CONDITION = {\"fixed\": \"\\\\textbf{LRS\\\\textsubscript{25}}\",\n",
    "                    \"random\": \"\\\\textbf{LRS\\\\textsubscript{500}}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"annotation_datasets/annotated_synth_dataset.json\", 'r') as json_file:\n",
    "    human_annotations = json.load(json_file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Synthetic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_annotations = []\n",
    "\n",
    "for llm in LLMS:\n",
    "    for fs in [\"random\", \"fixed\"]:\n",
    "       for split_id in range(6):\n",
    "           with open(f\"../07 train models/synth/{llm}/{fs}/split_{split_id}.json\", 'r') as json_file:\n",
    "              synthetic_data_split = json.load(json_file)\n",
    "              for example in  synthetic_data_split:\n",
    "                  llm_annotations.append(example)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_annotations_aspects = [([{\"aspect_category\": tag[\"label\"], \"aspect_polarity\": tag[\"polarity\"],\n",
    "                              \"aspect_term\": tag[\"text\"] if tag[\"text\"] != 'NULL' else None, \"start\": tag[\"start\"], \"end\": tag[\"end\"]} for tag in example[\"tags\"]], example[\"id\"]) for example in llm_annotations]\n",
    "human_annotations_aspects = [([{\"aspect_category\": tag[\"label\"], \"aspect_polarity\": tag[\"polarity\"], \"aspect_term\": tag[\"text\"]\n",
    "                                if tag[\"text\"] != 'NULL' else None, \"start\": tag[\"start\"], \"end\": tag[\"end\"]} for tag in example[\"tags\"]], example[\"id\"], example[\"model\"], example[\"few_shot_condtion\"]) for example in human_annotations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_with_id(id, dataset):\n",
    "    return [example for example in dataset if example[1] == id][0][0]\n",
    "\n",
    "for llm in LLMS:\n",
    "    dataset[llm] = {}\n",
    "    for fs in FEW_SHOT_CONDITIONS:\n",
    "        dataset[llm][fs] = {}\n",
    "        human_annotations_aspects_ids = [example[1] for example in human_annotations_aspects if example[2] == llm and example[3] == fs]\n",
    "        human_annotations_samples = [example[0] for example in human_annotations_aspects if example[2] == llm and example[3] == fs]\n",
    "        llm_annotations_samples = [get_example_with_id(id, llm_annotations_aspects) for id in human_annotations_aspects_ids]\n",
    "\n",
    "        dataset[llm][fs][\"human_annotation\"] = human_annotations_samples\n",
    "        dataset[llm][fs][\"llm_annotation\"] = llm_annotations_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect Term Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tp_tn_fp_fn_aspect_term(pred, label):\n",
    "    pred_set = set(\n",
    "        f\"{range['start']}_{range['end']}\" for range in pred)\n",
    "    label_set = set(\n",
    "        f\"{range['start']}_{range['end']}\" for range in label)\n",
    "\n",
    "    tp_set = pred_set & label_set\n",
    "    tp = len(tp_set)\n",
    "\n",
    "    fp_set = pred_set - tp_set\n",
    "    fp = len(fp_set)\n",
    "\n",
    "    fn_set = label_set - tp_set\n",
    "    fn = len(fn_set)\n",
    "\n",
    "    return tp, 0, fp, fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{2}{*}{\\textbf{Llama-3-70B}} & \\textbf{LRS\\textsubscript{25}} & 84.12 & 89.02 & 79.74 \\\\\n",
      " & \\textbf{LRS\\textsubscript{500}} & 81.52 & 88.65 & 75.46 \\\\\n",
      "\\hline\n",
      "\\multirow{2}{*}{\\textbf{GPT-3.5-turbo}} & \\textbf{LRS\\textsubscript{25}} & 75.11 & 91.44 & 63.72 \\\\\n",
      " & \\textbf{LRS\\textsubscript{500}} & 72.61 & 92.44 & 59.78 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for llm_idx, llm in enumerate(LLMS):\n",
    "    for fs_idx, fs in enumerate(FEW_SHOT_CONDITIONS):\n",
    "        llm_annotations_aspect_terms = [\n",
    "            [{\"start\": tag[\"start\"], \"end\": tag[\"end\"]} for tag in example if tag[\"aspect_term\"] is not None] for example in dataset[llm][fs][\"llm_annotation\"]]\n",
    "        human_annotations_aspect_terms = [\n",
    "            [{\"start\": tag[\"start\"], \"end\": tag[\"end\"]} for tag in example if tag[\"aspect_term\"] is not None] for example in dataset[llm][fs][\"human_annotation\"]]\n",
    "\n",
    "        tp_total = tn_total = fp_total = fn_total = 0\n",
    "        for i in range(len(human_annotations_aspect_terms)):\n",
    "            tp, tn, fp, fn = calculate_tp_tn_fp_fn_aspect_term(\n",
    "                llm_annotations_aspect_terms[i], human_annotations_aspect_terms[i])\n",
    "            tp_total += tp\n",
    "            tn_total += tn\n",
    "            fp_total += fp\n",
    "            fn_total += fn\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = (tp_total + tn_total) / (tp_total + tn_total + fp_total +\n",
    "                                            fn_total) if (tp_total + tn_total + fp_total + fn_total) > 0 else 0\n",
    "        precision = tp_total / \\\n",
    "            (tp_total + fp_total) if (tp_total + fp_total) > 0 else 0\n",
    "        recall = tp_total / \\\n",
    "            (tp_total + fn_total) if (tp_total + fn_total) > 0 else 0\n",
    "\n",
    "        f1 = 2 * tp_total / (2 * tp_total + fn_total + fp_total)\n",
    "\n",
    "        llm_print = \"\\\\multirow{2}{*}{\" + \\\n",
    "            LLMS_ENCODED[llm] + \"}\" if fs_idx == 0 else \"\"\n",
    "\n",
    "        fs_print = ENCODE_CONDITION[fs]\n",
    "\n",
    "        print(llm_print, \"&\", fs_print,\n",
    "              \"&\", \"{:.2f}\".format(f1*100),\n",
    "              \"&\", \"{:.2f}\".format(precision*100),\n",
    "              \"&\", \"{:.2f}\".format(recall*100), \"\\\\\\\\\")\n",
    "    print(\"\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_list_to_label(cat_list):\n",
    "    return [1 if cat in cat_list else 0 for cat in constants.ASPECT_CATEGORIES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{2}{*}{\\textbf{Llama-3-70B}} & \\textbf{LRS\\textsubscript{25}} & 85.90 & 85.26 \\\\\n",
      " & \\textbf{LRS\\textsubscript{500}} & 85.86 & 85.53 \\\\\n",
      "\\hline\n",
      "\\multirow{2}{*}{\\textbf{GPT-3.5-turbo}} & \\textbf{LRS\\textsubscript{25}} & 95.56 & 95.42 \\\\\n",
      " & \\textbf{LRS\\textsubscript{500}} & 95.60 & 95.61 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for llm_idx, llm in enumerate(LLMS):\n",
    "    for fs_idx, fs in enumerate(FEW_SHOT_CONDITIONS):\n",
    "        llm_annotations_aspect_categories = [category_list_to_label(\n",
    "            [tag[\"aspect_category\"] for tag in example]) for example in dataset[llm][fs][\"llm_annotation\"]]\n",
    "        human_annotations_aspect_categories = [category_list_to_label(\n",
    "            [tag[\"aspect_category\"] for tag in example]) for example in dataset[llm][fs][\"human_annotation\"]]\n",
    "\n",
    "        accuracy = accuracy_score(\n",
    "            human_annotations_aspect_categories, llm_annotations_aspect_categories)\n",
    "        f1_micro = f1_score(human_annotations_aspect_categories,\n",
    "                            llm_annotations_aspect_categories, average='micro')\n",
    "        f1_macro = f1_score(human_annotations_aspect_categories,\n",
    "                            llm_annotations_aspect_categories, average='macro')\n",
    "\n",
    "        llm_print = \"\\\\multirow{2}{*}{\" + \\\n",
    "            LLMS_ENCODED[llm] + \"}\" if fs_idx == 0 else \"\"\n",
    "\n",
    "        print(llm_print, \"&\", ENCODE_CONDITION[fs],\n",
    "              \"&\", \"{:.2f}\".format(f1_micro*100),\n",
    "              \"&\", \"{:.2f}\".format(f1_macro*100), \"\\\\\\\\\")\n",
    "    print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect Category (performance for each Aspect Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{10}{*}{\\textbf{Llama-3-70B}} & \\multirow{5}{*}{\\textbf{LRS\\textsubscript{25}}} & \\texttt{GENERAL-IMPRESSION} & 68.99 & 85.17 & 53.80 & 96.12 \\\\\n",
      " &  & \\texttt{FOOD} & 85.25 & 91.00 & 91.76 & 79.59 \\\\\n",
      " &  & \\texttt{SERVICE} & 92.26 & 95.50 & 94.71 & 89.94 \\\\\n",
      " &  & \\texttt{AMBIENCE} & 86.16 & 92.67 & 90.13 & 82.53 \\\\\n",
      " &  & \\texttt{PRICE} & 93.63 & 96.17 & 92.86 & 94.41 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-7}\\arrayrulecolor{black}\n",
      " & \\multirow{5}{*}{\\textbf{LRS\\textsubscript{500}}} & \\texttt{GENERAL-IMPRESSION} & 75.27 & 88.50 & 63.64 & 92.11 \\\\\n",
      " &  & \\texttt{FOOD} & 83.08 & 90.83 & 88.82 & 78.03 \\\\\n",
      " &  & \\texttt{SERVICE} & 91.07 & 95.00 & 95.62 & 86.93 \\\\\n",
      " &  & \\texttt{AMBIENCE} & 86.21 & 93.33 & 88.65 & 83.89 \\\\\n",
      " &  & \\texttt{PRICE} & 92.02 & 95.67 & 91.46 & 92.59 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-7}\\arrayrulecolor{black}\n",
      "\\hline\n",
      "\\multirow{10}{*}{\\textbf{GPT-3.5-turbo}} & \\multirow{5}{*}{\\textbf{LRS\\textsubscript{25}}} & \\texttt{GENERAL-IMPRESSION} & 92.07 & 95.67 & 87.28 & 97.42 \\\\\n",
      " &  & \\texttt{FOOD} & 93.26 & 96.00 & 99.40 & 87.83 \\\\\n",
      " &  & \\texttt{SERVICE} & 98.06 & 98.83 & 99.44 & 96.72 \\\\\n",
      " &  & \\texttt{AMBIENCE} & 94.23 & 97.00 & 98.66 & 90.18 \\\\\n",
      " &  & \\texttt{PRICE} & 99.47 & 99.67 & 98.94 & 100.00 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-7}\\arrayrulecolor{black}\n",
      " & \\multirow{5}{*}{\\textbf{LRS\\textsubscript{500}}} & \\texttt{GENERAL-IMPRESSION} & 94.39 & 97.17 & 89.94 & 99.31 \\\\\n",
      " &  & \\texttt{FOOD} & 89.78 & 94.50 & 99.32 & 81.92 \\\\\n",
      " &  & \\texttt{SERVICE} & 97.11 & 98.50 & 98.69 & 95.57 \\\\\n",
      " &  & \\texttt{AMBIENCE} & 97.69 & 98.83 & 98.67 & 96.73 \\\\\n",
      " &  & \\texttt{PRICE} & 99.08 & 99.50 & 98.18 & 100.00 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-7}\\arrayrulecolor{black}\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "def category_list_to_label(cat_list):\n",
    "    return [1 if cat in cat_list else 0 for cat in constants.ASPECT_CATEGORIES]\n",
    "\n",
    "\n",
    "for llm_idx, llm in enumerate(LLMS):\n",
    "    for fs_idx, fs in enumerate(FEW_SHOT_CONDITIONS):\n",
    "        llm_annotations_aspect_categories = [category_list_to_label(\n",
    "            [tag[\"aspect_category\"] for tag in example]) for example in dataset[llm][fs][\"llm_annotation\"]]\n",
    "        human_annotations_aspect_categories = [category_list_to_label(\n",
    "            [tag[\"aspect_category\"] for tag in example]) for example in dataset[llm][fs][\"human_annotation\"]]\n",
    "\n",
    "        for ac_idx, aspect_category in enumerate(constants.ASPECT_CATEGORIES):\n",
    "            idx = constants.ASPECT_CATEGORIES.index(aspect_category)\n",
    "\n",
    "            tp = sum((llm_annotations_aspect_categories[i][idx] == 1) and (\n",
    "                human_annotations_aspect_categories[i][idx] == 1) for i in range(len(llm_annotations_aspect_categories)))\n",
    "            fp = sum((llm_annotations_aspect_categories[i][idx] == 1) and (\n",
    "                human_annotations_aspect_categories[i][idx] == 0) for i in range(len(llm_annotations_aspect_categories)))\n",
    "            fn = sum((llm_annotations_aspect_categories[i][idx] == 0) and (\n",
    "                human_annotations_aspect_categories[i][idx] == 1) for i in range(len(llm_annotations_aspect_categories)))\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            accuracy = accuracy_score([example[idx] for example in human_annotations_aspect_categories], [\n",
    "                                      example[idx] for example in llm_annotations_aspect_categories])\n",
    "\n",
    "            f1 = 2 * tp / (2 * tp + fn + fp)\n",
    "\n",
    "            n_samples_in_class = sum(\n",
    "                example[idx] == 1 for example in human_annotations_aspect_categories)\n",
    "\n",
    "            llm_print = \"\\\\multirow{10}{*}{\" + \\\n",
    "                LLMS_ENCODED[llm] + \"}\" if fs_idx == 0 and ac_idx == 0 else \"\"\n",
    "            fs_print = \"\\\\multirow{5}{*}{\" + \\\n",
    "                ENCODE_CONDITION[fs] + \"}\" if ac_idx == 0 else \"\"\n",
    "\n",
    "            print(llm_print, \"&\", fs_print,\n",
    "                  \"&\", \"\\\\texttt{\"+aspect_category+\"}\",\n",
    "                  \"&\", \"{:.2f}\".format(f1*100),\n",
    "                  \"&\", \"{:.2f}\".format(accuracy*100),\n",
    "                  \"&\", \"{:.2f}\".format(precision*100),\n",
    "                  \"&\", \"{:.2f}\".format(recall*100), \"\\\\\\\\\")\n",
    "\n",
    "        print(\"\\\\arrayrulecolor{gray}\\cline{2-7}\\\\arrayrulecolor{black}\")\n",
    "\n",
    "    print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect Category + Sentiment Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC_POLARITY_COMBINATIONS = [cat+\"_\"+polarity for cat in constants.ASPECT_CATEGORIES for polarity in [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\", \"CONFLICT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_polarity_list_to_label(cat_pol_list):\n",
    "    return [1 if ac_pol in cat_pol_list else 0 for ac_pol in AC_POLARITY_COMBINATIONS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{2}{*}{\\textbf{Llama-3-70B}} & \\textbf{LRS\\textsubscript{25}} & 58.98 & 49.72 \\\\\n",
      " & \\textbf{LRS\\textsubscript{500}} & 61.68 & 48.67 \\\\\n",
      "\\hline\n",
      "\\multirow{2}{*}{\\textbf{GPT-3.5-turbo}} & \\textbf{LRS\\textsubscript{25}} & 84.37 & 71.46 \\\\\n",
      " & \\textbf{LRS\\textsubscript{500}} & 83.66 & 77.60 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for llm_idx, llm in enumerate(LLMS):\n",
    "    for fs_idx, fs in enumerate(FEW_SHOT_CONDITIONS):\n",
    "        llm_annotations_ac_pol = [category_polarity_list_to_label(\n",
    "            [tag[\"aspect_category\"]+\"_\"+tag[\"aspect_polarity\"] for tag in example]) for example in dataset[llm][fs][\"llm_annotation\"]]\n",
    "        human_annotations_ac_pol = [category_polarity_list_to_label(\n",
    "            [tag[\"aspect_category\"]+\"_\"+tag[\"aspect_polarity\"] for tag in example]) for example in dataset[llm][fs][\"human_annotation\"]]\n",
    "\n",
    "        accuracy = accuracy_score(\n",
    "            human_annotations_ac_pol, llm_annotations_ac_pol)\n",
    "\n",
    "        f1_micro = f1_score(human_annotations_ac_pol,\n",
    "                            llm_annotations_ac_pol, average='micro', zero_division=0)\n",
    "\n",
    "        f1_macro_dedicated = []\n",
    "        for i, aspect_category_sentiment in enumerate(constants.ASPECT_CATEGORY_POLARITIES):\n",
    "            class_labels = [label[i] for label in human_annotations_ac_pol]\n",
    "            class_predictions = [prediction[i] for prediction in llm_annotations_ac_pol]\n",
    "\n",
    "            f1 = f1_score(class_labels, class_predictions, zero_division=0)\n",
    "\n",
    "            if all(el == 0 for el in class_labels):\n",
    "                pass\n",
    "            else:\n",
    "                f1_macro_dedicated.append(f1)\n",
    "        f1_macro = np.mean(f1_macro_dedicated)\n",
    "\n",
    "        llm_print = \"\\\\multirow{2}{*}{\" + \\\n",
    "            LLMS_ENCODED[llm] + \"}\" if fs_idx == 0 else \"\"\n",
    "\n",
    "        print(llm_print,\n",
    "              \"&\", ENCODE_CONDITION[fs],\n",
    "              \"&\", \"{:.2f}\".format(f1_micro*100),\n",
    "              \"&\", \"{:.2f}\".format(f1_macro*100), \"\\\\\\\\\")\n",
    "    print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{30}{*}{\\textbf{Llama-3-70B}} & \\multirow{15}{*}{\\textbf{LRS\\textsubscript{25}}} & \\multirow{3}{*}{\\texttt{GENERAL-IMPRESSION}} & \\texttt{POSITIVE} & 66.13 & 93.00 & 58.57 & 75.93 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 57.14 & 92.50 & 44.78 & 78.95 \\\\\n",
      " &  &  & \\texttt{NEUTRAL} & 26.67 & 90.83 & 15.87 & 83.33 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  &  & \\texttt{POSITIVE} & 67.01 & 89.33 & 89.04 & 53.72 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 69.81 & 94.67 & 72.55 & 67.27 \\\\\n",
      " &  & \\multirow{3}{*}{\\texttt{FOOD}} & \\texttt{NEUTRAL} & 43.18 & 91.67 & 31.67 & 67.86 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  &  & \\texttt{POSITIVE} & 63.95 & 89.67 & 94.83 & 48.25 \\\\\n",
      " &  & \\multirow{3}{*}{\\texttt{SERVICE}} & \\texttt{NEGATIVE} & 83.17 & 97.17 & 85.71 & 80.77 \\\\\n",
      " &  &  & \\texttt{NEUTRAL} & 23.08 & 90.00 & 13.04 & 100.00 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  & \\multirow{3}{*}{\\texttt{AMBIENCE}} & \\texttt{POSITIVE} & 59.21 & 89.67 & 86.54 & 45.00 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 67.77 & 93.50 & 75.93 & 61.19 \\\\\n",
      " &  &  & \\texttt{NEUTRAL} & 19.18 & 90.17 & 12.07 & 46.67 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  &  & \\texttt{POSITIVE} & 65.19 & 92.17 & 68.75 & 61.97 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 77.37 & 94.83 & 80.30 & 74.65 \\\\\n",
      " &  & \\multirow{3}{*}{\\texttt{PRICE}} & \\texttt{NEUTRAL} & 48.15 & 90.67 & 39.39 & 61.90 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      "\\cline{2-8}\n",
      " & \\multirow{15}{*}{\\textbf{LRS\\textsubscript{500}}} & \\multirow{3}{*}{\\texttt{GENERAL-IMPRESSION}} & \\texttt{POSITIVE} & 67.74 & 93.33 & 71.19 & 64.62 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 63.16 & 94.17 & 51.72 & 81.08 \\\\\n",
      " &  &  & \\texttt{NEUTRAL} & 29.85 & 92.17 & 18.18 & 83.33 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  &  & \\texttt{POSITIVE} & 64.52 & 90.83 & 84.75 & 52.08 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 74.14 & 95.00 & 78.18 & 70.49 \\\\\n",
      " &  & \\multirow{3}{*}{\\texttt{FOOD}} & \\texttt{NEUTRAL} & 45.71 & 93.67 & 34.04 & 69.57 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  &  & \\texttt{POSITIVE} & 64.00 & 91.00 & 90.57 & 49.48 \\\\\n",
      " &  & \\multirow{3}{*}{\\texttt{SERVICE}} & \\texttt{NEGATIVE} & 83.72 & 96.50 & 87.10 & 80.60 \\\\\n",
      " &  &  & \\texttt{NEUTRAL} & 28.12 & 92.33 & 16.67 & 90.00 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  & \\multirow{3}{*}{\\texttt{AMBIENCE}} & \\texttt{POSITIVE} & 69.80 & 92.50 & 94.55 & 55.32 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 66.67 & 95.33 & 71.79 & 62.22 \\\\\n",
      " &  &  & \\texttt{NEUTRAL} & 23.88 & 91.50 & 14.81 & 61.54 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  &  & \\texttt{POSITIVE} & 66.07 & 93.67 & 61.67 & 71.15 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 73.13 & 94.00 & 75.38 & 71.01 \\\\\n",
      " &  & \\multirow{3}{*}{\\texttt{PRICE}} & \\texttt{NEUTRAL} & 48.42 & 91.83 & 46.00 & 51.11 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      "\\cline{2-8}\n",
      "\\hline\n",
      "\\multirow{30}{*}{\\textbf{GPT-3.5-turbo}} & \\multirow{15}{*}{\\textbf{LRS\\textsubscript{25}}} & \\multirow{3}{*}{\\texttt{GENERAL-IMPRESSION}} & \\texttt{POSITIVE} & 79.31 & 96.00 & 71.88 & 88.46 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 89.29 & 98.00 & 83.33 & 96.15 \\\\\n",
      " &  &  & \\texttt{NEUTRAL} & 86.96 & 97.50 & 78.12 & 98.04 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  &  & \\texttt{POSITIVE} & 85.31 & 96.50 & 100.00 & 74.39 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 99.07 & 99.83 & 100.00 & 98.15 \\\\\n",
      " &  & \\multirow{3}{*}{\\texttt{FOOD}} & \\texttt{NEUTRAL} & 96.06 & 99.17 & 93.85 & 98.39 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  &  & \\texttt{POSITIVE} & 90.91 & 98.17 & 91.67 & 90.16 \\\\\n",
      " &  & \\multirow{3}{*}{\\texttt{SERVICE}} & \\texttt{NEGATIVE} & 94.74 & 98.83 & 96.92 & 92.65 \\\\\n",
      " &  &  & \\texttt{NEUTRAL} & 91.53 & 98.33 & 84.38 & 100.00 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  & \\multirow{3}{*}{\\texttt{AMBIENCE}} & \\texttt{POSITIVE} & 81.90 & 96.83 & 95.56 & 71.67 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 93.85 & 98.67 & 95.31 & 92.42 \\\\\n",
      " &  &  & \\texttt{NEUTRAL} & 82.98 & 97.33 & 75.00 & 92.86 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  &  & \\texttt{POSITIVE} & 65.03 & 90.50 & 86.89 & 51.96 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 93.71 & 98.50 & 98.53 & 89.33 \\\\\n",
      " &  & \\multirow{3}{*}{\\texttt{PRICE}} & \\texttt{NEUTRAL} & 28.57 & 89.17 & 16.67 & 100.00 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      "\\cline{2-8}\n",
      " & \\multirow{15}{*}{\\textbf{LRS\\textsubscript{500}}} & \\multirow{3}{*}{\\texttt{GENERAL-IMPRESSION}} & \\texttt{POSITIVE} & 86.32 & 97.83 & 83.67 & 89.13 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 90.91 & 98.50 & 86.54 & 95.74 \\\\\n",
      " &  &  & \\texttt{NEUTRAL} & 88.50 & 97.83 & 80.65 & 98.04 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  &  & \\texttt{POSITIVE} & 80.82 & 95.33 & 98.33 & 68.60 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 96.15 & 99.33 & 96.15 & 96.15 \\\\\n",
      " &  & \\multirow{3}{*}{\\texttt{FOOD}} & \\texttt{NEUTRAL} & 92.50 & 99.00 & 97.37 & 88.10 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  &  & \\texttt{POSITIVE} & 91.89 & 98.50 & 91.07 & 92.73 \\\\\n",
      " &  & \\multirow{3}{*}{\\texttt{SERVICE}} & \\texttt{NEGATIVE} & 92.63 & 98.83 & 93.62 & 91.67 \\\\\n",
      " &  &  & \\texttt{NEUTRAL} & 94.83 & 99.00 & 91.67 & 98.21 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  & \\multirow{3}{*}{\\texttt{AMBIENCE}} & \\texttt{POSITIVE} & 88.24 & 98.00 & 93.75 & 83.33 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 94.74 & 99.00 & 94.74 & 94.74 \\\\\n",
      " &  &  & \\texttt{NEUTRAL} & 88.89 & 98.17 & 81.48 & 97.78 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      " &  &  & \\texttt{POSITIVE} & 53.90 & 89.17 & 65.52 & 45.78 \\\\\n",
      " &  &  & \\texttt{NEGATIVE} & 84.62 & 96.67 & 96.49 & 75.34 \\\\\n",
      " &  & \\multirow{3}{*}{\\texttt{PRICE}} & \\texttt{NEUTRAL} & 23.88 & 91.50 & 13.56 & 100.00 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-8}\\arrayrulecolor{black}\n",
      "\\cline{2-8}\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "def category_polarity_list_to_label(cat_polarity_list):\n",
    "    return [1 if cat_polarity in cat_polarity_list else 0 for cat_polarity in AC_POLARITY_COMBINATIONS]\n",
    "\n",
    "\n",
    "for llm_idx, llm in enumerate(LLMS):\n",
    "    for fs_idx, fs in enumerate(FEW_SHOT_CONDITIONS):\n",
    "        llm_annotations_ac_pol = [category_polarity_list_to_label(\n",
    "            [tag[\"aspect_category\"] + \"_\" + tag[\"aspect_polarity\"] for tag in example]) for example in dataset[llm][fs][\"llm_annotation\"]]\n",
    "        human_annotations_ac_pol = [category_polarity_list_to_label(\n",
    "            [tag[\"aspect_category\"] + \"_\" + tag[\"aspect_polarity\"] for tag in example]) for example in dataset[llm][fs][\"human_annotation\"]]\n",
    "\n",
    "        for ac_pol_idx, ac_pol_combination in enumerate(AC_POLARITY_COMBINATIONS):\n",
    "            idx = AC_POLARITY_COMBINATIONS.index(ac_pol_combination)\n",
    "\n",
    "            tp = sum((llm_annotations_ac_pol[i][idx] == 1) and (\n",
    "                human_annotations_ac_pol[i][idx] == 1) for i in range(len(llm_annotations_ac_pol)))\n",
    "            fp = sum((llm_annotations_ac_pol[i][idx] == 1) and (\n",
    "                human_annotations_ac_pol[i][idx] == 0) for i in range(len(llm_annotations_ac_pol)))\n",
    "            fn = sum((llm_annotations_ac_pol[i][idx] == 0) and (\n",
    "                human_annotations_ac_pol[i][idx] == 1) for i in range(len(llm_annotations_ac_pol)))\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            accuracy = accuracy_score([example[idx] for example in human_annotations_ac_pol], [\n",
    "                                      example[idx] for example in llm_annotations_ac_pol])\n",
    "            try:\n",
    "                f1 = 2 * tp / (2 * tp + fn + fp)\n",
    "            except:\n",
    "                f1 = 0\n",
    "\n",
    "            n_samples_in_class = sum(\n",
    "                example[idx] == 1 for example in human_annotations_ac_pol)\n",
    "\n",
    "            aspect_category, sentiment_polarity = ac_pol_combination.split(\"_\")\n",
    "\n",
    "            llm_print = \"\\\\multirow{30}{*}{\" + \\\n",
    "                LLMS_ENCODED[llm] + \\\n",
    "                \"}\" if fs_idx == 0 and ac_pol_idx == 0 else \"\"\n",
    "            fs_print = \"\\\\multirow{15}{*}{\" + \\\n",
    "                ENCODE_CONDITION[fs] + \"}\" if ac_pol_idx == 0 else \"\"\n",
    "            ac_print = \"\\\\multirow{3}{*}{\" + \"\\\\texttt{\" + \\\n",
    "                aspect_category+\"}\" + \"}\" if ac_pol_idx % 3 == 0 else \"\"\n",
    "\n",
    "            if sentiment_polarity != \"CONFLICT\":\n",
    "                print(llm_print,\n",
    "                      \"&\", fs_print,\n",
    "                      \"&\", ac_print,\n",
    "                      \"&\", \"\\\\texttt{\"+sentiment_polarity+\"}\",\n",
    "                      \"&\", \"{:.2f}\".format(f1*100),\n",
    "                      \"&\", \"{:.2f}\".format(accuracy*100),\n",
    "                      \"&\", \"{:.2f}\".format(precision*100),\n",
    "                      \"&\", \"{:.2f}\".format(recall*100), \"\\\\\\\\\")\n",
    "\n",
    "            if ac_pol_idx % 4 == 2:\n",
    "                print(\n",
    "                    \"\\\\arrayrulecolor{gray}\\cline{3-8}\\\\arrayrulecolor{black}\")\n",
    "\n",
    "        print(\"\\\\cline{2-8}\")\n",
    "    print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect Term + Sentiment Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tp_tn_fp_fn_e2e(pred, label):\n",
    "    pred_set = set(\n",
    "        f\"{range['start']}_{range['end']}_{range['polarity']}\" for range in pred)\n",
    "    label_set = set(\n",
    "        f\"{range['start']}_{range['end']}_{range['polarity']}\" for range in label)\n",
    "\n",
    "    tp_set = pred_set & label_set\n",
    "    tp = len(tp_set)\n",
    "\n",
    "    fp_set = pred_set - tp_set\n",
    "    fp = len(fp_set)\n",
    "\n",
    "    fn_set = label_set - tp_set\n",
    "    fn = len(fn_set)\n",
    "\n",
    "    return tp, 0, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tp_tn_fp_fn_e2e_total(pred_total, label_total):\n",
    "    tp_total = tn_total = fp_total = fn_total = 0\n",
    "    for i in range(len(label_total)):\n",
    "        tp, tn, fp, fn = calculate_tp_tn_fp_fn_e2e(\n",
    "            pred_total[i], label_total[i])\n",
    "        tp_total += tp\n",
    "        tn_total += tn\n",
    "        fp_total += fp\n",
    "        fn_total += fn\n",
    "    return tp_total, tn_total, fp_total, fn_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_macro_e2e(llm_annotations_aspect_terms, human_annotations_aspect_terms):\n",
    "    f1_scores = []\n",
    "    for pol in [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\", \"CONFLICT\"]:\n",
    "        llm_annotations_class = [[tag for tag in example if tag[\"polarity\"] == pol]\n",
    "                                 for example in llm_annotations_aspect_terms]\n",
    "        human_annotations_class = [[tag for tag in example if tag[\"polarity\"] == pol]\n",
    "                                   for example in human_annotations_aspect_terms]\n",
    "        \n",
    "        tp_total, tn_total, fp_total, fn_total = calculate_tp_tn_fp_fn_e2e_total(llm_annotations_class, human_annotations_class)\n",
    "        f1_scores.append(2 * tp_total / (2 * tp_total + fn_total + fp_total))\n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{2}{*}{\\textbf{Llama-3-70B}} & \\textbf{LRS\\textsubscript{25}} & 55.73 & 39.55 \\\\\n",
      " & \\textbf{LRS\\textsubscript{500}} & 56.05 & 40.11 \\\\\n",
      "\\hline\n",
      "\\multirow{2}{*}{\\textbf{GPT-3.5-turbo}} & \\textbf{LRS\\textsubscript{25}} & 67.23 & 50.28 \\\\\n",
      " & \\textbf{LRS\\textsubscript{500}} & 66.12 & 49.38 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for llm_idx, llm in enumerate(LLMS):\n",
    "    for fs_idx, fs in enumerate(FEW_SHOT_CONDITIONS):\n",
    "        llm_annotations_aspect_terms = [\n",
    "            [{\"start\": tag[\"start\"], \"end\": tag[\"end\"], \"polarity\": tag[\"aspect_polarity\"]} for tag in example if tag[\"aspect_term\"] is not None] for example in dataset[llm][fs][\"llm_annotation\"]]\n",
    "        human_annotations_aspect_terms = [\n",
    "            [{\"start\": tag[\"start\"], \"end\": tag[\"end\"], \"polarity\": tag[\"aspect_polarity\"]} for tag in example if tag[\"aspect_term\"] is not None] for example in dataset[llm][fs][\"human_annotation\"]]\n",
    "\n",
    "        tp_total, tn_total, fp_total, fn_total = calculate_tp_tn_fp_fn_e2e_total(\n",
    "            llm_annotations_aspect_terms, human_annotations_aspect_terms)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = (tp_total + tn_total) / (tp_total + tn_total + fp_total +\n",
    "                                            fn_total) if (tp_total + tn_total + fp_total + fn_total) > 0 else 0\n",
    "        precision = tp_total / \\\n",
    "            (tp_total + fp_total) if (tp_total + fp_total) > 0 else 0\n",
    "        recall = tp_total / \\\n",
    "            (tp_total + fn_total) if (tp_total + fn_total) > 0 else 0\n",
    "\n",
    "        f1_micro = 2 * tp_total / (2 * tp_total + fn_total + fp_total)\n",
    "\n",
    "        f1_macro = f1_macro_e2e(\n",
    "            llm_annotations_aspect_terms, human_annotations_aspect_terms)\n",
    "\n",
    "        llm_print = \"\\\\multirow{2}{*}{\" + \\\n",
    "            LLMS_ENCODED[llm] + \"}\" if fs_idx == 0 else \"\"\n",
    "\n",
    "        print(llm_print, \"&\", ENCODE_CONDITION[fs],\n",
    "              \"&\", \"{:.2f}\".format(f1_micro*100),\n",
    "              \"&\", \"{:.2f}\".format(f1_macro*100), \"\\\\\\\\\")\n",
    "    print(\"\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect Term + Aspect Category + Sentiment Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_macro(preds, labels):\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    for comb in [(ac, pol) for ac in constants.ASPECT_CATEGORIES for pol in constants.POLARITIES]:\n",
    "        aspect_category = comb[0]\n",
    "        polarity = comb[1]\n",
    "\n",
    "        pred_tuples_ac_pol = [[asp for asp in example if asp[\"aspect_category\"] ==\n",
    "                               aspect_category and asp[\"aspect_polarity\"] == polarity] for example in preds]\n",
    "        labels_tuples_ac_pol = [[asp for asp in example if asp[\"aspect_category\"] ==\n",
    "                                 aspect_category and asp[\"aspect_polarity\"] == polarity] for example in labels]\n",
    "\n",
    "        n_examples = sum([len(asp) for asp in labels_tuples_ac_pol])\n",
    "\n",
    "        ac_pol_metrics = calculate_metrics_for_examples(\n",
    "            labels_tuples_ac_pol, pred_tuples_ac_pol)\n",
    "        \n",
    "        if n_examples > 0:\n",
    "             f1_scores.append(ac_pol_metrics[\"f1\"])\n",
    "\n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{2}{*}{\\textbf{Llama-3-70B}} & \\textbf{LRS\\textsubscript{25}} & 46.71 & 43.94 \\\\\n",
      " & \\textbf{LRS\\textsubscript{500}} & 47.98 & 45.02 \\\\\n",
      "\\hline\n",
      "\\multirow{2}{*}{\\textbf{GPT-3.5-turbo}} & \\textbf{LRS\\textsubscript{25}} & 55.92 & 55.30 \\\\\n",
      " & \\textbf{LRS\\textsubscript{500}} & 55.38 & 54.71 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "for llm_idx, llm in enumerate(LLMS):\n",
    "    for fs_idx, fs in enumerate(FEW_SHOT_CONDITIONS):\n",
    "        metrics_triplets = calculate_metrics_for_examples(\n",
    "            dataset[llm][fs][\"human_annotation\"], dataset[llm][fs][\"llm_annotation\"])\n",
    "\n",
    "        f1_macro = compute_f1_macro(dataset[llm][fs][\"llm_annotation\"], dataset[llm][fs][\"human_annotation\"])\n",
    "\n",
    "        llm_print = \"\\\\multirow{2}{*}{\" + \\\n",
    "            LLMS_ENCODED[llm] + \"}\" if fs_idx == 0 else \"\"\n",
    "        print(llm_print,\n",
    "              \"&\", ENCODE_CONDITION[fs],\n",
    "              \"&\", \"{:.2f}\".format(metrics_triplets[\"f1\"] * 100),\n",
    "              \"&\", \"{:.2f}\".format(f1_macro * 100), \"\\\\\\\\\")\n",
    "\n",
    "    print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
