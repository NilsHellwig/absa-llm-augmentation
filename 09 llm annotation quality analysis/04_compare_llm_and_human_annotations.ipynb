{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Compare LLM and Human Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import json\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../07 train models/'))\n",
    "from TASD.evaluation import calculate_metrics_for_examples\n",
    "import constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMS = [\"Llama70B\", \"GPT-3\"]\n",
    "FEW_SHOT_CONDITIONS = [\"random\", \"fixed\"]\n",
    "AC_POLARITY_COMBINATIONS = [cat+\"_\"+polarity for cat in constants.ASPECT_CATEGORIES for polarity in constants.POLARITIES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"annotation_datasets/annotated_synth_dataset.json\", 'r') as json_file:\n",
    "    human_annotations = json.load(json_file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Synthetic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_annotations = []\n",
    "\n",
    "for llm in LLMS:\n",
    "    for fs in [\"random\", \"fixed\"]:\n",
    "       for split_id in range(5):\n",
    "           with open(f\"../07 train models/synth/{llm}/{fs}/split_{split_id}.json\", 'r') as json_file:\n",
    "              synthetic_data_split = json.load(json_file)\n",
    "              for example in  synthetic_data_split:\n",
    "                  llm_annotations.append(example)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_annotations_aspects = [([{\"aspect_category\": tag[\"label\"], \"aspect_polarity\": tag[\"polarity\"],\n",
    "                              \"aspect_term\": tag[\"text\"] if tag[\"text\"] != 'NULL' else None, \"start\": tag[\"start\"], \"end\": tag[\"end\"]} for tag in example[\"tags\"]], example[\"id\"]) for example in llm_annotations]\n",
    "human_annotations_aspects = [([{\"aspect_category\": tag[\"label\"], \"aspect_polarity\": tag[\"polarity\"], \"aspect_term\": tag[\"text\"]\n",
    "                                if tag[\"text\"] != 'NULL' else None, \"start\": tag[\"start\"], \"end\": tag[\"end\"]} for tag in example[\"tags\"]], example[\"id\"], example[\"model\"], example[\"few_shot_condtion\"]) for example in human_annotations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'aspect_category': 'GENERAL-IMPRESSION',\n",
       "   'aspect_polarity': 'POSITIVE',\n",
       "   'aspect_term': None,\n",
       "   'start': 0,\n",
       "   'end': 0}],\n",
       " 'a49a6f01-1ecc-4da0-b76b-f283f518fc60',\n",
       " 'Llama70B',\n",
       " 'random')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_annotations_aspects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_with_id(id, dataset):\n",
    "    return [example for example in dataset if example[1] == id][0][0]\n",
    "\n",
    "for llm in LLMS:\n",
    "    dataset[llm] = {}\n",
    "    for fs in FEW_SHOT_CONDITIONS:\n",
    "        dataset[llm][fs] = {}\n",
    "        human_annotations_aspects_ids = [example[1] for example in human_annotations_aspects if example[2] == llm and example[3] == fs]\n",
    "        human_annotations_samples = [example[0] for example in human_annotations_aspects if example[2] == llm and example[3] == fs]\n",
    "        llm_annotations_samples = [get_example_with_id(id, llm_annotations_aspects) for id in human_annotations_aspects_ids]\n",
    "\n",
    "        dataset[llm][fs][\"human_annotation\"] = human_annotations_samples\n",
    "        dataset[llm][fs][\"llm_annotation\"] = llm_annotations_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama70B random \n",
      " ----- ----- -----\n",
      "{'f1': 0.4669312169312169, 'recall': 0.500709219858156, 'precision': 0.43742255266418834, 'accuracy': 0.30457290767903367, 'tp': 353, 'tn': 0, 'fp': 454, 'fn': 352}\n",
      "Llama70B fixed \n",
      " ----- ----- -----\n",
      "{'f1': 0.4382227632379793, 'recall': 0.46272493573264784, 'precision': 0.4161849710982659, 'accuracy': 0.2805923616523772, 'tp': 360, 'tn': 0, 'fp': 505, 'fn': 418}\n",
      "GPT-3 random \n",
      " ----- ----- -----\n",
      "{'f1': 0.572644376899696, 'recall': 0.576499388004896, 'precision': 0.5688405797101449, 'accuracy': 0.40119250425894376, 'tp': 471, 'tn': 0, 'fp': 357, 'fn': 346}\n",
      "GPT-3 fixed \n",
      " ----- ----- -----\n",
      "{'f1': 0.5832414553472989, 'recall': 0.5977401129943503, 'precision': 0.5694294940796556, 'accuracy': 0.41167315175097274, 'tp': 529, 'tn': 0, 'fp': 400, 'fn': 356}\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for fs in FEW_SHOT_CONDITIONS:\n",
    "        print(llm, fs, \"\\n\", \"----- ----- -----\")\n",
    "        print(calculate_metrics_for_examples(dataset[llm][fs][\"human_annotation\"], dataset[llm][fs][\"llm_annotation\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_list_to_label(cat_list):\n",
    "    return [1 if cat in cat_list else 0 for cat in constants.ASPECT_CATEGORIES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama70B & random & 0.708029197080292 & 0.8110795454545454 & 0.8037365117505718\n",
      "Llama70B & fixed & 0.6498194945848376 & 0.7824377457404981 & 0.7734307720212131\n",
      "GPT-3 & random & 0.912751677852349 & 0.9550706033376123 & 0.9555349753741205\n",
      "GPT-3 & fixed & 0.9243697478991597 & 0.9649851632047478 & 0.9642089920810204\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for fs in FEW_SHOT_CONDITIONS:\n",
    "        llm_annotations_aspect_categories = [category_list_to_label(\n",
    "            [tag[\"aspect_category\"] for tag in example]) for example in dataset[llm][fs][\"llm_annotation\"]]\n",
    "        human_annotations_aspect_categories = [category_list_to_label(\n",
    "            [tag[\"aspect_category\"] for tag in example]) for example in dataset[llm][fs][\"human_annotation\"]]\n",
    "\n",
    "        accuracy = accuracy_score(\n",
    "            human_annotations_aspect_categories, llm_annotations_aspect_categories)\n",
    "        f1_micro = f1_score(human_annotations_aspect_categories,\n",
    "                            llm_annotations_aspect_categories, average='micro')\n",
    "        f1_macro = f1_score(human_annotations_aspect_categories,\n",
    "                            llm_annotations_aspect_categories, average='macro')\n",
    "\n",
    "        print(llm, \"&\", fs, \"&\", accuracy, \"&\", f1_micro, \"&\", f1_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect Category (performance for each Aspect Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama70B & random & GENERAL-IMPRESSION & 0.46938775510204084 & 0.8734177215189873 & 0.8394160583941606 & 79\n",
      "Llama70B & random & FOOD & 0.8540145985401459 & 0.6724137931034483 & 0.8594890510948905 & 174\n",
      "Llama70B & random & SERVICE & 0.912751677852349 & 0.8292682926829268 & 0.9251824817518248 & 164\n",
      "Llama70B & random & AMBIENCE & 0.86 & 0.8958333333333334 & 0.9343065693430657 & 144\n",
      "Llama70B & random & PRICE & 0.851063829787234 & 0.975609756097561 & 0.9562043795620438 & 123\n",
      "Llama70B & fixed & GENERAL-IMPRESSION & 0.46710526315789475 & 0.7634408602150538 & 0.8140794223826715 & 93\n",
      "Llama70B & fixed & FOOD & 0.8791946308724832 & 0.6752577319587629 & 0.8537906137184116 & 194\n",
      "Llama70B & fixed & SERVICE & 0.845679012345679 & 0.8616352201257862 & 0.9151624548736462 & 159\n",
      "Llama70B & fixed & AMBIENCE & 0.815068493150685 & 0.7828947368421053 & 0.8916967509025271 & 152\n",
      "Llama70B & fixed & PRICE & 0.8323353293413174 & 0.9144736842105263 & 0.9259927797833934 & 152\n",
      "GPT-3 & random & GENERAL-IMPRESSION & 0.910828025477707 & 0.9862068965517241 & 0.9731543624161074 & 145\n",
      "GPT-3 & random & FOOD & 0.993103448275862 & 0.8044692737430168 & 0.9395973154362416 & 179\n",
      "GPT-3 & random & SERVICE & 0.9939393939393939 & 0.9704142011834319 & 0.9899328859060402 & 169\n",
      "GPT-3 & random & AMBIENCE & 0.9863013698630136 & 0.9536423841059603 & 0.9848993288590604 & 151\n",
      "GPT-3 & random & PRICE & 0.9867549668874173 & 0.9933333333333333 & 0.9949664429530202 & 150\n",
      "GPT-3 & fixed & GENERAL-IMPRESSION & 0.9235294117647059 & 0.9691358024691358 & 0.9697478991596639 & 162\n",
      "GPT-3 & fixed & FOOD & 0.9872611464968153 & 0.8959537572254336 & 0.9663865546218487 & 173\n",
      "GPT-3 & fixed & SERVICE & 0.9943502824858758 & 0.967032967032967 & 0.9882352941176471 & 182\n",
      "GPT-3 & fixed & AMBIENCE & 0.9867549668874173 & 0.93125 & 0.9781512605042016 & 160\n",
      "GPT-3 & fixed & PRICE & 0.9943502824858758 & 1.0 & 0.9983193277310924 & 176\n"
     ]
    }
   ],
   "source": [
    "def category_list_to_label(cat_list):\n",
    "    return [1 if cat in cat_list else 0 for cat in constants.ASPECT_CATEGORIES]\n",
    "\n",
    "\n",
    "for llm in LLMS:\n",
    "    for fs in FEW_SHOT_CONDITIONS:\n",
    "        llm_annotations_aspect_categories = [category_list_to_label(\n",
    "            [tag[\"aspect_category\"] for tag in example]) for example in dataset[llm][fs][\"llm_annotation\"]]\n",
    "        human_annotations_aspect_categories = [category_list_to_label(\n",
    "            [tag[\"aspect_category\"] for tag in example]) for example in dataset[llm][fs][\"human_annotation\"]]\n",
    "\n",
    "        for aspect_category in constants.ASPECT_CATEGORIES:\n",
    "            idx = constants.ASPECT_CATEGORIES.index(aspect_category)\n",
    "\n",
    "            tp = sum((llm_annotations_aspect_categories[i][idx] == 1) and (\n",
    "                human_annotations_aspect_categories[i][idx] == 1) for i in range(len(llm_annotations_aspect_categories)))\n",
    "            fp = sum((llm_annotations_aspect_categories[i][idx] == 1) and (\n",
    "                human_annotations_aspect_categories[i][idx] == 0) for i in range(len(llm_annotations_aspect_categories)))\n",
    "            fn = sum((llm_annotations_aspect_categories[i][idx] == 0) and (\n",
    "                human_annotations_aspect_categories[i][idx] == 1) for i in range(len(llm_annotations_aspect_categories)))\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            accuracy = accuracy_score([example[idx] for example in human_annotations_aspect_categories], [\n",
    "                                      example[idx] for example in llm_annotations_aspect_categories])\n",
    "            n_samples_in_class = sum(\n",
    "                example[idx] == 1 for example in human_annotations_aspect_categories)\n",
    "\n",
    "            print(llm, \"&\", fs, \"&\", aspect_category, \"&\", precision,\n",
    "                  \"&\", recall, \"&\", accuracy, \"&\", n_samples_in_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect Category + Sentiment Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_polarity_list_to_label(cat_pol_list):\n",
    "    return [1 if ac_pol in cat_pol_list else 0 for ac_pol in AC_POLARITY_COMBINATIONS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama70B & random & 0.48722627737226276 & 0.5718213058419245 & 0.5413943549657524\n",
      "Llama70B & fixed & 0.427797833935018 & 0.5363408521303258 & 0.4974647635692283\n",
      "GPT-3 & random & 0.7986577181208053 & 0.8453865336658354 & 0.8318841090991446\n",
      "GPT-3 & fixed & 0.7831932773109244 & 0.8631221719457014 & 0.8542210405117728\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for fs in FEW_SHOT_CONDITIONS:\n",
    "        llm_annotations_ac_pol = [category_polarity_list_to_label(\n",
    "            [tag[\"aspect_category\"]+\"_\"+tag[\"aspect_polarity\"] for tag in example]) for example in dataset[llm][fs][\"llm_annotation\"]]\n",
    "        human_annotations_ac_pol = [category_polarity_list_to_label(\n",
    "            [tag[\"aspect_category\"]+\"_\"+tag[\"aspect_polarity\"] for tag in example]) for example in dataset[llm][fs][\"human_annotation\"]]\n",
    "\n",
    "        accuracy = accuracy_score(\n",
    "            human_annotations_ac_pol, llm_annotations_ac_pol)\n",
    "        f1_micro = f1_score(human_annotations_ac_pol,\n",
    "                            llm_annotations_ac_pol, average='micro')\n",
    "        f1_macro = f1_score(human_annotations_ac_pol,\n",
    "                            llm_annotations_ac_pol, average='macro')\n",
    "\n",
    "        print(llm, \"&\", fs, \"&\", accuracy, \"&\", f1_micro, \"&\", f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama70B & random & GENERAL-IMPRESSION & POSITIVE & 0.5777777777777777 & 0.5777777777777777 & 0.9306569343065694 & 45\n",
      "Llama70B & random & GENERAL-IMPRESSION & NEUTRAL & 0.1076923076923077 & 0.7 & 0.8886861313868614 & 10\n",
      "Llama70B & random & GENERAL-IMPRESSION & NEGATIVE & 0.3333333333333333 & 0.6666666666666666 & 0.927007299270073 & 24\n",
      "Llama70B & random & FOOD & POSITIVE & 0.8 & 0.43478260869565216 & 0.8868613138686131 & 92\n",
      "Llama70B & random & FOOD & NEUTRAL & 0.42857142857142855 & 0.65625 & 0.9288321167883211 & 32\n",
      "Llama70B & random & FOOD & NEGATIVE & 0.813953488372093 & 0.660377358490566 & 0.9525547445255474 & 53\n",
      "Llama70B & random & SERVICE & POSITIVE & 0.859375 & 0.5238095238095238 & 0.8923357664233577 & 105\n",
      "Llama70B & random & SERVICE & NEUTRAL & 0.18181818181818182 & 0.9090909090909091 & 0.916058394160584 & 11\n",
      "Llama70B & random & SERVICE & NEGATIVE & 0.85 & 0.6938775510204082 & 0.9616788321167883 & 49\n",
      "Llama70B & random & AMBIENCE & POSITIVE & 0.8461538461538461 & 0.5432098765432098 & 0.9178832116788321 & 81\n",
      "Llama70B & random & AMBIENCE & NEUTRAL & 0.11538461538461539 & 0.6666666666666666 & 0.9105839416058394 & 9\n",
      "Llama70B & random & AMBIENCE & NEGATIVE & 0.7142857142857143 & 0.7272727272727273 & 0.9434306569343066 & 55\n",
      "Llama70B & random & PRICE & POSITIVE & 0.58 & 0.7073170731707317 & 0.9397810218978102 & 41\n",
      "Llama70B & random & PRICE & NEUTRAL & 0.4166666666666667 & 0.6896551724137931 & 0.9324817518248175 & 29\n",
      "Llama70B & random & PRICE & NEGATIVE & 0.673469387755102 & 0.6226415094339622 & 0.9343065693430657 & 53\n",
      "Llama70B & fixed & GENERAL-IMPRESSION & POSITIVE & 0.4745762711864407 & 0.5 & 0.8935018050541517 & 56\n",
      "Llama70B & fixed & GENERAL-IMPRESSION & NEUTRAL & 0.12727272727272726 & 0.7 & 0.907942238267148 & 10\n",
      "Llama70B & fixed & GENERAL-IMPRESSION & NEGATIVE & 0.2549019607843137 & 0.48148148148148145 & 0.9061371841155235 & 27\n",
      "Llama70B & fixed & FOOD & POSITIVE & 0.8518518518518519 & 0.4380952380952381 & 0.8790613718411552 & 105\n",
      "Llama70B & fixed & FOOD & NEUTRAL & 0.39285714285714285 & 0.6470588235294118 & 0.9169675090252708 & 34\n",
      "Llama70B & fixed & FOOD & NEGATIVE & 0.6888888888888889 & 0.5081967213114754 & 0.9205776173285198 & 61\n",
      "Llama70B & fixed & SERVICE & POSITIVE & 0.7678571428571429 & 0.4942528735632184 & 0.8971119133574007 & 87\n",
      "Llama70B & fixed & SERVICE & NEUTRAL & 0.1864406779661017 & 1.0 & 0.9133574007220217 & 11\n",
      "Llama70B & fixed & SERVICE & NEGATIVE & 0.8070175438596491 & 0.7419354838709677 & 0.9512635379061372 & 62\n",
      "Llama70B & fixed & AMBIENCE & POSITIVE & 0.7391304347826086 & 0.40963855421686746 & 0.8898916967509025 & 83\n",
      "Llama70B & fixed & AMBIENCE & NEUTRAL & 0.125 & 0.6666666666666666 & 0.9187725631768953 & 9\n",
      "Llama70B & fixed & AMBIENCE & NEGATIVE & 0.7076923076923077 & 0.7301587301587301 & 0.9350180505415162 & 63\n",
      "Llama70B & fixed & PRICE & POSITIVE & 0.6166666666666667 & 0.6271186440677966 & 0.9187725631768953 & 59\n",
      "Llama70B & fixed & PRICE & NEUTRAL & 0.22388059701492538 & 0.625 & 0.8898916967509025 & 24\n",
      "Llama70B & fixed & PRICE & NEGATIVE & 0.7543859649122807 & 0.6142857142857143 & 0.9259927797833934 & 70\n",
      "GPT-3 & random & GENERAL-IMPRESSION & POSITIVE & 0.8695652173913043 & 0.9090909090909091 & 0.9832214765100671 & 44\n",
      "GPT-3 & random & GENERAL-IMPRESSION & NEUTRAL & 0.7833333333333333 & 0.9791666666666666 & 0.9765100671140939 & 48\n",
      "GPT-3 & random & GENERAL-IMPRESSION & NEGATIVE & 0.8909090909090909 & 0.9245283018867925 & 0.9832214765100671 & 53\n",
      "GPT-3 & random & FOOD & POSITIVE & 0.9827586206896551 & 0.6627906976744186 & 0.9496644295302014 & 86\n",
      "GPT-3 & random & FOOD & NEUTRAL & 0.9459459459459459 & 0.8536585365853658 & 0.9865771812080537 & 41\n",
      "GPT-3 & random & FOOD & NEGATIVE & 0.9629629629629629 & 0.9454545454545454 & 0.9916107382550335 & 55\n",
      "GPT-3 & random & SERVICE & POSITIVE & 0.96875 & 0.9538461538461539 & 0.9916107382550335 & 65\n",
      "GPT-3 & random & SERVICE & NEUTRAL & 0.9354838709677419 & 0.9830508474576272 & 0.9916107382550335 & 59\n",
      "GPT-3 & random & SERVICE & NEGATIVE & 0.9787234042553191 & 0.9387755102040817 & 0.9932885906040269 & 49\n",
      "GPT-3 & random & AMBIENCE & POSITIVE & 0.9583333333333334 & 0.8214285714285714 & 0.9798657718120806 & 56\n",
      "GPT-3 & random & AMBIENCE & NEUTRAL & 0.7959183673469388 & 0.9512195121951219 & 0.9798657718120806 & 41\n",
      "GPT-3 & random & AMBIENCE & NEGATIVE & 0.9310344827586207 & 0.9473684210526315 & 0.988255033557047 & 57\n",
      "GPT-3 & random & PRICE & POSITIVE & 0.6610169491525424 & 0.4936708860759494 & 0.8993288590604027 & 79\n",
      "GPT-3 & random & PRICE & NEUTRAL & 0.11538461538461539 & 1.0 & 0.9228187919463087 & 6\n",
      "GPT-3 & random & PRICE & NEGATIVE & 0.9795918367346939 & 0.7164179104477612 & 0.9664429530201343 & 67\n",
      "GPT-3 & fixed & GENERAL-IMPRESSION & POSITIVE & 0.8548387096774194 & 0.8983050847457628 & 0.9747899159663865 & 59\n",
      "GPT-3 & fixed & GENERAL-IMPRESSION & NEUTRAL & 0.8 & 0.9811320754716981 & 0.9764705882352941 & 53\n",
      "GPT-3 & fixed & GENERAL-IMPRESSION & NEGATIVE & 0.8727272727272727 & 0.96 & 0.984873949579832 & 50\n",
      "GPT-3 & fixed & FOOD & POSITIVE & 1.0 & 0.7702702702702703 & 0.9714285714285714 & 74\n",
      "GPT-3 & fixed & FOOD & NEUTRAL & 0.9322033898305084 & 0.9821428571428571 & 0.9915966386554622 & 56\n",
      "GPT-3 & fixed & FOOD & NEGATIVE & 0.98 & 0.98 & 0.9966386554621849 & 50\n",
      "GPT-3 & fixed & SERVICE & POSITIVE & 0.9298245614035088 & 0.8983050847457628 & 0.9831932773109243 & 59\n",
      "GPT-3 & fixed & SERVICE & NEUTRAL & 0.8676470588235294 & 1.0 & 0.984873949579832 & 59\n",
      "GPT-3 & fixed & SERVICE & NEGATIVE & 0.9838709677419355 & 0.9242424242424242 & 0.9899159663865547 & 66\n",
      "GPT-3 & fixed & AMBIENCE & POSITIVE & 0.9574468085106383 & 0.8035714285714286 & 0.9781512605042016 & 56\n",
      "GPT-3 & fixed & AMBIENCE & NEUTRAL & 0.7924528301886793 & 0.9333333333333333 & 0.9764705882352941 & 45\n",
      "GPT-3 & fixed & AMBIENCE & NEGATIVE & 0.9384615384615385 & 0.9384615384615385 & 0.9865546218487395 & 65\n",
      "GPT-3 & fixed & PRICE & POSITIVE & 0.875 & 0.5656565656565656 & 0.9142857142857143 & 99\n",
      "GPT-3 & fixed & PRICE & NEUTRAL & 0.14492753623188406 & 0.9090909090909091 & 0.8991596638655462 & 11\n",
      "GPT-3 & fixed & PRICE & NEGATIVE & 1.0 & 0.8732394366197183 & 0.984873949579832 & 71\n"
     ]
    }
   ],
   "source": [
    "def category_polarity_list_to_label(cat_polarity_list):\n",
    "    return [1 if cat_polarity in cat_polarity_list else 0 for cat_polarity in AC_POLARITY_COMBINATIONS]\n",
    "\n",
    "for llm in LLMS:\n",
    "    for fs in FEW_SHOT_CONDITIONS:\n",
    "        llm_annotations_ac_pol = [category_polarity_list_to_label(\n",
    "            [tag[\"aspect_category\"] + \"_\" + tag[\"aspect_polarity\"] for tag in example]) for example in dataset[llm][fs][\"llm_annotation\"]]\n",
    "        human_annotations_ac_pol = [category_polarity_list_to_label(\n",
    "            [tag[\"aspect_category\"] + \"_\" + tag[\"aspect_polarity\"] for tag in example]) for example in dataset[llm][fs][\"human_annotation\"]]\n",
    "\n",
    "        for ac_pol_combination in AC_POLARITY_COMBINATIONS:\n",
    "            idx = AC_POLARITY_COMBINATIONS.index(ac_pol_combination)\n",
    "\n",
    "            tp = sum((llm_annotations_ac_pol[i][idx] == 1) and (\n",
    "                human_annotations_ac_pol[i][idx] == 1) for i in range(len(llm_annotations_ac_pol)))\n",
    "            fp = sum((llm_annotations_ac_pol[i][idx] == 1) and (\n",
    "                human_annotations_ac_pol[i][idx] == 0) for i in range(len(llm_annotations_ac_pol)))\n",
    "            fn = sum((llm_annotations_ac_pol[i][idx] == 0) and (\n",
    "                human_annotations_ac_pol[i][idx] == 1) for i in range(len(llm_annotations_ac_pol)))\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            accuracy = accuracy_score([example[idx] for example in human_annotations_ac_pol], [\n",
    "                                      example[idx] for example in llm_annotations_ac_pol])\n",
    "            n_samples_in_class = sum(\n",
    "                example[idx] == 1 for example in human_annotations_ac_pol)\n",
    "\n",
    "            aspect_category, sentiment_polarity = ac_pol_combination.split(\"_\")\n",
    "\n",
    "            print(llm, \"&\", fs, \"&\", aspect_category, \"&\", sentiment_polarity, \"&\", precision,\n",
    "                  \"&\", recall, \"&\", accuracy, \"&\", n_samples_in_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect Term Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tp_tn_fp_fn_aspect_term(pred, label):\n",
    "    pred_set = set(\n",
    "        f\"{range['start']}_{range['end']}\" for range in pred)\n",
    "    label_set = set(\n",
    "        f\"{range['start']}_{range['end']}\" for range in label)\n",
    "\n",
    "    tp_set = pred_set & label_set\n",
    "    tp = len(tp_set)\n",
    "\n",
    "    fp_set = pred_set - tp_set\n",
    "    fp = len(fp_set)\n",
    "\n",
    "    fn_set = label_set - tp_set\n",
    "    fn = len(fn_set)\n",
    "\n",
    "    return tp, 0, fp, fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama70B & random & 0.7027417027417028 & 0.8254237288135593 & 0.8650088809946714 & 0.7893030794165316\n",
      "Llama70B & fixed & 0.7067183462532299 & 0.8281604844814534 & 0.882258064516129 & 0.7803138373751783\n",
      "GPT-3 & random & 0.5872395833333334 & 0.7399507793273175 & 0.9356846473029046 & 0.6119402985074627\n",
      "GPT-3 & fixed & 0.62004662004662 & 0.7654676258992805 & 0.918825561312608 & 0.655980271270037\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for fs in FEW_SHOT_CONDITIONS:\n",
    "        llm_annotations_aspect_terms = [\n",
    "            [{\"start\": tag[\"start\"], \"end\": tag[\"end\"]} for tag in example if tag[\"aspect_term\"] is not None] for example in dataset[llm][fs][\"llm_annotation\"]]\n",
    "        human_annotations_aspect_terms = [\n",
    "            [{\"start\": tag[\"start\"], \"end\": tag[\"end\"]} for tag in example if tag[\"aspect_term\"] is not None] for example in dataset[llm][fs][\"human_annotation\"]]\n",
    "\n",
    "        tp_total = tn_total = fp_total = fn_total = 0\n",
    "        for i in range(len(human_annotations_aspect_terms)):\n",
    "            tp, tn, fp, fn = calculate_tp_tn_fp_fn_aspect_term(\n",
    "                llm_annotations_aspect_terms[i], human_annotations_aspect_terms[i])\n",
    "            tp_total += tp\n",
    "            tn_total += tn\n",
    "            fp_total += fp\n",
    "            fn_total += fn\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = (tp_total + tn_total) / (tp_total + tn_total + fp_total +\n",
    "                                            fn_total) if (tp_total + tn_total + fp_total + fn_total) > 0 else 0\n",
    "        precision = tp_total / \\\n",
    "            (tp_total + fp_total) if (tp_total + fp_total) > 0 else 0\n",
    "        recall = tp_total / \\\n",
    "            (tp_total + fn_total) if (tp_total + fn_total) > 0 else 0\n",
    "\n",
    "\n",
    "        f1 = 2 * tp_total / (2 * tp_total + fn_total + fp_total)\n",
    "\n",
    "        print(llm, \"&\", fs, \"&\", accuracy, \"&\",\n",
    "              f1, \"&\", precision, \"&\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End-2-End ABSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tp_tn_fp_fn_e2e(pred, label):\n",
    "    pred_set = set(\n",
    "        f\"{range['start']}_{range['end']}_{range['polarity']}\" for range in pred)\n",
    "    label_set = set(\n",
    "        f\"{range['start']}_{range['end']}_{range['polarity']}\" for range in label)\n",
    "\n",
    "    tp_set = pred_set & label_set\n",
    "    tp = len(tp_set)\n",
    "\n",
    "    fp_set = pred_set - tp_set\n",
    "    fp = len(fp_set)\n",
    "\n",
    "    fn_set = label_set - tp_set\n",
    "    fn = len(fn_set)\n",
    "\n",
    "    return tp, 0, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama70B & random & 0.4107142857142857 & 0.5822784810126582 & 0.6095406360424028 & 0.5573505654281099\n",
      "Llama70B & fixed & 0.4088983050847458 & 0.5804511278195489 & 0.6166134185303515 & 0.5482954545454546\n",
      "GPT-3 & random & 0.5141800246609125 & 0.6791530944625407 & 0.8651452282157677 & 0.5589812332439679\n",
      "GPT-3 & fixed & 0.5318681318681319 & 0.6944045911047346 & 0.8359240069084629 & 0.5938650306748466\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for fs in FEW_SHOT_CONDITIONS:\n",
    "        llm_annotations_aspect_terms = [\n",
    "            [{\"start\": tag[\"start\"], \"end\": tag[\"end\"], \"polarity\": tag[\"aspect_polarity\"]} for tag in example if tag[\"aspect_term\"] is not None] for example in dataset[llm][fs][\"llm_annotation\"]]\n",
    "        human_annotations_aspect_terms = [\n",
    "            [{\"start\": tag[\"start\"], \"end\": tag[\"end\"], \"polarity\": tag[\"aspect_polarity\"]} for tag in example if tag[\"aspect_term\"] is not None] for example in dataset[llm][fs][\"human_annotation\"]]\n",
    "\n",
    "        tp_total = tn_total = fp_total = fn_total = 0\n",
    "        for i in range(len(human_annotations_aspect_terms)):\n",
    "            tp, tn, fp, fn = calculate_tp_tn_fp_fn_e2e(\n",
    "                llm_annotations_aspect_terms[i], human_annotations_aspect_terms[i])\n",
    "            tp_total += tp\n",
    "            tn_total += tn\n",
    "            fp_total += fp\n",
    "            fn_total += fn\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = (tp_total + tn_total) / (tp_total + tn_total + fp_total +\n",
    "                                            fn_total) if (tp_total + tn_total + fp_total + fn_total) > 0 else 0\n",
    "        precision = tp_total / \\\n",
    "            (tp_total + fp_total) if (tp_total + fp_total) > 0 else 0\n",
    "        recall = tp_total / \\\n",
    "            (tp_total + fn_total) if (tp_total + fn_total) > 0 else 0\n",
    "\n",
    "        f1 = 2 * tp_total / (2 * tp_total + fn_total + fp_total)\n",
    "\n",
    "        print(llm, \"&\", fs, \"&\", accuracy, \"&\",\n",
    "              f1, \"&\", precision, \"&\", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
